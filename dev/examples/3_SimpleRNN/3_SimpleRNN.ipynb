{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training a Simple LSTM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial we will go over using a recurrent neural network to classify clockwise\n",
    "and anticlockwise spirals. By the end of this tutorial you will be able to:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Create custom Lux models.\n",
    "2. Become familiar with the Lux recurrent neural network API.\n",
    "3. Training using Optimisers.jl and Zygote.jl."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Package Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: If you wish to use `AutoZygote()` for automatic differentiation,\n",
    "add Zygote to your project dependencies and include `using Zygote`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ADTypes, Lux, JLD2, MLUtils, Optimisers, Printf, Reactant, Random"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use MLUtils to generate 500 (noisy) clockwise and 500 (noisy) anticlockwise\n",
    "spirals. Using this data we will create a `MLUtils.DataLoader`. Our dataloader will give\n",
    "us sequences of size 2 × seq_len × batch_size and we need to predict a binary value\n",
    "whether the sequence is clockwise or anticlockwise."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function create_dataset(; dataset_size=1000, sequence_length=50)\n",
    "    # Create the spirals\n",
    "    data = [MLUtils.Datasets.make_spiral(sequence_length) for _ in 1:dataset_size]\n",
    "    # Get the labels\n",
    "    labels = vcat(repeat([0.0f0], dataset_size ÷ 2), repeat([1.0f0], dataset_size ÷ 2))\n",
    "    clockwise_spirals = [\n",
    "        reshape(d[1][:, 1:sequence_length], :, sequence_length, 1) for\n",
    "        d in data[1:(dataset_size ÷ 2)]\n",
    "    ]\n",
    "    anticlockwise_spirals = [\n",
    "        reshape(d[1][:, (sequence_length + 1):end], :, sequence_length, 1) for\n",
    "        d in data[((dataset_size ÷ 2) + 1):end]\n",
    "    ]\n",
    "    x_data = Float32.(cat(clockwise_spirals..., anticlockwise_spirals...; dims=3))\n",
    "    return x_data, labels\n",
    "end\n",
    "\n",
    "function get_dataloaders(; dataset_size=1000, sequence_length=50)\n",
    "    x_data, labels = create_dataset(; dataset_size, sequence_length)\n",
    "    # Split the dataset\n",
    "    (x_train, y_train), (x_val, y_val) = splitobs((x_data, labels); at=0.8, shuffle=true)\n",
    "    # Create DataLoaders\n",
    "    return (\n",
    "        # Use DataLoader to automatically minibatch and shuffle the data\n",
    "        DataLoader(\n",
    "            collect.((x_train, y_train)); batchsize=128, shuffle=true, partial=false\n",
    "        ),\n",
    "        # Don't shuffle the validation data\n",
    "        DataLoader(collect.((x_val, y_val)); batchsize=128, shuffle=false, partial=false),\n",
    "    )\n",
    "end\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will be extending the `Lux.AbstractLuxContainerLayer` type for our custom model\n",
    "since it will contain a LSTM block and a classifier head."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We pass the field names `lstm_cell` and `classifier` to the type to ensure that the\n",
    "parameters and states are automatically populated and we don't have to define\n",
    "`Lux.initialparameters` and `Lux.initialstates`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To understand more about container layers, please look at\n",
    "Container Layer."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct SpiralClassifier{L,C} <: AbstractLuxContainerLayer{(:lstm_cell, :classifier)}\n",
    "    lstm_cell::L\n",
    "    classifier::C\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We won't define the model from scratch but rather use the `Lux.LSTMCell` and\n",
    "`Lux.Dense`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function SpiralClassifier(in_dims, hidden_dims, out_dims)\n",
    "    return SpiralClassifier(\n",
    "        LSTMCell(in_dims => hidden_dims), Dense(hidden_dims => out_dims, sigmoid)\n",
    "    )\n",
    "end\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use default Lux blocks -- `Recurrence(LSTMCell(in_dims => hidden_dims)` -- instead\n",
    "of defining the following. But let's still do it for the sake of it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to define the behavior of the Classifier when it is invoked."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function (s::SpiralClassifier)(\n",
    "    x::AbstractArray{T,3}, ps::NamedTuple, st::NamedTuple\n",
    ") where {T}\n",
    "    # First we will have to run the sequence through the LSTM Cell\n",
    "    # The first call to LSTM Cell will create the initial hidden state\n",
    "    # See that the parameters and states are automatically populated into a field called\n",
    "    # `lstm_cell` We use `eachslice` to get the elements in the sequence without copying,\n",
    "    # and `Iterators.peel` to split out the first element for LSTM initialization.\n",
    "    x_init, x_rest = Iterators.peel(LuxOps.eachslice(x, Val(2)))\n",
    "    (y, carry), st_lstm = s.lstm_cell(x_init, ps.lstm_cell, st.lstm_cell)\n",
    "    # Now that we have the hidden state and memory in `carry` we will pass the input and\n",
    "    # `carry` jointly\n",
    "    for x in x_rest\n",
    "        (y, carry), st_lstm = s.lstm_cell((x, carry), ps.lstm_cell, st_lstm)\n",
    "    end\n",
    "    # After running through the sequence we will pass the output through the classifier\n",
    "    y, st_classifier = s.classifier(y, ps.classifier, st.classifier)\n",
    "    # Finally remember to create the updated state\n",
    "    st = merge(st, (classifier=st_classifier, lstm_cell=st_lstm))\n",
    "    return vec(y), st\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using the `@compact` API"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also define the model using the `Lux.@compact` API, which is a more concise\n",
    "way of defining models. This macro automatically handles the boilerplate code for you and\n",
    "as such we recommend this way of defining custom layers"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function SpiralClassifierCompact(in_dims, hidden_dims, out_dims)\n",
    "    lstm_cell = LSTMCell(in_dims => hidden_dims)\n",
    "    classifier = Dense(hidden_dims => out_dims, sigmoid)\n",
    "    return @compact(; lstm_cell, classifier) do x::AbstractArray{T,3} where {T}\n",
    "        x_init, x_rest = Iterators.peel(LuxOps.eachslice(x, Val(2)))\n",
    "        y, carry = lstm_cell(x_init)\n",
    "        for x in x_rest\n",
    "            y, carry = lstm_cell((x, carry))\n",
    "        end\n",
    "        @return vec(classifier(y))\n",
    "    end\n",
    "end\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining Accuracy, Loss and Optimiser"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's define the binary cross-entropy loss. Typically it is recommended to use\n",
    "`logitbinarycrossentropy` since it is more numerically stable, but for the sake of\n",
    "simplicity we will use `binarycrossentropy`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "const lossfn = BinaryCrossEntropyLoss()\n",
    "\n",
    "function compute_loss(model, ps, st, (x, y))\n",
    "    ŷ, st_ = model(x, ps, st)\n",
    "    loss = lossfn(ŷ, y)\n",
    "    return loss, st_, (; y_pred=ŷ)\n",
    "end\n",
    "\n",
    "matches(y_pred, y_true) = sum((y_pred .> 0.5f0) .== y_true)\n",
    "accuracy(y_pred, y_true) = matches(y_pred, y_true) / length(y_pred)\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the Model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function main(model_type)\n",
    "    dev = reactant_device()\n",
    "    cdev = cpu_device()\n",
    "\n",
    "    # Get the dataloaders\n",
    "    train_loader, val_loader = dev(get_dataloaders())\n",
    "\n",
    "    # Create the model\n",
    "    model = model_type(2, 8, 1)\n",
    "    ps, st = dev(Lux.setup(Random.default_rng(), model))\n",
    "\n",
    "    train_state = Training.TrainState(model, ps, st, Adam(0.01f0))\n",
    "    model_compiled = if dev isa ReactantDevice\n",
    "        Reactant.with_config(;\n",
    "            dot_general_precision=PrecisionConfig.HIGH,\n",
    "            convolution_precision=PrecisionConfig.HIGH,\n",
    "        ) do\n",
    "            @compile model(first(train_loader)[1], ps, Lux.testmode(st))\n",
    "        end\n",
    "    else\n",
    "        model\n",
    "    end\n",
    "    ad = dev isa ReactantDevice ? AutoEnzyme() : AutoZygote()\n",
    "\n",
    "    for epoch in 1:25\n",
    "        # Train the model\n",
    "        total_loss = 0.0f0\n",
    "        total_samples = 0\n",
    "        for (x, y) in train_loader\n",
    "            (_, loss, _, train_state) = Training.single_train_step!(\n",
    "                ad, lossfn, (x, y), train_state\n",
    "            )\n",
    "            total_loss += loss * length(y)\n",
    "            total_samples += length(y)\n",
    "        end\n",
    "        @printf(\"Epoch [%3d]: Loss %4.5f\\n\", epoch, total_loss / total_samples)\n",
    "\n",
    "        # Validate the model\n",
    "        total_acc = 0.0f0\n",
    "        total_loss = 0.0f0\n",
    "        total_samples = 0\n",
    "\n",
    "        st_ = Lux.testmode(train_state.states)\n",
    "        for (x, y) in val_loader\n",
    "            ŷ, st_ = model_compiled(x, train_state.parameters, st_)\n",
    "            ŷ, y = cdev(ŷ), cdev(y)\n",
    "            total_acc += accuracy(ŷ, y) * length(y)\n",
    "            total_loss += lossfn(ŷ, y) * length(y)\n",
    "            total_samples += length(y)\n",
    "        end\n",
    "\n",
    "        @printf(\n",
    "            \"Validation:\\tLoss %4.5f\\tAccuracy %4.5f\\n\",\n",
    "            total_loss / total_samples,\n",
    "            total_acc / total_samples\n",
    "        )\n",
    "    end\n",
    "\n",
    "    return cpu_device()((train_state.parameters, train_state.states))\n",
    "end\n",
    "\n",
    "ps_trained, st_trained = main(SpiralClassifier)\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also train the compact model with the exact same code!"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ps_trained2, st_trained2 = main(SpiralClassifierCompact)\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can save the model using JLD2 (and any other serialization library of your choice)\n",
    "Note that we transfer the model to CPU before saving. Additionally, we recommend that\n",
    "you don't save the model struct and only save the parameters and states."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@save \"trained_model.jld2\" ps_trained st_trained"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try loading the model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load \"trained_model.jld2\" ps_trained st_trained"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.7",
   "language": "julia"
  }
 },
 "nbformat": 4
}