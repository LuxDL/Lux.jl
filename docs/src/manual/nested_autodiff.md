# [Nested Automatic Differentiation](@id nested_autodiff)

!!! note

    This is a relatively new feature in Lux, so there might be some rough edges. If you
    encounter any issues, please let us know by opening an issue on the
    [GitHub repository](https://github.com/LuxDL/Lux.jl).

In this manual, we will explore how to use automatic differentiation (AD) inside your layers
or loss functions and have Lux automatically switch the AD backend with a faster one when
needed.

!!! tip

    Don't wan't Lux to do this switching for you? You can disable it by setting the
    `automatic_nested_ad_switching` Preference to `false`.

    Remember that if you are using ForwardDiff inside a Zygote call, it will drop gradients
    (with a warning message), so it is not recommended to use this combination.

Let's explore this using some questions that were posted on the
[Julia Discourse forum](https://discourse.julialang.org/).

```@example nested_ad
using ADTypes, Lux, LinearAlgebra, Zygote, ForwardDiff, Random
using ComponentArrays, FiniteDiff
```

First let's set the stage using some minor changes that need to be made for this feature to
work:

  - Switching only works if a [`StatefulLuxLayer`](@ref) is being used, with the following
    function calls:
    - For operations on the inputs:
      - `(<some-function> ∘ <StatefulLuxLayer>)(x::AbstractArray)`
      - `(<StatefulLuxLayer> ∘ <some-function>)(x::AbstractArray)`
      - `(<StatefulLuxLayer>)(x::AbstractArray)`
    - For operations on the parameters:
      - `(<some-function> ∘ Base.Fix1(<StatefulLuxLayer>, x))(ps)`
      - `(Base.Fix1(<StatefulLuxLayer>, x) ∘ <some-function>)(ps)`
      - `(Base.Fix1(<StatefulLuxLayer>, x))(ps)`
  - Currently we have custom routines implemented for:
    - `Zygote.<gradient|jacobian>`
    - `ForwardDiff.<gradient|jacobian>`
    - [`vector_jacobian_product`](@ref)
    - [`jacobian_vector_product`](@ref)
    - [`batched_jacobian`](@ref)
  - Switching only happens for `ChainRules` compatible AD libraries.

We plan to capture `DifferentiationInterface`, and `Enzyme.autodiff` calls in the
future (PRs are welcome).

!!! tip

    [`@compact`](@ref) uses [`StatefulLuxLayer`](@ref)s internally, so you can directly use
    these features inside a layer generated by [`@compact`](@ref).

## Loss Function containing Jacobian Computation

This problem comes from `@facusapienza` on [Discourse](https://discourse.julialang.org/t/nested-and-different-ad-methods-altogether-how-to-add-ad-calculations-inside-my-loss-function-when-using-neural-differential-equations/108985).
In this case, we want to add a regularization term to the neural DE based on first-order
derivatives. The neural DE part is not important here and we can demonstrate this easily
with a standard neural network.

```@example nested_ad
function loss_function1(model, x, ps, st, y)
    # Make it a stateful layer
    smodel = StatefulLuxLayer{true}(model, ps, st)
    ŷ = smodel(x)
    loss_emp = sum(abs2, ŷ .- y)
    # You can use `Zygote.jacobian` as well but ForwardDiff tends to be more efficient here
    J = ForwardDiff.jacobian(smodel, x)
    loss_reg = abs2(norm(J))
    return loss_emp + loss_reg
end

# Using Batchnorm to show that it is possible
model = Chain(Dense(2 => 4, tanh), BatchNorm(4), Dense(4 => 2))
ps, st = Lux.setup(Xoshiro(0), model)
x = rand(Xoshiro(0), Float32, 2, 10)
y = rand(Xoshiro(11), Float32, 2, 10)

loss_function1(model, x, ps, st, y)
```

So our loss function works, let's take the gradient (forward diff doesn't nest nicely here):

```@example nested_ad
_, ∂x, ∂ps, _, _ = Zygote.gradient(loss_function1, model, x, ps, st, y)
```

Now let's verify the gradients using finite differences:

```@example nested_ad
∂x_fd = FiniteDiff.finite_difference_gradient(x -> loss_function1(model, x, ps, st, y), x)
∂ps_fd = FiniteDiff.finite_difference_gradient(ps -> loss_function1(model, x, ps, st, y),
    ComponentArray(ps))

println("∞-norm(∂x - ∂x_fd): ", norm(∂x .- ∂x_fd, Inf))
@assert norm(∂x .- ∂x_fd, Inf) < 1e-1 # hide
println("∞-norm(∂ps - ∂ps_fd): ", norm(ComponentArray(∂ps) .- ∂ps_fd, Inf))
@assert norm(ComponentArray(∂ps) .- ∂ps_fd, Inf) < 1e-1 # hide
nothing; # hide
```

That's pretty good, of course you will have some error from the finite differences
calculation.

## Loss Function contains Gradient Computation

Ok here I am going to cheat a bit. This comes from a discussion on nested AD for PINNs
on [Discourse](https://discourse.julialang.org/t/is-it-possible-to-do-nested-ad-elegantly-in-julia-pinns/98888/21).
As the consensus there, we shouldn't use nested AD for 3rd or higher order differentiation.
Note that in the example there, the user uses `ForwardDiff.derivative` but we will use
`ForwardDiff.gradient` instead, as we typically deal with array inputs and outputs.

```@example nested_ad
function loss_function2(model, t, ps, st)
    smodel = StatefulLuxLayer{true}(model, ps, st)
    ŷ = only(Zygote.gradient(Base.Fix1(sum, abs2) ∘ smodel, t)) # Zygote returns a tuple
    return sum(abs2, ŷ .- cos.(t))
end

model = Chain(Dense(1 => 12,tanh), Dense(12 => 12,tanh), Dense(12 => 12,tanh),
    Dense(12 => 1))
ps, st = Lux.setup(Xoshiro(0), model)
t = rand(Xoshiro(0), Float32, 1, 16)
```

Now the moment of truth:

```@example nested_ad
_, ∂t, ∂ps, _ = Zygote.gradient(loss_function2, model, t, ps, st)
```

Boom that worked! Let's verify the gradient using forward diff:

```@example nested_ad
∂t_fd = ForwardDiff.gradient(t -> loss_function2(model, t, ps, st), t)
∂ps_fd = ForwardDiff.gradient(ps -> loss_function2(model, t, ps, st), ComponentArray(ps))

println("∞-norm(∂t - ∂t_fd): ", norm(∂t .- ∂t_fd, Inf))
@assert norm(∂t .- ∂t_fd, Inf) < 1e-3 # hide
println("∞-norm(∂ps - ∂ps_fd): ", norm(ComponentArray(∂ps) .- ∂ps_fd, Inf))
@assert norm(ComponentArray(∂ps) .- ∂ps_fd, Inf) < 1e-3 # hide
nothing; # hide
```

## Loss Function computing the Jacobian of the Parameters

The above example shows how to compute the gradient/jacobian wrt the inputs in the loss
function. However, what if we want to compute the jacobian wrt the parameters?
This problem has been taken from [Issue 610](https://github.com/LuxDL/Lux.jl/issues/610).

We resolve these setups by using the `Base.Fix1` wrapper around the stateful layer and
fixing the input to the stateful layer.

```@example nested_ad
function loss_function3(model, x, ps, st)
    smodel = StatefulLuxLayer{true}(model, ps, st)
    J = only(Zygote.jacobian(Base.Fix1(smodel, x), ps)) # Zygote returns a tuple
    return sum(abs2, J)
end

model = Chain(Dense(1 => 12,tanh), Dense(12 => 12,tanh), Dense(12 => 12,tanh),
    Dense(12 => 1))
ps, st = Lux.setup(Xoshiro(0), model)
ps = ComponentArray(ps)  # needs to be an AbstractArray for most jacobian functions
x = rand(Xoshiro(0), Float32, 1, 16)
```

We can as usual compute the gradient/jacobian of the loss function:

```@example nested_ad
_, ∂x, ∂ps, _ = Zygote.gradient(loss_function3, model, x, ps, st)
```

Now let's verify the gradient using forward diff:

```@example nested_ad
∂x_fd = ForwardDiff.gradient(x -> loss_function3(model, x, ps, st), x)
∂ps_fd = ForwardDiff.gradient(ps -> loss_function3(model, x, ps, st), ComponentArray(ps))

println("∞-norm(∂x - ∂x_fd): ", norm(∂x .- ∂x_fd, Inf))
@assert norm(∂x .- ∂x_fd, Inf) < 1e-3 # hide
println("∞-norm(∂ps - ∂ps_fd): ", norm(ComponentArray(∂ps) .- ∂ps_fd, Inf))
@assert norm(ComponentArray(∂ps) .- ∂ps_fd, Inf) < 1e-3 # hide
nothing; # hide
```

## Hutchinson Trace Estimation

Hutchinson Trace Estimation often shows up in machine learning literature to provide a fast
estimate of the trace of a Jacobian Matrix. This is based off of
[Hutchinson 1990](https://www.nowozin.net/sebastian/blog/thoughts-on-trace-estimation-in-deep-learning.html)
which computes the estimated trace of a matrix ``A \in \mathbb{R}^{D \times D}`` using
random vectors ``v \in \mathbb{R}^{D}`` s.t. ``\mathbb{E}\left[v v^T\right] = I``.

```math
\text{Tr}(A) = \mathbb{E}\left[v^T A v\right] = \frac{1}{V} \sum_{i = 1}^V v_i^T A v_i
```

We can use this to compute the trace of a Jacobian Matrix ``J \in \mathbb{R}^{D \times D}``
using the following algorithm:

```math
\text{Tr}(J) = \frac{1}{V} \sum_{i = 1}^V v_i^T J v_i
```

Note that we can compute this using two methods:

1. Compute ``v_i^T J`` using a Vector-Jacobian product and then do a matrix-vector product
   to get the trace.
2. Compute ``J v_i`` using a Jacobian-Vector product and then do a matrix-vector product to
   get the trace.

For simplicity, we will use a single sample of ``v_i`` to compute the trace. Additionally,
we will fix the sample to ensure that our tests against the finite difference implementation
are not affected by the randomness in the sample.

### Computing using the Vector-Jacobian Product

```@example nested_ad
function hutchinson_trace_vjp(model, x, ps, st, v)
    smodel = StatefulLuxLayer{true}(model, ps, st)
    vjp = vector_jacobian_product(smodel, AutoZygote(), x, v)
    return sum(batched_matmul(reshape(vjp, 1, :, size(vjp, ndims(vjp))),
               reshape(v, :, 1, size(v, ndims(v)))))
end
```

This vjp version will be the fastest and most scalable and hence is the recommended way for
computing hutchinson trace.

### Computing using the Jacobian-Vector Product

```@example nested_ad
function hutchinson_trace_jvp(model, x, ps, st, v)
    smodel = StatefulLuxLayer{true}(model, ps, st)
    jvp = jacobian_vector_product(smodel, AutoForwardDiff(), x, v)
    return sum(batched_matmul(reshape(v, 1, :, size(v, ndims(v))),
               reshape(jvp, :, 1, size(jvp, ndims(jvp)))))
end
```

### Computing using the Full Jacobian

This is definitely not recommended, but we are showing it for completeness.

```@example nested_ad
function hutchinson_trace_full_jacobian(model, x, ps, st, v)
    smodel = StatefulLuxLayer{true}(model, ps, st)
    J = ForwardDiff.jacobian(smodel, x)
    return vec(v)' * J * vec(v)
end
```

Now let's compute the trace and compare the results:

```@example nested_ad
model = Chain(Dense(4 => 12,tanh), Dense(12 => 12,tanh), Dense(12 => 12,tanh),
    Dense(12 => 4))
ps, st = Lux.setup(Xoshiro(0), model)
x = rand(Xoshiro(0), Float32, 4, 12)
v = (rand(Xoshiro(12), Float32, 4, 12) .> 0.5f0) * 2.0f0 .- 1.0f0  # rademacher sample
nothing; # hide
```

```@example nested_ad
tr_vjp = hutchinson_trace_vjp(model, x, ps, st, v)
tr_jvp = hutchinson_trace_jvp(model, x, ps, st, v)
tr_full_jacobian = hutchinson_trace_full_jacobian(model, x, ps, st, v)
println("Tr(J) using vjp: ", tr_vjp)
println("Tr(J) using jvp: ", tr_jvp)
println("Tr(J) using full jacobian: ", tr_full_jacobian)
@assert tr_vjp ≈ tr_jvp ≈ tr_full_jacobian # hide
nothing; # hide
```

Now that we have verified that the results are the same, let's try to differentiate the
trace estimate. This often shows up as a regularization term in neural networks.

```@example nested_ad
_, ∂x_vjp, ∂ps_vjp, _, _ = Zygote.gradient(hutchinson_trace_vjp, model, x, ps, st, v)
_, ∂x_jvp, ∂ps_jvp, _, _ = Zygote.gradient(hutchinson_trace_jvp, model, x, ps, st, v)
_, ∂x_full_jacobian, ∂ps_full_jacobian, _, _ = Zygote.gradient(hutchinson_trace_full_jacobian,
    model, x, ps, st, v)
nothing; # hide
```

For sanity check, let's verify that the gradients are the same:

```@example nested_ad
println("∞-norm(∂x using vjp): ", norm(∂x_vjp .- ∂x_jvp, Inf))
println("∞-norm(∂ps using vjp): ",
    norm(ComponentArray(∂ps_vjp) .- ComponentArray(∂ps_jvp), Inf))
println("∞-norm(∂x using full jacobian): ", norm(∂x_full_jacobian .- ∂x_vjp, Inf))
println("∞-norm(∂ps using full jacobian): ",
    norm(ComponentArray(∂ps_full_jacobian) .- ComponentArray(∂ps_vjp), Inf))
@assert norm(∂x_vjp .- ∂x_jvp, Inf) < 1e-3 # hide
@assert norm(ComponentArray(∂ps_vjp) .- ComponentArray(∂ps_jvp), Inf) < 1e-3 # hide
@assert norm(∂x_full_jacobian .- ∂x_vjp, Inf) < 1e-3 # hide
@assert norm(ComponentArray(∂ps_full_jacobian) .- ComponentArray(∂ps_vjp), Inf) < 1e-3 # hide
nothing; # hide
```
