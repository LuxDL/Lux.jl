# [Nested Automatic Differentiation](@id nested_autodiff)

!!! note "Reactant"

    Reactant.jl natively supports nested AD (with orders greater than 2nd order). For more
    robust nested AD, use Lux with Reactant.jl.

In this manual, we will explore how to use automatic differentiation (AD) inside your layers
or loss functions and have Lux automatically switch the AD backend with a faster one when
needed.

!!! tip

    Don't wan't Lux to do this switching for you? You can disable it by setting the
    `automatic_nested_ad_switching` Preference to `false`.

    Remember that if you are using ForwardDiff inside a Zygote call, it will drop gradients
    (with a warning message), so it is not recommended to use this combination.

Let's explore this using some questions that were posted on the
[Julia Discourse forum](https://discourse.julialang.org/).

```@example nested_ad
using ADTypes, Lux, LinearAlgebra, Zygote, ForwardDiff, Random, StableRNGs
using ComponentArrays, FiniteDiff
```

First let's set the stage using some minor changes that need to be made for this feature to
work:

  - Switching only works if a [`StatefulLuxLayer`](@ref) is being used, with the following
    function calls:
    - For operations on the inputs:
      - `(<some-function> ∘ <StatefulLuxLayer>)(x::AbstractArray)`
      - `(<StatefulLuxLayer> ∘ <some-function>)(x::AbstractArray)`
      - `(<StatefulLuxLayer>)(x::AbstractArray)`
    - For operations on the parameters:
      - `(<some-function> ∘ Base.Fix1(<StatefulLuxLayer>, x))(ps)`
      - `(Base.Fix1(<StatefulLuxLayer>, x) ∘ <some-function>)(ps)`
      - `(Base.Fix1(<StatefulLuxLayer>, x))(ps)`
  - Currently we have custom routines implemented for:
    - `Zygote.<gradient|jacobian>`
    - `ForwardDiff.<gradient|jacobian>`
    - [`vector_jacobian_product`](@ref)
    - [`jacobian_vector_product`](@ref)
    - [`batched_jacobian`](@ref)
  - Switching only happens for `ChainRules` compatible AD libraries.

We plan to capture `DifferentiationInterface`, and `Enzyme.autodiff` calls in the
future (PRs are welcome).

!!! tip

    [`@compact`](@ref) uses [`StatefulLuxLayer`](@ref)s internally, so you can directly use
    these features inside a layer generated by [`@compact`](@ref).

## Loss Function containing Jacobian Computation

This problem comes from `@facusapienza` on [Discourse](https://discourse.julialang.org/t/nested-and-different-ad-methods-altogether-how-to-add-ad-calculations-inside-my-loss-function-when-using-neural-differential-equations/108985).
In this case, we want to add a regularization term to the neural DE based on first-order
derivatives. The neural DE part is not important here and we can demonstrate this easily
with a standard neural network.

```@example nested_ad
function loss_function1(model, x, ps, st, y)
    # Make it a stateful layer
    smodel = StatefulLuxLayer(model, ps, st)
    ŷ = smodel(x)
    loss_emp = sum(abs2, ŷ .- y)
    # You can use `Zygote.jacobian` as well but ForwardDiff tends to be more efficient here
    J = ForwardDiff.jacobian(smodel, x)
    loss_reg = abs2(norm(J .* 0.01f0))
    return loss_emp + loss_reg
end

# Using Batchnorm to show that it is possible
model = Chain(Dense(2 => 4, tanh), BatchNorm(4), Dense(4 => 2))
ps, st = Lux.setup(StableRNG(0), model)
x = randn(StableRNG(0), Float32, 2, 10)
y = randn(StableRNG(11), Float32, 2, 10)

loss_function1(model, x, ps, Lux.testmode(st), y)
```

So our loss function works, let's take the gradient (forward diff doesn't nest nicely here):

```@example nested_ad
_, ∂x, ∂ps, _, _ = Zygote.gradient(loss_function1, model, x, ps, st, y)
```

Now let's verify the gradients using finite differences:

```@example nested_ad
∂x_fd = FiniteDiff.finite_difference_gradient(x -> loss_function1(model, x, ps, st, y), x)
∂ps_fd = FiniteDiff.finite_difference_gradient(ps -> loss_function1(model, x, ps, st, y),
    ComponentArray(ps))

println("∞-norm(∂x - ∂x_fd): ", norm(∂x .- ∂x_fd, Inf))
@assert norm(∂x .- ∂x_fd, Inf) < 1e-2 # hide
println("∞-norm(∂ps - ∂ps_fd): ", norm(ComponentArray(∂ps) .- ∂ps_fd, Inf))
@assert norm(ComponentArray(∂ps) .- ∂ps_fd, Inf) < 1e-2 # hide
nothing; # hide
```

That's pretty good, of course you will have some error from the finite differences
calculation.

### Using Batched Jacobian for Multiple Inputs

Notice that in this example the Jacobian `J` consists on the full matrix of derivatives of `smodel` with respect
the different inputs in `x`. In many cases, we are interested in computing the Jacobian with respect to each
input individually, avoiding the unnecessary calculation of zero entries of the Jacobian. This can be achieved with
[`batched_jacobian`](@ref) to parse the calculation of the Jacobian per each single input. Using the same example
from the previous section:

```@example nested_ad
model = Chain(Dense(2 => 4, tanh), Dense(4 => 2))
ps, st = Lux.setup(StableRNG(0), model)
x = randn(StableRNG(0), Float32, 2, 10)
y = randn(StableRNG(11), Float32, 2, 10)

function loss_function_batched(model, x, ps, st, y)
    # Make it a stateful layer
    smodel = StatefulLuxLayer(model, ps, st)
    ŷ = smodel(x)
    loss_emp = sum(abs2, ŷ .- y)
    # You can use `AutoZygote()` as well but `AutoForwardDiff()` tends to be more efficient here
    J = batched_jacobian(smodel, AutoForwardDiff(), x)
    loss_reg = abs2(norm(J .* 0.01f0))
    return loss_emp + loss_reg
end

loss_function_batched(model, x, ps, st, y)
```

Notice that in this last example we removed `BatchNorm()` from the neural network. This is done so outputs corresponding
to different inputs don't have an algebraic dependency due to the batch normalization happening in the neural network.
We can now verify again the value of the Jacobian:

```@example nested_ad
∂x_fd = FiniteDiff.finite_difference_gradient(x -> loss_function_batched(model, x, ps, st, y), x)
∂ps_fd = FiniteDiff.finite_difference_gradient(ps -> loss_function_batched(model, x, ps, st, y),
    ComponentArray(ps))

_, ∂x_b, ∂ps_b, _, _ = Zygote.gradient(loss_function_batched, model, x, ps, st, y)
println("∞-norm(∂x_b - ∂x_fd): ", norm(∂x_b .- ∂x_fd, Inf))
@assert norm(∂x_b .- ∂x_fd, Inf) < 1e-2 # hide
println("∞-norm(∂ps_b - ∂ps_fd): ", norm(ComponentArray(∂ps_b) .- ∂ps_fd, Inf))
@assert norm(ComponentArray(∂ps_b) .- ∂ps_fd, Inf) < 1e-2 # hide
```

In this example, it is important to remark that now `batched_jacobian` returns a 3D array with the Jacobian calculation
for each independent input value in `x`.

## Loss Function contains Gradient Computation

Ok here I am going to cheat a bit. This comes from a discussion on nested AD for PINNs
on [Discourse](https://discourse.julialang.org/t/is-it-possible-to-do-nested-ad-elegantly-in-julia-pinns/98888/21).
As the consensus there, we shouldn't use nested AD for 3rd or higher order differentiation.
Note that in the example there, the user uses `ForwardDiff.derivative` but we will use
`ForwardDiff.gradient` instead, as we typically deal with array inputs and outputs.

```@example nested_ad
function loss_function2(model, t, ps, st)
    smodel = StatefulLuxLayer(model, ps, st)
    ŷ = only(Zygote.gradient(Base.Fix1(sum, abs2) ∘ smodel, t)) # Zygote returns a tuple
    return sum(abs2, ŷ .- cos.(t))
end

model = Chain(Dense(1 => 12,tanh), Dense(12 => 12,tanh), Dense(12 => 12,tanh),
    Dense(12 => 1))
ps, st = Lux.setup(StableRNG(0), model)
t = rand(StableRNG(0), Float32, 1, 16)
```

Now the moment of truth:

```@example nested_ad
_, ∂t, ∂ps, _ = Zygote.gradient(loss_function2, model, t, ps, st)
```

Boom that worked! Let's verify the gradient using forward diff:

```@example nested_ad
∂t_fd = ForwardDiff.gradient(t -> loss_function2(model, t, ps, st), t)
∂ps_fd = ForwardDiff.gradient(ps -> loss_function2(model, t, ps, st), ComponentArray(ps))

println("∞-norm(∂t - ∂t_fd): ", norm(∂t .- ∂t_fd, Inf))
@assert norm(∂t .- ∂t_fd, Inf) < 1e-3 # hide
println("∞-norm(∂ps - ∂ps_fd): ", norm(ComponentArray(∂ps) .- ∂ps_fd, Inf))
@assert norm(ComponentArray(∂ps) .- ∂ps_fd, Inf) < 1e-3 # hide
nothing; # hide
```

## Loss Function computing the Jacobian of the Parameters

The above example shows how to compute the gradient/jacobian wrt the inputs in the loss
function. However, what if we want to compute the jacobian wrt the parameters?
This problem has been taken from [Issue 610](https://github.com/LuxDL/Lux.jl/issues/610).

We resolve these setups by using the `Base.Fix1` wrapper around the stateful layer and
fixing the input to the stateful layer.

```@example nested_ad
function loss_function3(model, x, ps, st)
    smodel = StatefulLuxLayer(model, ps, st)
    J = only(Zygote.jacobian(Base.Fix1(smodel, x), ps)) # Zygote returns a tuple
    return sum(abs2, J)
end

model = Chain(Dense(1 => 12,tanh), Dense(12 => 12,tanh), Dense(12 => 12,tanh),
    Dense(12 => 1))
ps, st = Lux.setup(StableRNG(0), model)
ps = ComponentArray(ps)  # needs to be an AbstractArray for most jacobian functions
x = rand(StableRNG(0), Float32, 1, 16)
```

We can as usual compute the gradient/jacobian of the loss function:

```@example nested_ad
_, ∂x, ∂ps, _ = Zygote.gradient(loss_function3, model, x, ps, st)
```

Now let's verify the gradient using forward diff:

```@example nested_ad
∂x_fd = ForwardDiff.gradient(x -> loss_function3(model, x, ps, st), x)
∂ps_fd = ForwardDiff.gradient(ps -> loss_function3(model, x, ps, st), ComponentArray(ps))

println("∞-norm(∂x - ∂x_fd): ", norm(∂x .- ∂x_fd, Inf))
@assert norm(∂x .- ∂x_fd, Inf) < 1e-3 # hide
println("∞-norm(∂ps - ∂ps_fd): ", norm(ComponentArray(∂ps) .- ∂ps_fd, Inf))
@assert norm(ComponentArray(∂ps) .- ∂ps_fd, Inf) < 1e-3 # hide
nothing; # hide
```

## Hutchinson Trace Estimation

Hutchinson Trace Estimation often shows up in machine learning literature to provide a fast
estimate of the trace of a Jacobian Matrix. This is based off of
[Hutchinson 1990](https://www.nowozin.net/sebastian/blog/thoughts-on-trace-estimation-in-deep-learning.html)
which computes the estimated trace of a matrix ``A \in \mathbb{R}^{D \times D}`` using
random vectors ``v \in \mathbb{R}^{D}`` s.t. ``\mathbb{E}\left[v v^T\right] = I``.

```math
\text{Tr}(A) = \mathbb{E}\left[v^T A v\right] = \frac{1}{V} \sum_{i = 1}^V v_i^T A v_i
```

We can use this to compute the trace of a Jacobian Matrix ``J \in \mathbb{R}^{D \times D}``
using the following algorithm:

```math
\text{Tr}(J) = \frac{1}{V} \sum_{i = 1}^V v_i^T J v_i
```

Note that we can compute this using two methods:

1. Compute ``v_i^T J`` using a Vector-Jacobian product and then do a matrix-vector product
   to get the trace.
2. Compute ``J v_i`` using a Jacobian-Vector product and then do a matrix-vector product to
   get the trace.

For simplicity, we will use a single sample of ``v_i`` to compute the trace. Additionally,
we will fix the sample to ensure that our tests against the finite difference implementation
are not affected by the randomness in the sample.

### Computing using the Vector-Jacobian Product

```@example nested_ad
function hutchinson_trace_vjp(model, x, ps, st, v)
    smodel = StatefulLuxLayer(model, ps, st)
    vjp = vector_jacobian_product(smodel, AutoZygote(), x, v)
    return sum(batched_matmul(reshape(vjp, 1, :, size(vjp, ndims(vjp))),
               reshape(v, :, 1, size(v, ndims(v)))))
end
```

This vjp version will be the fastest and most scalable and hence is the recommended way for
computing hutchinson trace.

### Computing using the Jacobian-Vector Product

```@example nested_ad
function hutchinson_trace_jvp(model, x, ps, st, v)
    smodel = StatefulLuxLayer(model, ps, st)
    jvp = jacobian_vector_product(smodel, AutoForwardDiff(), x, v)
    return sum(batched_matmul(reshape(v, 1, :, size(v, ndims(v))),
               reshape(jvp, :, 1, size(jvp, ndims(jvp)))))
end
```

### Computing using the Full Jacobian

This is definitely not recommended, but we are showing it for completeness.

```@example nested_ad
function hutchinson_trace_full_jacobian(model, x, ps, st, v)
    smodel = StatefulLuxLayer(model, ps, st)
    J = ForwardDiff.jacobian(smodel, x)
    return vec(v)' * J * vec(v)
end
```

Now let's compute the trace and compare the results:

```@example nested_ad
model = Chain(Dense(4 => 12,tanh), Dense(12 => 12,tanh), Dense(12 => 12,tanh),
    Dense(12 => 4))
ps, st = Lux.setup(StableRNG(0), model)
x = rand(StableRNG(0), Float32, 4, 12)
v = (rand(StableRNG(12), Float32, 4, 12) .> 0.5f0) * 2.0f0 .- 1.0f0  # rademacher sample
nothing; # hide
```

```@example nested_ad
tr_vjp = hutchinson_trace_vjp(model, x, ps, st, v)
tr_jvp = hutchinson_trace_jvp(model, x, ps, st, v)
tr_full_jacobian = hutchinson_trace_full_jacobian(model, x, ps, st, v)
println("Tr(J) using vjp: ", tr_vjp)
println("Tr(J) using jvp: ", tr_jvp)
println("Tr(J) using full jacobian: ", tr_full_jacobian)
@assert tr_vjp ≈ tr_jvp ≈ tr_full_jacobian # hide
nothing; # hide
```

Now that we have verified that the results are the same, let's try to differentiate the
trace estimate. This often shows up as a regularization term in neural networks.

```@example nested_ad
_, ∂x_vjp, ∂ps_vjp, _, _ = Zygote.gradient(hutchinson_trace_vjp, model, x, ps, st, v)
_, ∂x_jvp, ∂ps_jvp, _, _ = Zygote.gradient(hutchinson_trace_jvp, model, x, ps, st, v)
_, ∂x_full_jacobian, ∂ps_full_jacobian, _, _ = Zygote.gradient(hutchinson_trace_full_jacobian,
    model, x, ps, st, v)
nothing; # hide
```

For sanity check, let's verify that the gradients are the same:

```@example nested_ad
println("∞-norm(∂x using vjp): ", norm(∂x_vjp .- ∂x_jvp, Inf))
println("∞-norm(∂ps using vjp): ",
    norm(ComponentArray(∂ps_vjp) .- ComponentArray(∂ps_jvp), Inf))
println("∞-norm(∂x using full jacobian): ", norm(∂x_full_jacobian .- ∂x_vjp, Inf))
println("∞-norm(∂ps using full jacobian): ",
    norm(ComponentArray(∂ps_full_jacobian) .- ComponentArray(∂ps_vjp), Inf))
@assert norm(∂x_vjp .- ∂x_jvp, Inf) < 1e-3 # hide
@assert norm(ComponentArray(∂ps_vjp) .- ComponentArray(∂ps_jvp), Inf) < 1e-3 # hide
@assert norm(∂x_full_jacobian .- ∂x_vjp, Inf) < 1e-3 # hide
@assert norm(ComponentArray(∂ps_full_jacobian) .- ComponentArray(∂ps_vjp), Inf) < 1e-3 # hide
nothing; # hide
```
