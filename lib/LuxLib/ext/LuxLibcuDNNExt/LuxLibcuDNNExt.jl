module LuxLibcuDNNExt

using LuxLib: LuxLib, Optional, âˆ‚âˆ…, Impl, Utils
using CUDA: CUDA, CuArray, CuVector, CU_NULL, DenseCuArray, DenseCuVector
using ChainRulesCore: ChainRulesCore
using cuDNN: cuDNN, cudnnBatchNormalizationBackward,
             cudnnBatchNormalizationForwardInference, CUDNN_BATCHNORM_SPATIAL,
             cudnnBatchNormalizationForwardTraining, cudnnTensorDescriptor,
             CUDNN_TENSOR_NCHW, cudnnDataType
using FastClosures: @closure
using Static: StaticBool

const CRC = ChainRulesCore

const cuDNNFloat = Union{Float32, Float64}

include("batchnorm.jl")

# api/batchnorm.jl
const CUDNN_BN_ARRAY_TYPE = Union{
    CuArray{<:cuDNNFloat, 2}, CuArray{<:cuDNNFloat, 4}, CuArray{<:cuDNNFloat, 5}}
const BNParamType = Optional{<:CuVector{<:cuDNNFloat}}

function Impl.batchnorm(
        x::CUDNN_BN_ARRAY_TYPE, Î³::BNParamType, Î²::BNParamType, rÎ¼::BNParamType,
        rÏƒÂ²::BNParamType, training::StaticBool, Ïƒ::F, m::Real, Ïµ::Real) where {F}
    rÎ¼â‚™, rÏƒÂ²â‚™ = Impl.get_batchnorm_statistics(x, rÎ¼, rÏƒÂ², training)
    y = Impl.batchnorm_cudnn(Î³, Î², x, rÎ¼â‚™, rÏƒÂ²â‚™, m, Ïµ, training)[1]
    return Impl.activation!!(Ïƒ, y), vec(rÎ¼â‚™), vec(rÏƒÂ²â‚™)
end

function CRC.rrule(
        ::typeof(Impl.batchnorm_cudnn), Î³, Î², x, rÎ¼, rÏƒÂ², m, Ïµ, training::StaticBool)
    # TODO: Transition this to an error in the future
    Utils.known(training) || @warn "`training=Val(false)` but gradient was called." maxlog=1
    y, xÎ¼, xÏƒâ»Â² = Impl.batchnorm_cudnn(Î³, Î², x, rÎ¼, rÏƒÂ², m, Ïµ, training)
    ğ’«x, ğ’«Î³, ğ’«Î² = CRC.ProjectTo(x), CRC.ProjectTo(Î³), CRC.ProjectTo(Î²)
    âˆ‡batchnorm_cudnn = @closure Î” -> begin
        âˆ‚Î³, âˆ‚Î², âˆ‚x = Impl.âˆ‡batchnorm_cudnn(
            Î³, Î², x, CRC.unthunk(first(Î”)), rÎ¼, rÏƒÂ², xÎ¼, xÏƒâ»Â², Ïµ)
        return âˆ‚âˆ…, ğ’«Î³(âˆ‚Î³), ğ’«Î²(âˆ‚Î²), ğ’«x(âˆ‚x), âˆ‚âˆ…, âˆ‚âˆ…, âˆ‚âˆ…, âˆ‚âˆ…, âˆ‚âˆ…
    end
    return (y, xÎ¼, xÏƒâ»Â²), âˆ‡batchnorm_cudnn
end

end
