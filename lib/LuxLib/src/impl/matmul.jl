# Wrappers over Base & LinearAlgebra implementations to use poly algs if needed
matmuladd(A, B, ::Nothing) = matmul(A, B)
function matmuladd(A::AbstractMatrix, B::AbstractVector, bias::AbstractVector)
    return matmuladd(A, get_utils(:insert_batch_dim)(B), bias)
end
function matmuladd(A::AbstractMatrix, B::AbstractMatrix, bias::AbstractVector)
    return matmuladd(internal_operation_mode((A, B, bias)), A, B, bias)
end

function matmuladd(
        ::GenericBroadcastOp, A::AbstractMatrix, B::AbstractMatrix, bias::AbstractVector)
    return muladd(A, B, bias)
end
function matmuladd(opmode::AbstractInternalArrayOpMode, A::AbstractMatrix,
        B::AbstractMatrix, bias::AbstractVector)
    if size(A, 2) != size(B, 1)
        throw(DimensionMismatch(lazy"A has shape ($(size(A, 1)), $(size(A, 2))) but B has shape ($(size(B, 1)), $(size(B, 2)))"))
    end
    if length(bias) != size(A, 1)
        throw(DimensionMismatch(lazy"bias has length $(length(bias)) but A has shape ($(size(A, 1)), $(size(A, 2)))"))
    end
    C = similar(A, promote_type(eltype(A), eltype(B), eltype(bias)), size(A, 1), size(B, 2))
    matmuladd!(C, opmode, A, B, bias)
    return C
end

function matmul(A::AbstractMatrix, B::AbstractVector)
    return vec(matmul(A, get_utils(:insert_batch_dim)(B)))
end
function matmul(A::AbstractMatrix, B::AbstractMatrix)
    if size(A, 2) != size(B, 1)
        throw(DimensionMismatch(lazy"A has shape ($(size(A, 1)), $(size(A, 2))) but B has shape ($(size(B, 1)), $(size(B, 2)))"))
    end
    return matmul(internal_operation_mode((A, B)), A, B)
end

matmul(::GenericBroadcastOp, A::AbstractMatrix, B::AbstractMatrix) = A * B
function matmul(::AbstractInternalArrayOpMode, A::AbstractMatrix, B::AbstractMatrix)
    C = similar(A, promote_type(eltype(A), eltype(B)), size(A, 1), size(B, 2))
    matmul!(C, A, B)
    return C
end

# Slightly higher level. Here we make decisions about which implementation to use
function matmuladd!(C::AbstractMatrix, A::AbstractMatrix, B::AbstractMatrix, ::Nothing)
    matmul!(C, A, B)
    return
end
function matmuladd!(
        C::AbstractMatrix, A::AbstractMatrix, B::AbstractMatrix, bias::AbstractVector)
    matmuladd!(C, internal_operation_mode((C, A, B, bias)), A, B, bias)
    return
end

function matmuladd!(C::AbstractMatrix, ::AbstractInternalArrayOpMode,
        A::AbstractMatrix, B::AbstractMatrix, bias::AbstractVector)
    C .= bias
    mul!(C, A, B, true, true)
    return
end

function matmuladd!(C::AbstractMatrix, ::GPUBroadcastOp{CUDADevice},
        A::AbstractMatrix, B::AbstractMatrix, bias::AbstractVector)
    cublasLt_fused_dense!(C, identity, A, B, bias)
    return
end

function matmuladd!(C::AbstractMatrix, ::LoopedArrayOp, A::AbstractMatrix,
        B::AbstractMatrix, bias::AbstractVector)
    if LV.check_args(C, A, B, bias) && System.fits_in_l2cache(C, A, B, bias)
        matmuladd_loopvec!(C, A, B, bias)
        return
    end
    matmuladd_cpu_fallback!(C, A, B, bias)
    return
end

function matmul!(C::AbstractMatrix, A::AbstractMatrix, B::AbstractMatrix)
    matmul!(C, internal_operation_mode((C, A, B)), A, B)
    return
end

function matmul!(C::AbstractMatrix, ::AbstractInternalArrayOpMode,
        A::AbstractMatrix, B::AbstractMatrix)
    mul!(C, A, B)
    return
end

function matmul!(C::AbstractMatrix, ::LoopedArrayOp, A::AbstractMatrix, B::AbstractMatrix)
    return matmul_cpu!(C, System.use_octavian(), System.explicit_blas_loaded(), A, B)
end

for spl_blas in (True, False)
    @eval begin
        function matmul_cpu!( # Octavian can be used
                C::AbstractMatrix, ::True, ::$(spl_blas),
                A::AbstractMatrix, B::AbstractMatrix)
            if LV.check_args(C, A, B)
                if System.fits_in_l1cache(C, A, B)
                    matmul_loopvec!(C, A, B, true, false)
                    return
                elseif $(Utils.known(spl_blas()) ? System.fits_in_l2cache :
                         System.fits_in_l3cache)(C, A, B)
                    matmul_octavian!(C, A, B, true, false)
                    return
                end
            end
            matmul_cpu_fallback!(C, A, B, true, false)
            return
        end

        function matmul_cpu!( # Octavian cannot be used
                C::AbstractMatrix, ::False, ::$(spl_blas),
                A::AbstractMatrix, B::AbstractMatrix)
            if LV.check_args(C, A, B)
                if $(Utils.known(spl_blas()) ? System.fits_in_l1cache :
                     System.fits_in_l2cache)(C, A, B)
                    matmul_loopvec!(C, A, B, true, false)
                    return
                end
            end
            matmul_cpu_fallback!(C, A, B, true, false)
            return
        end
    end
end

# Low-Level Matmul implementations -- Either call libraries or implement our own
# We force inlining here to avoid allocations in the inner loops
@inline function matmul_octavian!(
        C::AbstractMatrix, A::AbstractMatrix, B::AbstractMatrix, Î±::Number, Î²::Number)
    Octavian.matmul!(C, A, B, Î±, Î²)
    return
end

# Best case fallback, we are likely going to hit BLAS
@inline function matmul_cpu_fallback!(C::AbstractMatrix{T}, A::AbstractMatrix{T},
        B::AbstractMatrix{T}, Î±::Number, Î²::Number) where {T}
    matmul_linalg_default!(C, A, B, Î±, Î²)
    return
end

@inline function matmul_cpu_fallback!(C::AbstractMatrix{T}, A::AbstractMatrix{AT},
        B::AbstractMatrix{BT}, Î±::Number, Î²::Number) where {T, AT, BT}
    if LV.check_args(C, A, B)  # Use Octavian if possible. Don't check via `use_octavian()`
        matmul_octavian!(C, A, B, Î±, Î²)
        return
    end
    # Generic fallback is actually quite good starting julia 1.11
    @static if VERSION â‰¥ v"1.11-"
        @warn lazy"Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [$(typeof(C))]: A [$(typeof(A))] x B [$(typeof(B))]). Falling back to generic implementation. This may be slow." maxlog=1
        Aâ€², Bâ€² = A, B
    else
        @warn lazy"Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [$(typeof(C))]: A [$(typeof(A))] x B [$(typeof(B))]). Converting to common type to to attempt to use BLAS. This may be slow." maxlog=1
        Aâ€², Bâ€² = Utils.ofeltype_array(T, A), Utils.ofeltype_array(T, B)
    end
    matmul_linalg_default!(C, Aâ€², Bâ€², Î±, Î²)
    return
end

@inline function matmul_linalg_default!(
        C::AbstractMatrix, A::AbstractMatrix, B::AbstractMatrix, Î±::Number, Î²::Number)
    mul!(C, A, B, Î±, Î²)
    return
end

for serial in (true, false)
    opname = serial ? :serial_matmul_loopvec! : :matmul_loopvec!
    @eval @inline function $opname(
            C::AbstractMatrix, A::AbstractMatrix, B::AbstractMatrix, Î±::Number, Î²::Number)
        if !iszero(Î²) # Secial case this because Base.FastMath.mul_fast(NaN, false) = NaN
            @turbo thread=$(!serial) for K in indices((C, B), 2), J in indices((C, A), 1)
                Câ±¼â‚– = zero(eltype(C))
                for I in indices((A, B), (2, 1))
                    Câ±¼â‚– += A[J, I] * B[I, K]
                end
                C[J, K] = Î± * Câ±¼â‚– + Î² * C[J, K]
            end
        else
            @turbo thread=$(!serial) for K in indices((C, B), 2), J in indices((C, A), 1)
                Câ±¼â‚– = zero(eltype(C))
                for I in indices((A, B), (2, 1))
                    Câ±¼â‚– += A[J, I] * B[I, K]
                end
                C[J, K] = Î± * Câ±¼â‚–
            end
        end
    end
end

@inline function matmuladd_loopvec!(
        C::AbstractMatrix, A::AbstractMatrix, B::AbstractMatrix, bias::AbstractVector)
    @tturbo for K in indices((C, B), 2), J in indices((C, A), 1)
        Câ±¼â‚– = zero(eltype(C))
        for I in indices((A, B), (2, 1))
            Câ±¼â‚– += A[J, I] * B[I, K]
        end
        C[J, K] = bias[J] + Câ±¼â‚–
    end
    return
end

@inline function matmuladd_cpu_fallback!(
        C::AbstractMatrix, A::AbstractMatrix, B::AbstractMatrix, bias::AbstractVector)
    C .= bias
    matmul_cpu_fallback!(C, A, B, true, true)
    return
end

# ChainRules
function CRC.rrule(::typeof(matmul), A::AbstractMatrix, B::AbstractMatrix)
    ğ’«A, ğ’«B = CRC.ProjectTo(A), CRC.ProjectTo(B)
    âˆ‡matmul = @closure Î”â€² -> begin
        Î” = CRC.unthunk(Î”â€²)
        âˆ‚A = CRC.@thunk(ğ’«A(matmul(Î”, B')))
        âˆ‚B = CRC.@thunk(ğ’«B(matmul(A', Î”)))
        return âˆ‚âˆ…, âˆ‚A, âˆ‚B
    end
    return matmul(A, B), âˆ‡matmul
end

function CRC.rrule(
        ::typeof(matmuladd), A::AbstractMatrix, B::AbstractMatrix, bias::AbstractVector)
    ğ’«A, ğ’«B, ğ’«bias = CRC.ProjectTo(A), CRC.ProjectTo(B), CRC.ProjectTo(bias)
    âˆ‡matmuladd = @closure Î”â€² -> begin
        Î” = CRC.unthunk(Î”â€²)
        âˆ‚A = CRC.@thunk(ğ’«A(matmul(Î”, B')))
        âˆ‚B = CRC.@thunk(ğ’«B(matmul(A', Î”)))
        âˆ‚bias = CRC.@thunk(ğ’«bias(âˆ‡bias_add(bias, Î”)))
        return âˆ‚âˆ…, âˆ‚A, âˆ‚B, âˆ‚bias
    end
    return matmuladd(A, B, bias), âˆ‡matmuladd
end

# EnzymeRules
## ReverseMode
Utils.@enzyme_alternative matmul_octavian! matmul_linalg_default!
Utils.@enzyme_alternative serial_matmul_loopvec! matmul_linalg_default!
Utils.@enzyme_alternative matmul_loopvec! matmul_linalg_default!

Utils.@enzyme_alternative matmuladd_loopvec! matmuladd_cpu_fallback!

## ForwardMode
# NOTE: forward mode works fine with LoopVectorization but not with Octavian
function EnzymeRules.forward(
        ::EnzymeCore.Const{typeof(matmul_octavian!)}, ::Type{RT}, args...) where {RT}
    return EnzymeCore.autodiff(
        EnzymeCore.Forward, EnzymeCore.Const(matmul_linalg_default!), RT, args...)
end
