import{_ as n,c as a,o as i,al as r}from"./chunks/framework.CH1Y2M5m.js";const f=JSON.parse('{"title":"References","description":"","frontmatter":{},"headers":[],"relativePath":"references.md","filePath":"references.md","lastUpdated":null}'),o={name:"references.md"};function t(l,e,s,m,p,c){return i(),a("div",null,[...e[0]||(e[0]=[r('<h1 id="References" tabindex="-1">References <a class="header-anchor" href="#References" aria-label="Permalink to &quot;References {#References}&quot;">​</a></h1><ol><li><p><a id="vaswani2017attention"></a> A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser and I. Polosukhin. <em>Attention is all you need</em>. Advances in neural information processing systems <strong>30</strong> (2017).</p></li><li><p><a id="su2024roformer"></a> J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo and Y. Liu. <em>Roformer: Enhanced transformer with rotary position embedding</em>. Neurocomputing <strong>568</strong>, 127063 (2024).</p></li><li><p><a id="goodfellow2013maxout"></a> I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville and Y. Bengio. <em>Maxout networks</em>. In: <em>International conference on machine learning</em> (PMLR, 2013); pp. 1319–1327.</p></li><li><p><a id="ulyanov2016instance"></a> D. Ulyanov, A. Vedaldi and V. Lempitsky. <em>Instance normalization: The missing ingredient for fast stylization</em>, arXiv preprint arXiv:1607.08022 (2016).</p></li><li><p><a id="lin2017focal"></a> T.-Y. Lin, P. Goyal, R. Girshick, K. He and P. Dollár. <em>Focal loss for dense object detection</em>. In: <em>Proceedings of the IEEE international conference on computer vision</em> (2017); pp. 2980–2988.</p></li><li><p><a id="milletari2016v"></a> F. Milletari, N. Navab and S.-A. Ahmadi. <em>V-net: Fully convolutional neural networks for volumetric medical image segmentation</em>. In: <em>2016 fourth international conference on 3D vision (3DV)</em> (Ieee, 2016); pp. 565–571.</p></li><li><p><a id="hadsell2006dimensionality"></a> R. Hadsell, S. Chopra and Y. LeCun. <em>Dimensionality reduction by learning an invariant mapping</em>. In: <em>2006 IEEE computer society conference on computer vision and pattern recognition (CVPR&#39;06)</em>, Vol. 2 (IEEE, 2006); pp. 1735–1742.</p></li><li><p><a id="klambauer2017self"></a> G. Klambauer, T. Unterthiner, A. Mayr and S. Hochreiter. <em>Self-normalizing neural networks</em>. Advances in neural information processing systems <strong>30</strong> (2017).</p></li><li><p><a id="srivastava2014dropout"></a> N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and R. Salakhutdinov. <em>Dropout: a simple way to prevent neural networks from overfitting</em>. The journal of machine learning research <strong>15</strong>, 1929–1958 (2014).</p></li><li><p><a id="ioffe2015batch"></a> S. Ioffe and C. Szegedy. <em>Batch normalization: Accelerating deep network training by reducing internal covariate shift</em>. In: <em>International conference on machine learning</em> (pmlr, 2015); pp. 448–456.</p></li><li><p><a id="wu2018group"></a> Y. Wu and K. He. <em>Group normalization</em>. In: <em>Proceedings of the European conference on computer vision (ECCV)</em> (2018); pp. 3–19.</p></li><li><p><a id="ba2016layer"></a> J. L. Ba, J. R. Kiros and G. E. Hinton. <em>Layer normalization</em>, arXiv preprint arXiv:1607.06450 (2016).</p></li></ol>',2)])])}const u=n(o,[["render",t]]);export{f as __pageData,u as default};
