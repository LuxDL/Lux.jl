{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Normalizing Flows for Density Estimation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial demonstrates how to use Lux to train a\n",
    "[RealNVP](https://arxiv.org/abs/1605.08803). This is based on the\n",
    "[RealNVP implementation in MLX](https://github.com/ml-explore/mlx-examples/blob/main/normalizing_flow/)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Lux,\n",
    "    Reactant,\n",
    "    Random,\n",
    "    Statistics,\n",
    "    Enzyme,\n",
    "    MLUtils,\n",
    "    ConcreteStructs,\n",
    "    Printf,\n",
    "    Optimisers,\n",
    "    CairoMakie\n",
    "\n",
    "const xdev = reactant_device(; force=true)\n",
    "const cdev = cpu_device()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define & Load the Moons Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define a function to generate data from the moons dataset. We use the code here from\n",
    "[this tutorial](https://liorsinai.github.io/machine-learning/2024/08/19/micrograd-5-mlp.html#moons-dataset)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function make_moons(\n",
    "    rng::AbstractRNG,\n",
    "    ::Type{T},\n",
    "    n_samples::Int=100;\n",
    "    noise::Union{Nothing,AbstractFloat}=nothing,\n",
    ") where {T}\n",
    "    n_moons = n_samples ÷ 2\n",
    "    t_min, t_max = T(0), T(π)\n",
    "    t_inner = rand(rng, T, n_moons) * (t_max - t_min) .+ t_min\n",
    "    t_outer = rand(rng, T, n_moons) * (t_max - t_min) .+ t_min\n",
    "    outer_circ_x = cos.(t_outer)\n",
    "    outer_circ_y = sin.(t_outer) .+ T(1)\n",
    "    inner_circ_x = 1 .- cos.(t_inner)\n",
    "    inner_circ_y = 1 .- sin.(t_inner) .- T(1)\n",
    "\n",
    "    data = [outer_circ_x outer_circ_y; inner_circ_x inner_circ_y]\n",
    "    z = permutedims(data, (2, 1))\n",
    "    noise !== nothing && (z .+= T(noise) * randn(rng, T, size(z)))\n",
    "    return z\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's visualize the dataset"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fig = Figure()\n",
    "ax = CairoMakie.Axis(fig[1, 1]; xlabel=\"x\", ylabel=\"y\")\n",
    "\n",
    "z = make_moons(Random.default_rng(), Float32, 10_000; noise=0.1)\n",
    "scatter!(ax, z[1, :], z[2, :]; markersize=2)\n",
    "\n",
    "fig"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function load_moons_dataloader(\n",
    "    args...; batchsize::Int, noise::Union{Nothing,AbstractFloat}=nothing, kwargs...\n",
    ")\n",
    "    return DataLoader(\n",
    "        make_moons(args...; noise); batchsize, shuffle=true, partial=false, kwargs...\n",
    "    )\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bijectors Implementation"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "abstract type AbstractBijector end\n",
    "\n",
    "@concrete struct AffineBijector <: AbstractBijector\n",
    "    shift <: AbstractArray\n",
    "    log_scale <: AbstractArray\n",
    "end\n",
    "\n",
    "function AffineBijector(shift_and_log_scale::AbstractArray{T,N}) where {T,N}\n",
    "    n = size(shift_and_log_scale, 1) ÷ 2\n",
    "    idxs = ntuple(Returns(Colon()), N - 1)\n",
    "    return AffineBijector(\n",
    "        shift_and_log_scale[1:n, idxs...], shift_and_log_scale[(n + 1):end, idxs...]\n",
    "    )\n",
    "end\n",
    "\n",
    "function forward_and_log_det(bj::AffineBijector, x::AbstractArray)\n",
    "    y = x .* exp.(bj.log_scale) .+ bj.shift\n",
    "    return y, bj.log_scale\n",
    "end\n",
    "\n",
    "function inverse_and_log_det(bj::AffineBijector, y::AbstractArray)\n",
    "    x = (y .- bj.shift) ./ exp.(bj.log_scale)\n",
    "    return x, -bj.log_scale\n",
    "end\n",
    "\n",
    "@concrete struct MaskedCoupling <: AbstractBijector\n",
    "    mask <: AbstractArray\n",
    "    conditioner\n",
    "    bijector\n",
    "end\n",
    "\n",
    "function apply_mask(bj::MaskedCoupling, x::AbstractArray, fn::F) where {F}\n",
    "    x_masked = x .* (1 .- bj.mask)\n",
    "    bijector_params = bj.conditioner(x_masked)\n",
    "    y, log_det = fn(bijector_params)\n",
    "    log_det = log_det .* bj.mask\n",
    "    y = ifelse.(bj.mask, y, x)\n",
    "    return y, dsum(log_det; dims=Tuple(collect(1:(ndims(x) - 1))))\n",
    "end\n",
    "\n",
    "function forward_and_log_det(bj::MaskedCoupling, x::AbstractArray)\n",
    "    return apply_mask(bj, x, params -> forward_and_log_det(bj.bijector(params), x))\n",
    "end\n",
    "\n",
    "function inverse_and_log_det(bj::MaskedCoupling, y::AbstractArray)\n",
    "    return apply_mask(bj, y, params -> inverse_and_log_det(bj.bijector(params), y))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Definition"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function MLP(in_dims::Int, hidden_dims::Int, out_dims::Int, n_layers::Int; activation=gelu)\n",
    "    return Chain(\n",
    "        Dense(in_dims => hidden_dims, activation),\n",
    "        [Dense(hidden_dims => hidden_dims, activation) for _ in 1:(n_layers - 1)]...,\n",
    "        Dense(hidden_dims => out_dims),\n",
    "    )\n",
    "end\n",
    "\n",
    "@concrete struct RealNVP <: AbstractLuxContainerLayer{(:conditioners,)}\n",
    "    conditioners\n",
    "    dist_dims::Int\n",
    "    n_transforms::Int\n",
    "end\n",
    "\n",
    "const StatefulRealNVP{M} = StatefulLuxLayer{M,<:RealNVP}\n",
    "\n",
    "function Lux.initialstates(rng::AbstractRNG, l::RealNVP)\n",
    "    mask_list = Vector{Bool}[\n",
    "        collect(1:(l.dist_dims)) .% 2 .== i % 2 for i in 1:(l.n_transforms)\n",
    "    ]\n",
    "    return (; mask_list, conditioners=Lux.initialstates(rng, l.conditioners))\n",
    "end\n",
    "\n",
    "function RealNVP(; n_transforms::Int, dist_dims::Int, hidden_dims::Int, n_layers::Int)\n",
    "    conditioners = [\n",
    "        MLP(dist_dims, hidden_dims, 2 * dist_dims, n_layers; activation=gelu) for\n",
    "        _ in 1:n_transforms\n",
    "    ]\n",
    "    conditioners = NamedTuple{ntuple(Base.Fix1(Symbol, :conditioners_), n_transforms)}(\n",
    "        Tuple(conditioners)\n",
    "    )\n",
    "    return RealNVP(conditioners, dist_dims, n_transforms)\n",
    "end\n",
    "\n",
    "log_prob(x::AbstractArray{T}) where {T} = -T(0.5 * log(2π)) .- T(0.5) .* abs2.(x)\n",
    "\n",
    "function log_prob(l::StatefulRealNVP, x::AbstractArray{T}) where {T}\n",
    "    smodels = [\n",
    "        StatefulLuxLayer(conditioner, l.ps.conditioners[i], l.st.conditioners[i]) for\n",
    "        (i, conditioner) in enumerate(l.model.conditioners)\n",
    "    ]\n",
    "\n",
    "    lprob = zeros_like(x, size(x, ndims(x)))\n",
    "    for (mask, conditioner) in Iterators.reverse(zip(l.st.mask_list, smodels))\n",
    "        bj = MaskedCoupling(mask, conditioner, AffineBijector)\n",
    "        x, log_det = inverse_and_log_det(bj, x)\n",
    "        lprob += log_det\n",
    "    end\n",
    "    lprob += dsum(log_prob(x); dims=Tuple(collect(1:(ndims(x) - 1))))\n",
    "\n",
    "    conditioners = NamedTuple{\n",
    "        ntuple(Base.Fix1(Symbol, :conditioners_), l.model.n_transforms)\n",
    "    }(\n",
    "        Tuple([smodel.st for smodel in smodels])\n",
    "    )\n",
    "    l.st = merge(l.st, (; conditioners))\n",
    "\n",
    "    return lprob\n",
    "end\n",
    "\n",
    "function sample(\n",
    "    rng::AbstractRNG,\n",
    "    ::Type{T},\n",
    "    d::StatefulRealNVP,\n",
    "    nsamples::Int,\n",
    "    nsteps::Int=length(d.model.conditioners),\n",
    ") where {T}\n",
    "    @assert 1 ≤ nsteps ≤ length(d.model.conditioners)\n",
    "\n",
    "    smodels = [\n",
    "        StatefulLuxLayer(conditioner, d.ps.conditioners[i], d.st.conditioners[i]) for\n",
    "        (i, conditioner) in enumerate(d.model.conditioners)\n",
    "    ]\n",
    "\n",
    "    x = randn(rng, T, d.model.dist_dims, nsamples)\n",
    "    for (i, (mask, conditioner)) in enumerate(zip(d.st.mask_list, smodels))\n",
    "        x, _ = forward_and_log_det(MaskedCoupling(mask, conditioner, AffineBijector), x)\n",
    "        i ≥ nsteps && break\n",
    "    end\n",
    "    return x\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "dsum(x; dims) = dropdims(sum(x; dims); dims)\n",
    "\n",
    "function loss_function(model, ps, st, x)\n",
    "    smodel = StatefulLuxLayer(model, ps, st)\n",
    "    lprob = log_prob(smodel, x)\n",
    "    return -mean(lprob), smodel.st, (;)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the Model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function main(;\n",
    "    maxiters::Int=10_000,\n",
    "    n_train_samples::Int=100_000,\n",
    "    batchsize::Int=128,\n",
    "    n_transforms::Int=6,\n",
    "    hidden_dims::Int=16,\n",
    "    n_layers::Int=4,\n",
    "    lr::Float64=0.0004,\n",
    "    noise::Float64=0.06,\n",
    ")\n",
    "    rng = Random.default_rng()\n",
    "    Random.seed!(rng, 0)\n",
    "\n",
    "    dataloader = Iterators.cycle(\n",
    "        xdev(load_moons_dataloader(rng, Float32, n_train_samples; batchsize, noise))\n",
    "    )\n",
    "\n",
    "    model = RealNVP(; n_transforms, dist_dims=2, hidden_dims, n_layers)\n",
    "    ps, st = xdev(Lux.setup(rng, model))\n",
    "    opt = Adam(lr)\n",
    "\n",
    "    train_state = Training.TrainState(model, ps, st, opt)\n",
    "    @printf \"Total Trainable Parameters: %d\\n\" Lux.parameterlength(ps)\n",
    "\n",
    "    total_samples = 0\n",
    "    start_time = time()\n",
    "\n",
    "    for (iter, x) in enumerate(dataloader)\n",
    "        total_samples += size(x, ndims(x))\n",
    "        (_, loss, _, train_state) = Training.single_train_step!(\n",
    "            AutoEnzyme(), loss_function, x, train_state; return_gradients=Val(false)\n",
    "        )\n",
    "\n",
    "        isnan(loss) && error(\"NaN loss encountered in iter $(iter)!\")\n",
    "\n",
    "        if iter == 1 || iter == maxiters || iter % 1000 == 0\n",
    "            throughput = total_samples / (time() - start_time)\n",
    "            @printf \"Iter: [%6d/%6d]\\tTraining Loss: %.6f\\t\\\n",
    "                     Throughput: %.6f samples/s\\n\" iter maxiters loss throughput\n",
    "        end\n",
    "\n",
    "        iter ≥ maxiters && break\n",
    "    end\n",
    "\n",
    "    return StatefulLuxLayer(model, train_state.parameters, Lux.testmode(train_state.states))\n",
    "end\n",
    "\n",
    "trained_model = main()\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing the Results"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "z_stages = Matrix{Float32}[]\n",
    "for i in 1:(trained_model.model.n_transforms)\n",
    "    z = @jit sample(Random.default_rng(), Float32, trained_model, 10_000, i)\n",
    "    push!(z_stages, Array(z))\n",
    "end\n",
    "\n",
    "begin\n",
    "    fig = Figure(; size=(1200, 800))\n",
    "\n",
    "    for (idx, z) in enumerate(z_stages)\n",
    "        i, j = (idx - 1) ÷ 3, (idx - 1) % 3\n",
    "        ax = Axis(fig[i, j]; title=\"$(idx) transforms\")\n",
    "        scatter!(ax, z[1, :], z[2, :]; markersize=2)\n",
    "    end\n",
    "\n",
    "    fig\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://raw.githubusercontent.com/LuxDL/Lux.jl/main/docs/src/public/realnvp.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.7",
   "language": "julia"
  }
 },
 "nbformat": 4
}
