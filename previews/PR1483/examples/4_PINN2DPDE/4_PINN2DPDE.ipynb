{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training a PINN on 2D PDE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial we will go over using a PINN to solve 2D PDEs. We will be using the\n",
    "system from [NeuralPDE Tutorials](https://docs.sciml.ai/NeuralPDE/stable/tutorials/gpu/).\n",
    "However, we will be using our custom loss function and use nested AD capabilities of\n",
    "Lux.jl."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a demonstration of Lux.jl. For serious use cases of PINNs, please refer to\n",
    "the package: [NeuralPDE.jl](https://github.com/SciML/NeuralPDE.jl)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Package Imports"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Lux,\n",
    "    Optimisers,\n",
    "    Random,\n",
    "    Printf,\n",
    "    Statistics,\n",
    "    MLUtils,\n",
    "    OnlineStats,\n",
    "    CairoMakie,\n",
    "    Reactant,\n",
    "    Enzyme\n",
    "\n",
    "const xdev = reactant_device(; force=true)\n",
    "const cdev = cpu_device()\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem Definition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since Lux supports efficient nested AD upto 2nd order, we will rewrite the problem\n",
    "with first order derivatives, so that we can compute the gradients of the loss using\n",
    "2nd order AD."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the Neural Networks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the networks take 3 input variables and output a scalar value. Here, we will define\n",
    "a wrapper over the 3 networks, so that we can train them using\n",
    "`Training.TrainState`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct PINN{M} <: AbstractLuxWrapperLayer{:model}\n",
    "    model::M\n",
    "end\n",
    "\n",
    "function PINN(; hidden_dims::Int=32)\n",
    "    return PINN(\n",
    "        Chain(\n",
    "            Dense(3 => hidden_dims, tanh),\n",
    "            Dense(hidden_dims => hidden_dims, tanh),\n",
    "            Dense(hidden_dims => hidden_dims, tanh),\n",
    "            Dense(hidden_dims => 1),\n",
    "        ),\n",
    "    )\n",
    "end\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the Loss Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will define a custom loss function to compute the loss using 2nd order AD.\n",
    "For that, first we'll need to define the derivatives of our model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function ∂u_∂t(model::StatefulLuxLayer, xyt::AbstractArray)\n",
    "    return Enzyme.gradient(Enzyme.Reverse, sum ∘ model, xyt)[1][3, :]\n",
    "end\n",
    "\n",
    "function ∂u_∂x(model::StatefulLuxLayer, xyt::AbstractArray)\n",
    "    return Enzyme.gradient(Enzyme.Reverse, sum ∘ model, xyt)[1][1, :]\n",
    "end\n",
    "\n",
    "function ∂u_∂y(model::StatefulLuxLayer, xyt::AbstractArray)\n",
    "    return Enzyme.gradient(Enzyme.Reverse, sum ∘ model, xyt)[1][2, :]\n",
    "end\n",
    "\n",
    "function ∂²u_∂x²(model::StatefulLuxLayer, xyt::AbstractArray)\n",
    "    return Enzyme.gradient(Enzyme.Reverse, sum ∘ ∂u_∂x, Enzyme.Const(model), xyt)[2][1, :]\n",
    "end\n",
    "\n",
    "function ∂²u_∂y²(model::StatefulLuxLayer, xyt::AbstractArray)\n",
    "    return Enzyme.gradient(Enzyme.Reverse, sum ∘ ∂u_∂y, Enzyme.Const(model), xyt)[2][2, :]\n",
    "end\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the following loss function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function physics_informed_loss_function(model::StatefulLuxLayer, xyt::AbstractArray)\n",
    "    return mean(abs2, ∂u_∂t(model, xyt) .- ∂²u_∂x²(model, xyt) .- ∂²u_∂y²(model, xyt))\n",
    "end\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Additionally, we need to compute the loss with respect to the boundary conditions."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function mse_loss_function(\n",
    "    model::StatefulLuxLayer, target::AbstractArray, xyt::AbstractArray\n",
    ")\n",
    "    return MSELoss()(model(xyt), target)\n",
    "end\n",
    "\n",
    "function loss_function(model, ps, st, (xyt, target_data, xyt_bc, target_bc))\n",
    "    smodel = StatefulLuxLayer(model, ps, st)\n",
    "    physics_loss = physics_informed_loss_function(smodel, xyt)\n",
    "    data_loss = mse_loss_function(smodel, target_data, xyt)\n",
    "    bc_loss = mse_loss_function(smodel, target_bc, xyt_bc)\n",
    "    loss = physics_loss + data_loss + bc_loss\n",
    "    return loss, smodel.st, (; physics_loss, data_loss, bc_loss)\n",
    "end\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate the Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will generate some random data to train the model on. We will take data on a square\n",
    "spatial and temporal domain $x \\in [0, 2]$, $y \\in [0, 2]$, and $t \\in [0, 2]$. Typically,\n",
    "you want to be smarter about the sampling process, but for the sake of simplicity, we will\n",
    "skip that."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "analytical_solution(x, y, t) = @. exp(x + y) * cos(x + y + 4t)\n",
    "analytical_solution(xyt) = analytical_solution(xyt[1, :], xyt[2, :], xyt[3, :])\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "begin\n",
    "    grid_len = 16\n",
    "\n",
    "    grid = range(0.0f0, 2.0f0; length=grid_len)\n",
    "    xyt = stack([[elem...] for elem in vec(collect(Iterators.product(grid, grid, grid)))])\n",
    "\n",
    "    target_data = reshape(analytical_solution(xyt), 1, :)\n",
    "\n",
    "    bc_len = 512\n",
    "\n",
    "    x = collect(range(0.0f0, 2.0f0; length=bc_len))\n",
    "    y = collect(range(0.0f0, 2.0f0; length=bc_len))\n",
    "    t = collect(range(0.0f0, 2.0f0; length=bc_len))\n",
    "\n",
    "    xyt_bc = hcat(\n",
    "        stack((x, y, zeros(Float32, bc_len)); dims=1),\n",
    "        stack((zeros(Float32, bc_len), y, t); dims=1),\n",
    "        stack((ones(Float32, bc_len) .* 2, y, t); dims=1),\n",
    "        stack((x, zeros(Float32, bc_len), t); dims=1),\n",
    "        stack((x, ones(Float32, bc_len) .* 2, t); dims=1),\n",
    "    )\n",
    "    target_bc = reshape(analytical_solution(xyt_bc), 1, :)\n",
    "\n",
    "    min_target_bc, max_target_bc = extrema(target_bc)\n",
    "    min_data, max_data = extrema(target_data)\n",
    "    min_pde_val, max_pde_val = min(min_data, min_target_bc), max(max_data, max_target_bc)\n",
    "\n",
    "    xyt = (xyt .- minimum(xyt)) ./ (maximum(xyt) .- minimum(xyt))\n",
    "    xyt_bc = (xyt_bc .- minimum(xyt_bc)) ./ (maximum(xyt_bc) .- minimum(xyt_bc))\n",
    "    target_bc = (target_bc .- min_pde_val) ./ (max_pde_val - min_pde_val)\n",
    "    target_data = (target_data .- min_pde_val) ./ (max_pde_val - min_pde_val)\n",
    "end\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function train_model(\n",
    "    xyt,\n",
    "    target_data,\n",
    "    xyt_bc,\n",
    "    target_bc;\n",
    "    seed::Int=0,\n",
    "    maxiters::Int=50000,\n",
    "    hidden_dims::Int=128,\n",
    ")\n",
    "    rng = Random.default_rng()\n",
    "    Random.seed!(rng, seed)\n",
    "\n",
    "    pinn = PINN(; hidden_dims)\n",
    "    ps, st = Lux.setup(rng, pinn) |> xdev\n",
    "\n",
    "    bc_dataloader =\n",
    "        DataLoader((xyt_bc, target_bc); batchsize=128, shuffle=true, partial=false) |> xdev\n",
    "    pde_dataloader =\n",
    "        DataLoader((xyt, target_data); batchsize=128, shuffle=true, partial=false) |> xdev\n",
    "\n",
    "    train_state = Training.TrainState(pinn, ps, st, Adam(0.005f0))\n",
    "\n",
    "    lr = i -> i < 5000 ? 0.005f0 : (i < 10000 ? 0.0005f0 : 0.00005f0)\n",
    "\n",
    "    total_loss_tracker, physics_loss_tracker, data_loss_tracker, bc_loss_tracker = ntuple(\n",
    "        _ -> OnlineStats.CircBuff(Float32, 32; rev=true), 4\n",
    "    )\n",
    "\n",
    "    iter = 1\n",
    "    for ((xyt_batch, target_data_batch), (xyt_bc_batch, target_bc_batch)) in\n",
    "        zip(Iterators.cycle(pde_dataloader), Iterators.cycle(bc_dataloader))\n",
    "        Optimisers.adjust!(train_state, lr(iter))\n",
    "\n",
    "        _, loss, stats, train_state = Training.single_train_step!(\n",
    "            AutoEnzyme(),\n",
    "            loss_function,\n",
    "            (xyt_batch, target_data_batch, xyt_bc_batch, target_bc_batch),\n",
    "            train_state;\n",
    "            return_gradients=Val(false),\n",
    "        )\n",
    "\n",
    "        fit!(total_loss_tracker, Float32(loss))\n",
    "        fit!(physics_loss_tracker, Float32(stats.physics_loss))\n",
    "        fit!(data_loss_tracker, Float32(stats.data_loss))\n",
    "        fit!(bc_loss_tracker, Float32(stats.bc_loss))\n",
    "\n",
    "        mean_loss = mean(OnlineStats.value(total_loss_tracker))\n",
    "        mean_physics_loss = mean(OnlineStats.value(physics_loss_tracker))\n",
    "        mean_data_loss = mean(OnlineStats.value(data_loss_tracker))\n",
    "        mean_bc_loss = mean(OnlineStats.value(bc_loss_tracker))\n",
    "\n",
    "        isnan(loss) && throw(ArgumentError(\"NaN Loss Detected\"))\n",
    "\n",
    "        if iter % 1000 == 1 || iter == maxiters\n",
    "            @printf(\n",
    "                \"Iteration: [%6d/%6d] \\t Loss: %.9f (%.9f) \\t Physics Loss: %.9f \\\n",
    "                 (%.9f) \\t Data Loss: %.9f (%.9f) \\t BC \\\n",
    "                 Loss: %.9f (%.9f)\\n\",\n",
    "                iter,\n",
    "                maxiters,\n",
    "                loss,\n",
    "                mean_loss,\n",
    "                stats.physics_loss,\n",
    "                mean_physics_loss,\n",
    "                stats.data_loss,\n",
    "                mean_data_loss,\n",
    "                stats.bc_loss,\n",
    "                mean_bc_loss\n",
    "            )\n",
    "        end\n",
    "\n",
    "        iter += 1\n",
    "        iter ≥ maxiters && break\n",
    "    end\n",
    "\n",
    "    return StatefulLuxLayer(pinn, cdev(train_state.parameters), cdev(train_state.states))\n",
    "end\n",
    "\n",
    "trained_model = train_model(xyt, target_data, xyt_bc, target_bc)\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing the Results"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ts, xs, ys = 0.0f0:0.05f0:2.0f0, 0.0f0:0.02f0:2.0f0, 0.0f0:0.02f0:2.0f0\n",
    "grid = stack([[elem...] for elem in vec(collect(Iterators.product(xs, ys, ts)))])\n",
    "\n",
    "u_real = reshape(analytical_solution(grid), length(xs), length(ys), length(ts))\n",
    "\n",
    "grid_normalized = (grid .- minimum(grid)) ./ (maximum(grid) .- minimum(grid))\n",
    "u_pred = reshape(trained_model(grid_normalized), length(xs), length(ys), length(ts))\n",
    "u_pred = u_pred .* (max_pde_val - min_pde_val) .+ min_pde_val\n",
    "\n",
    "begin\n",
    "    fig = Figure()\n",
    "    ax = CairoMakie.Axis(fig[1, 1]; xlabel=\"x\", ylabel=\"y\")\n",
    "    errs = [abs.(u_pred[:, :, i] .- u_real[:, :, i]) for i in 1:length(ts)]\n",
    "    Colorbar(fig[1, 2]; limits=extrema(stack(errs)))\n",
    "\n",
    "    CairoMakie.record(fig, \"pinn_nested_ad.gif\", 1:length(ts); framerate=10) do i\n",
    "        ax.title = \"Abs. Predictor Error | Time: $(ts[i])\"\n",
    "        err = errs[i]\n",
    "        contour!(ax, xs, ys, err; levels=10, linewidth=2)\n",
    "        heatmap!(ax, xs, ys, err)\n",
    "        return fig\n",
    "    end\n",
    "\n",
    "    fig\n",
    "end\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](pinn_nested_ad.gif)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.7",
   "language": "julia"
  }
 },
 "nbformat": 4
}
