{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ImageNet Classification using Distributed Data Parallel Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This implements training of popular model architectures, such as ResNet, AlexNet, and VGG\n",
    "on the ImageNet dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For distributed data-parallel training we need to launch this script using `mpiexecjl`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setup [MPI.jl](https://juliaparallel.org/MPI.jl/).\n",
    "If your system has functional NCCL we will use it for all CUDA communications.\n",
    "Otherwise, we will use MPI for all communications."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "mpiexecjl -np 4 julia --startup=no --project=examples/ImageNet -t auto\\\n",
    "  examples/ImageNet/main.jl \\\n",
    "  --model-name=\"ViT\" \\\n",
    "  --model-kind=\"tiny\" \\\n",
    "  --train-batchsize=256 \\\n",
    "  --val-batchsize=256 \\\n",
    "  --optimizer-kind=\"sgd\" \\\n",
    "  --learning-rate=0.01 \\\n",
    "  --base-path=\"/home/avik-pal/data/ImageNet/\"\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For single-node training, we can simply launch the script using `julia`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "julia --startup=no --project=examples/ImageNet -t auto examples/ImageNet/main.jl \\\n",
    "  --model-name=\"ViT\" \\\n",
    "  --model-kind=\"tiny\" \\\n",
    "  --train-batchsize=256 \\\n",
    "  --val-batchsize=256 \\\n",
    "  --optimizer-kind=\"sgd\" \\\n",
    "  --learning-rate=0.01 \\\n",
    "  --base-path=\"/home/avik-pal/data/ImageNet/\"\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Package Imports"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Boltz, Lux, MLDataDevices\n",
    "# import Metalhead # Install and load this package to use the Metalhead models with Lux\n",
    "\n",
    "using Dates, Random\n",
    "using DataAugmentation,\n",
    "    FileIO, MLUtils, OneHotArrays, Optimisers, ParameterSchedulers, Setfield\n",
    "using Comonicon, Format\n",
    "using JLD2\n",
    "using Zygote\n",
    "\n",
    "using LuxCUDA\n",
    "# using AMDGPU # Install and load AMDGPU to train models on AMD GPUs with ROCm\n",
    "using MPI: MPI\n",
    "# Enables distributed training in Lux. NCCL is needed for CUDA GPUs\n",
    "using NCCL: NCCL\n",
    "\n",
    "const gdev = gpu_device()\n",
    "const cdev = cpu_device()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Distributed Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use NCCL for NVIDIA GPUs and MPI for anything else"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "const distributed_backend = try\n",
    "    if gdev isa CUDADevice\n",
    "        DistributedUtils.initialize(NCCLBackend)\n",
    "        DistributedUtils.get_distributed_backend(NCCLBackend)\n",
    "    else\n",
    "        DistributedUtils.initialize(MPIBackend)\n",
    "        DistributedUtils.get_distributed_backend(MPIBackend)\n",
    "    end\n",
    "catch err\n",
    "    @error \"Could not initialize distributed training. Error: $err\"\n",
    "    nothing\n",
    "end\n",
    "\n",
    "const local_rank =\n",
    "    distributed_backend === nothing ? 0 : DistributedUtils.local_rank(distributed_backend)\n",
    "const total_workers = if distributed_backend === nothing\n",
    "    1\n",
    "else\n",
    "    DistributedUtils.total_workers(distributed_backend)\n",
    "end\n",
    "const is_distributed = total_workers > 1\n",
    "const should_log = !is_distributed || local_rank == 0"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading for ImageNet"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# We need the data to be in a specific format. See the\n",
    "# [README.md](<unknown>/examples/ImageNet/README.md) for more details.\n",
    "\n",
    "const IMAGENET_CORRUPTED_FILES = [\n",
    "    \"n01739381_1309.JPEG\",\n",
    "    \"n02077923_14822.JPEG\",\n",
    "    \"n02447366_23489.JPEG\",\n",
    "    \"n02492035_15739.JPEG\",\n",
    "    \"n02747177_10752.JPEG\",\n",
    "    \"n03018349_4028.JPEG\",\n",
    "    \"n03062245_4620.JPEG\",\n",
    "    \"n03347037_9675.JPEG\",\n",
    "    \"n03467068_12171.JPEG\",\n",
    "    \"n03529860_11437.JPEG\",\n",
    "    \"n03544143_17228.JPEG\",\n",
    "    \"n03633091_5218.JPEG\",\n",
    "    \"n03710637_5125.JPEG\",\n",
    "    \"n03961711_5286.JPEG\",\n",
    "    \"n04033995_2932.JPEG\",\n",
    "    \"n04258138_17003.JPEG\",\n",
    "    \"n04264628_27969.JPEG\",\n",
    "    \"n04336792_7448.JPEG\",\n",
    "    \"n04371774_5854.JPEG\",\n",
    "    \"n04596742_4225.JPEG\",\n",
    "    \"n07583066_647.JPEG\",\n",
    "    \"n13037406_4650.JPEG\",\n",
    "    \"n02105855_2933.JPEG\",\n",
    "    \"ILSVRC2012_val_00019877.JPEG\",\n",
    "]\n",
    "\n",
    "function load_imagenet1k(base_path::String, split::Symbol)\n",
    "    @assert split in (:train, :val)\n",
    "    full_path = joinpath(base_path, string(split))\n",
    "    synsets = sort(readdir(full_path))\n",
    "    @assert length(synsets) == 1000 \"There should be 1000 subdirectories in $(full_path).\"\n",
    "\n",
    "    image_files = String[]\n",
    "    labels = Int[]\n",
    "    for (i, synset) in enumerate(synsets)\n",
    "        filenames = readdir(joinpath(full_path, synset))\n",
    "        filter!(x -> x ∉ IMAGENET_CORRUPTED_FILES, filenames)\n",
    "        paths = joinpath.((full_path,), (synset,), filenames)\n",
    "        append!(image_files, paths)\n",
    "        append!(labels, repeat([i - 1], length(paths)))\n",
    "    end\n",
    "\n",
    "    return image_files, labels\n",
    "end\n",
    "\n",
    "default_image_size(::Type{Vision.VisionTransformer}, ::Nothing) = 256\n",
    "default_image_size(::Type{Vision.VisionTransformer}, size::Int) = size\n",
    "default_image_size(_, ::Nothing) = 224\n",
    "default_image_size(_, size::Int) = size\n",
    "\n",
    "struct MakeColoredImage <: DataAugmentation.Transform end\n",
    "\n",
    "function DataAugmentation.apply(\n",
    "    ::MakeColoredImage, item::DataAugmentation.AbstractArrayItem; randstate=nothing\n",
    ")\n",
    "    data = itemdata(item)\n",
    "    (ndims(data) == 2 || size(data, 3) == 1) && (data = cat(data, data, data; dims=Val(3)))\n",
    "    return DataAugmentation.setdata(item, data)\n",
    "end\n",
    "\n",
    "struct FileDataset\n",
    "    files\n",
    "    labels\n",
    "    augment\n",
    "end\n",
    "\n",
    "Base.length(dataset::FileDataset) = length(dataset.files)\n",
    "\n",
    "function Base.getindex(dataset::FileDataset, i::Int)\n",
    "    img = Image(FileIO.load(dataset.files[i]))\n",
    "    aug_img = itemdata(DataAugmentation.apply(dataset.augment, img))\n",
    "    return aug_img, OneHotArrays.onehot(dataset.labels[i], 0:999)\n",
    "end\n",
    "\n",
    "function construct_dataloaders(;\n",
    "    base_path::String, train_batchsize, val_batchsize, image_size::Int\n",
    ")\n",
    "    sensible_println(\"=> creating dataloaders.\")\n",
    "\n",
    "    train_augment =\n",
    "        ScaleFixed((256, 256)) |>\n",
    "        Maybe(FlipX(), 0.5) |>\n",
    "        RandomResizeCrop((image_size, image_size)) |>\n",
    "        PinOrigin() |>\n",
    "        ImageToTensor() |>\n",
    "        MakeColoredImage() |>\n",
    "        ToEltype(Float32) |>\n",
    "        Normalize((0.485f0, 0.456f0, 0.406f0), (0.229f0, 0.224f0, 0.225f0))\n",
    "    train_files, train_labels = load_imagenet1k(base_path, :train)\n",
    "\n",
    "    train_dataset = FileDataset(train_files, train_labels, train_augment)\n",
    "\n",
    "    val_augment =\n",
    "        ScaleFixed((image_size, image_size)) |>\n",
    "        PinOrigin() |>\n",
    "        ImageToTensor() |>\n",
    "        MakeColoredImage() |>\n",
    "        ToEltype(Float32) |>\n",
    "        Normalize((0.485f0, 0.456f0, 0.406f0), (0.229f0, 0.224f0, 0.225f0))\n",
    "    val_files, val_labels = load_imagenet1k(base_path, :val)\n",
    "\n",
    "    val_dataset = FileDataset(val_files, val_labels, val_augment)\n",
    "\n",
    "    if is_distributed\n",
    "        train_dataset = DistributedUtils.DistributedDataContainer(\n",
    "            distributed_backend, train_dataset\n",
    "        )\n",
    "        val_dataset = DistributedUtils.DistributedDataContainer(\n",
    "            distributed_backend, val_dataset\n",
    "        )\n",
    "    end\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset;\n",
    "        batchsize=train_batchsize ÷ total_workers,\n",
    "        partial=false,\n",
    "        collate=true,\n",
    "        shuffle=true,\n",
    "        parallel=true,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset;\n",
    "        batchsize=val_batchsize ÷ total_workers,\n",
    "        partial=true,\n",
    "        collate=true,\n",
    "        shuffle=false,\n",
    "        parallel=true,\n",
    "    )\n",
    "\n",
    "    return gdev(train_dataloader), gdev(val_dataloader)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Construction"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function construct_model(;\n",
    "    rng::AbstractRNG, model_name::String, model_args, pretrained::Bool=false\n",
    ")\n",
    "    model = getproperty(Vision, Symbol(model_name))(model_args...; pretrained)\n",
    "    ps, st = gdev(Lux.setup(rng, model))\n",
    "\n",
    "    sensible_println(\"=> model `$(model_name)` created.\")\n",
    "    pretrained && sensible_println(\"==> using pre-trained model`\")\n",
    "    sensible_println(\"==> number of trainable parameters: $(Lux.parameterlength(ps))\")\n",
    "    sensible_println(\"==> number of states: $(Lux.statelength(st))\")\n",
    "\n",
    "    if is_distributed\n",
    "        ps = DistributedUtils.synchronize!!(distributed_backend, ps)\n",
    "        st = DistributedUtils.synchronize!!(distributed_backend, st)\n",
    "        sensible_println(\"==> synced model parameters and states across all ranks\")\n",
    "    end\n",
    "\n",
    "    return model, ps, st\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizer Configuration"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function construct_optimizer_and_scheduler(;\n",
    "    kind::String,\n",
    "    learning_rate::AbstractFloat,\n",
    "    nesterov::Bool,\n",
    "    momentum::AbstractFloat,\n",
    "    weight_decay::AbstractFloat,\n",
    "    scheduler_kind::String,\n",
    "    cycle_length::Int,\n",
    "    damp_factor::AbstractFloat,\n",
    "    lr_step_decay::AbstractFloat,\n",
    "    lr_step::Vector{Int},\n",
    ")\n",
    "    sensible_println(\"=> creating optimizer.\")\n",
    "\n",
    "    kind = Symbol(kind)\n",
    "    optimizer = if kind == :adam\n",
    "        Adam(learning_rate)\n",
    "    elseif kind == :sgd\n",
    "        if nesterov\n",
    "            Nesterov(learning_rate, momentum)\n",
    "        elseif iszero(momentum)\n",
    "            Descent(learning_rate)\n",
    "        else\n",
    "            Momentum(learning_rate, momentum)\n",
    "        end\n",
    "    else\n",
    "        throw(ArgumentError(\"Unknown value for `optimizer` = $kind. Supported options are: \\\n",
    "                             `adam` and `sgd`.\"))\n",
    "    end\n",
    "\n",
    "    optimizer = if iszero(weight_decay)\n",
    "        optimizer\n",
    "    else\n",
    "        OptimiserChain(optimizer, WeightDecay(weight_decay))\n",
    "    end\n",
    "\n",
    "    sensible_println(\"=> creating scheduler.\")\n",
    "\n",
    "    scheduler_kind = Symbol(scheduler_kind)\n",
    "    scheduler = if scheduler_kind == :cosine\n",
    "        l0 = learning_rate\n",
    "        l1 = learning_rate / 100\n",
    "        ComposedSchedule(\n",
    "            CosAnneal(l0, l1, cycle_length), Step(l0, damp_factor, cycle_length)\n",
    "        )\n",
    "    elseif scheduler_kind == :constant\n",
    "        Constant(learning_rate)\n",
    "    elseif scheduler_kind == :step\n",
    "        Step(learning_rate, lr_step_decay, lr_step)\n",
    "    else\n",
    "        throw(ArgumentError(\"Unknown value for `lr_scheduler` = $(scheduler_kind). \\\n",
    "                             Supported options are: `constant`, `step` and `cosine`.\"))\n",
    "    end\n",
    "\n",
    "    optimizer = if is_distributed\n",
    "        DistributedUtils.DistributedOptimizer(distributed_backend, optimizer)\n",
    "    else\n",
    "        optimizer\n",
    "    end\n",
    "\n",
    "    return optimizer, scheduler\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utility Functions"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "const logitcrossentropy = CrossEntropyLoss(; logits=Val(true))\n",
    "\n",
    "function loss_function(model, ps, st, (img, y))\n",
    "    ŷ, stₙ = model(img, ps, st)\n",
    "    return logitcrossentropy(ŷ, y), stₙ, (; prediction=ŷ)\n",
    "end\n",
    "\n",
    "sensible_println(msg) = should_log && println(\"[$(now())] \", msg)\n",
    "sensible_print(msg) = should_log && print(\"[$(now())] \", msg)\n",
    "\n",
    "function accuracy(ŷ::AbstractMatrix, y::AbstractMatrix, topk=(1,))\n",
    "    pred_labels = partialsortperm.(eachcol(cdev(ŷ)), Ref(1:maximum(topk)); rev=true)\n",
    "    true_labels = onecold(cdev(y))\n",
    "    accuracies = Vector{Float64}(undef, length(topk))\n",
    "    for (i, k) in enumerate(topk)\n",
    "        accuracies[i] = sum(\n",
    "            map((a, b) -> sum(view(a, 1:k) .== b), pred_labels, true_labels)\n",
    "        )\n",
    "    end\n",
    "    accuracies .= accuracies .* 100 ./ size(y, 2)\n",
    "    return accuracies\n",
    "end\n",
    "\n",
    "function save_checkpoint(state::NamedTuple; is_best::Bool, filename::String)\n",
    "    should_log || return nothing\n",
    "    @assert last(splitext(filename)) == \".jld2\" \"Filename should have a .jld2 extension.\"\n",
    "    isdir(dirname(filename)) || mkpath(dirname(filename))\n",
    "    save(filename; state)\n",
    "    sensible_println(\"=> saved checkpoint `$(filename)`.\")\n",
    "    if is_best\n",
    "        symlink_safe(filename, joinpath(dirname(filename), \"model_best.jld2\"))\n",
    "        sensible_println(\"=> best model updated to `$(filename)`!\")\n",
    "    end\n",
    "    symlink_safe(filename, joinpath(dirname(filename), \"model_current.jld2\"))\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function symlink_safe(src, dest)\n",
    "    rm(dest; force=true)\n",
    "    symlink(src, dest)\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function load_checkpoint(filename::String)\n",
    "    try ## NOTE(@avik-pal): ispath is failing for symlinks?\n",
    "        return JLD2.load(filename)[:state]\n",
    "    catch\n",
    "        sensible_println(\"$(filename) could not be loaded. This might be because the file \\\n",
    "                          is absent or is corrupt. Proceeding by returning `nothing`.\")\n",
    "        return nothing\n",
    "    end\n",
    "end\n",
    "\n",
    "function full_gc_and_reclaim()\n",
    "    GC.gc(true)\n",
    "    MLDataDevices.functional(CUDADevice) && CUDA.reclaim()\n",
    "    MLDataDevices.functional(AMDGPUDevice) && AMDGPU.reclaim()\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "@kwdef mutable struct AverageMeter\n",
    "    fmtstr\n",
    "    val::Float64 = 0.0\n",
    "    sum::Float64 = 0.0\n",
    "    count::Int = 0\n",
    "    average::Float64 = 0\n",
    "end\n",
    "\n",
    "function AverageMeter(name::String, fmt::String)\n",
    "    return AverageMeter(; fmtstr=FormatExpr(\"$(name) {1:$(fmt)} ({2:$(fmt)})\"))\n",
    "end\n",
    "\n",
    "function (meter::AverageMeter)(val, n::Int)\n",
    "    meter.val = val\n",
    "    s = val * n\n",
    "    if is_distributed\n",
    "        v = [s, typeof(val)(n)]\n",
    "        DistributedUtils.allreduce!(backend, v, +)\n",
    "        s, n = v[1], Int(v[2])\n",
    "    end\n",
    "    meter.sum += s\n",
    "    meter.count += n\n",
    "    meter.average = meter.sum / meter.count\n",
    "    return meter.average\n",
    "end\n",
    "\n",
    "function reset_meter!(meter::AverageMeter)\n",
    "    meter.val = 0.0\n",
    "    meter.sum = 0.0\n",
    "    meter.count = 0\n",
    "    meter.average = 0.0\n",
    "    return meter\n",
    "end\n",
    "\n",
    "function print_meter(meter::AverageMeter)\n",
    "    return should_log && printfmt(meter.fmtstr, meter.val, meter.average)\n",
    "end\n",
    "\n",
    "struct ProgressMeter\n",
    "    batch_fmtstr\n",
    "    meters\n",
    "end\n",
    "\n",
    "function ProgressMeter(num_batches::Int, meters, prefix::String=\"\")\n",
    "    fmt = \"%\" * string(length(string(num_batches))) * \"d\"\n",
    "    fmt2 = \"{1:\" * string(length(string(num_batches))) * \"d}\"\n",
    "    prefix = prefix != \"\" ? endswith(prefix, \" \") ? prefix : prefix * \" \" : \"\"\n",
    "    batch_fmtstr = FormatExpr(\"$prefix[$fmt2/\" * cfmt(fmt, num_batches) * \"]\")\n",
    "    return ProgressMeter(batch_fmtstr, meters)\n",
    "end\n",
    "\n",
    "reset_meter!(meter::ProgressMeter) = foreach(reset_meter!, meter.meters)\n",
    "\n",
    "function print_meter(meter::ProgressMeter, batch::Int)\n",
    "    should_log || return nothing\n",
    "    printfmt(meter.batch_fmtstr, batch)\n",
    "    foreach(meter.meters) do x\n",
    "        print(\"\\t\")\n",
    "        print_meter(x)\n",
    "        return nothing\n",
    "    end\n",
    "    println()\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "get_loggable_values(meter::ProgressMeter) = getproperty.(meter.meters, :average)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Validation Loops"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function validate(val_loader, model, ps, st, step, total_steps)\n",
    "    batch_time = AverageMeter(\"Batch Time\", \"6.5f\")\n",
    "    data_time = AverageMeter(\"Data Time\", \"6.5f\")\n",
    "    forward_time = AverageMeter(\"Forward Pass Time\", \"6.5f\")\n",
    "    losses = AverageMeter(\"Loss\", \".6f\")\n",
    "    top1 = AverageMeter(\"Acc@1\", \"6.4f\")\n",
    "    top5 = AverageMeter(\"Acc@5\", \"6.4f\")\n",
    "\n",
    "    progress = ProgressMeter(\n",
    "        total_steps, (batch_time, data_time, forward_time, losses, top1, top5), \"Val:\"\n",
    "    )\n",
    "\n",
    "    st = Lux.testmode(st)\n",
    "    t = time()\n",
    "    for (img, y) in val_loader\n",
    "        t_data, t = time() - t, time()\n",
    "\n",
    "        bsize = size(img, ndims(img))\n",
    "\n",
    "        loss, st, stats = loss_function(model, ps, st, (img, y))\n",
    "        t_forward = time() - t\n",
    "\n",
    "        acc1, acc5 = accuracy(stats.prediction, y, (1, 5))\n",
    "\n",
    "        top1(acc1, bsize)\n",
    "        top5(acc5, bsize)\n",
    "        losses(loss, bsize)\n",
    "        data_time(t_data, bsize)\n",
    "        forward_time(t_forward, bsize)\n",
    "        batch_time(t_data + t_forward, bsize)\n",
    "\n",
    "        t = time()\n",
    "    end\n",
    "\n",
    "    print_meter(progress, step)\n",
    "    return top1.average\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entry Point"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Comonicon.@main function main(;\n",
    "    seed::Int=0,\n",
    "    model_name::String,\n",
    "    model_kind::String=\"nokind\",\n",
    "    depth::Int=-1,\n",
    "    pretrained::Bool=false,\n",
    "    base_path::String=\"\",\n",
    "    train_batchsize::Int=64,\n",
    "    val_batchsize::Int=64,\n",
    "    image_size::Int=-1,\n",
    "    optimizer_kind::String=\"sgd\",\n",
    "    learning_rate::Float32=0.01f0,\n",
    "    nesterov::Bool=false,\n",
    "    momentum::Float32=0.0f0,\n",
    "    weight_decay::Float32=0.0f0,\n",
    "    scheduler_kind::String=\"step\",\n",
    "    cycle_length::Int=50000,\n",
    "    damp_factor::Float32=1.2f0,\n",
    "    lr_step_decay::Float32=0.1f0,\n",
    "    lr_step::Vector{Int}=[100000, 250000, 500000],\n",
    "    expt_id::String=\"\",\n",
    "    expt_subdir::String=@__DIR__,\n",
    "    resume::String=\"\",\n",
    "    evaluate::Bool=false,\n",
    "    total_steps::Int=800000,\n",
    "    evaluate_every::Int=10000,\n",
    "    print_frequency::Int=100,\n",
    ")\n",
    "    best_acc1 = 0\n",
    "\n",
    "    rng = Random.default_rng()\n",
    "    Random.seed!(rng, seed)\n",
    "\n",
    "    model_type = getproperty(Vision, Symbol(model_name))\n",
    "    image_size = default_image_size(model_type, image_size == -1 ? nothing : image_size)\n",
    "\n",
    "    depth = depth == -1 ? nothing : depth\n",
    "    model_kind = model_kind == \"nokind\" ? nothing : Symbol(model_kind)\n",
    "    model_args = if model_kind === nothing && depth === nothing\n",
    "        ()\n",
    "    elseif model_kind !== nothing\n",
    "        (model_kind,)\n",
    "    else\n",
    "        (depth,)\n",
    "    end\n",
    "    model, ps, st = construct_model(; rng, model_name, model_args, pretrained)\n",
    "\n",
    "    ds_train, ds_val = construct_dataloaders(;\n",
    "        base_path, train_batchsize, val_batchsize, image_size\n",
    "    )\n",
    "\n",
    "    opt, scheduler = construct_optimizer_and_scheduler(;\n",
    "        kind=optimizer_kind,\n",
    "        learning_rate,\n",
    "        nesterov,\n",
    "        momentum,\n",
    "        weight_decay,\n",
    "        scheduler_kind,\n",
    "        cycle_length,\n",
    "        damp_factor,\n",
    "        lr_step_decay,\n",
    "        lr_step,\n",
    "    )\n",
    "\n",
    "    expt_name = \"name-$(model_name)_seed-$(seed)_id-$(expt_id)\"\n",
    "    ckpt_dir = joinpath(expt_subdir, \"checkpoints\", expt_name)\n",
    "\n",
    "    rpath = resume == \"\" ? joinpath(ckpt_dir, \"model_current.jld2\") : resume\n",
    "\n",
    "    ckpt = load_checkpoint(rpath)\n",
    "    if !isnothing(ckpt)\n",
    "        ps, st = gdev((ckpt.ps, ckpt.st))\n",
    "        initial_step = ckpt.step\n",
    "        sensible_println(\"=> training started from $(initial_step)\")\n",
    "    else\n",
    "        initial_step = 1\n",
    "    end\n",
    "\n",
    "    validate(ds_val, model, ps, st, 0, total_steps)\n",
    "    evaluate && return nothing\n",
    "\n",
    "    full_gc_and_reclaim()\n",
    "\n",
    "    batch_time = AverageMeter(\"Batch Time\", \"6.5f\")\n",
    "    data_time = AverageMeter(\"Data Time\", \"6.5f\")\n",
    "    training_time = AverageMeter(\"Training Time\", \"6.5f\")\n",
    "    losses = AverageMeter(\"Loss\", \".6f\")\n",
    "    top1 = AverageMeter(\"Acc@1\", \"6.4f\")\n",
    "    top5 = AverageMeter(\"Acc@5\", \"6.4f\")\n",
    "\n",
    "    progress = ProgressMeter(\n",
    "        total_steps, (batch_time, data_time, training_time, losses, top1, top5), \"Train:\"\n",
    "    )\n",
    "\n",
    "    st = Lux.trainmode(st)\n",
    "    train_state = Training.TrainState(model, ps, st, opt)\n",
    "    if is_distributed\n",
    "        @set! train_state.optimizer_state = DistributedUtils.synchronize!!(\n",
    "            distributed_backend, train_state.optimizer_state\n",
    "        )\n",
    "    end\n",
    "\n",
    "    train_loader = Iterators.cycle(ds_train)\n",
    "    _, train_loader_state = iterate(train_loader)\n",
    "    for step in initial_step:total_steps\n",
    "        t = time()\n",
    "        (img, y), train_loader_state = iterate(train_loader, train_loader_state)\n",
    "        t_data = time() - t\n",
    "\n",
    "        bsize = size(img, ndims(img))\n",
    "\n",
    "        t = time()\n",
    "        _, loss, stats, train_state = Training.single_train_step!(\n",
    "            AutoZygote(), loss_function, (img, y), train_state\n",
    "        )\n",
    "        t_training = time() - t\n",
    "\n",
    "        isnan(loss) && throw(ArgumentError(\"NaN loss encountered.\"))\n",
    "\n",
    "        acc1, acc5 = accuracy(stats.prediction, y, (1, 5))\n",
    "\n",
    "        top1(acc1, bsize)\n",
    "        top5(acc5, bsize)\n",
    "        losses(loss, bsize)\n",
    "        data_time(t_data, bsize)\n",
    "        training_time(t_training, bsize)\n",
    "        batch_time(t_data + t_training, bsize)\n",
    "\n",
    "        if step % print_frequency == 1 || step == total_steps\n",
    "            print_meter(progress, step)\n",
    "            reset_meter!(progress)\n",
    "        end\n",
    "\n",
    "        if step % evaluate_every == 0\n",
    "            acc1 = validate(ds_val, model, ps, st, step, total_steps)\n",
    "            is_best = acc1 > best_acc1\n",
    "            best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "            save_state = (; ps=cdev(ps), st=cdev(st), step)\n",
    "            if should_log()\n",
    "                save_checkpoint(\n",
    "                    save_state; is_best, filename=joinpath(ckpt_dir, \"model_$(step).jld2\")\n",
    "                )\n",
    "            end\n",
    "        end\n",
    "\n",
    "        Optimisers.adjust!(train_state.optimizer_state, scheduler(step + 1))\n",
    "    end\n",
    "\n",
    "    return nothing\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.7",
   "language": "julia"
  }
 },
 "nbformat": 4
}
