{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building a LSTM Encoder-Decoder model using Lux.jl"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This examples is based on [LSTM_encoder_decoder](https://github.com/lkulowski/LSTM_encoder_decoder)\n",
    "by [Laura Kulowski](https://github.com/lkulowski)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Lux, Reactant, Random, Optimisers, Statistics, Enzyme, Printf, CairoMakie, MLUtils\n",
    "\n",
    "const xdev = reactant_device(; force=true)\n",
    "const cdev = cpu_device()\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate synthetic data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function synthetic_data(Nt=2000, tf=80 * Float32(π))\n",
    "    t = range(0.0f0, tf; length=Nt)\n",
    "    y = sin.(2.0f0 * t) .+ 0.5f0 * cos.(t) .+ randn(Float32, Nt) * 0.2f0\n",
    "    return t, y\n",
    "end\n",
    "\n",
    "function train_test_split(t, y, split=0.8)\n",
    "    indx_split = ceil(Int, length(t) * split)\n",
    "    indx_train = 1:indx_split\n",
    "    indx_test = (indx_split + 1):length(t)\n",
    "\n",
    "    t_train = t[indx_train]\n",
    "    y_train = reshape(y[indx_train], 1, :)\n",
    "\n",
    "    t_test = t[indx_test]\n",
    "    y_test = reshape(y[indx_test], 1, :)\n",
    "\n",
    "    return t_train, y_train, t_test, y_test\n",
    "end\n",
    "\n",
    "function windowed_dataset(y; input_window=5, output_window=1, stride=1, num_features=1)\n",
    "    L = size(y, ndims(y))\n",
    "    num_samples = (L - input_window - output_window) ÷ stride + 1\n",
    "\n",
    "    X = zeros(Float32, num_features, input_window, num_samples)\n",
    "    Y = zeros(Float32, num_features, output_window, num_samples)\n",
    "\n",
    "    for ii in 1:num_samples, ff in 1:num_features\n",
    "        start_x = stride * (ii - 1) + 1\n",
    "        end_x = start_x + input_window - 1\n",
    "        X[ff, :, ii] .= y[start_x:end_x]\n",
    "\n",
    "        start_y = stride * (ii - 1) + input_window + 1\n",
    "        end_y = start_y + output_window - 1\n",
    "        Y[ff, :, ii] .= y[start_y:end_y]\n",
    "    end\n",
    "\n",
    "    return X, Y\n",
    "end\n",
    "\n",
    "t, y = synthetic_data()\n",
    "\n",
    "begin\n",
    "    fig = Figure(; size=(1000, 400))\n",
    "    ax = Axis(fig[1, 1]; title=\"Synthetic Time Series\", xlabel=\"t\", ylabel=\"y\")\n",
    "\n",
    "    lines!(ax, t, y; label=\"y\", color=:black, linewidth=2)\n",
    "\n",
    "    fig\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "t_train, y_train, t_test, y_test = train_test_split(t, y)\n",
    "\n",
    "begin\n",
    "    fig = Figure(; size=(1000, 400))\n",
    "    ax = Axis(\n",
    "        fig[1, 1];\n",
    "        title=\"Time Series Split into Train and Test Sets\",\n",
    "        xlabel=\"t\",\n",
    "        ylabel=\"y\",\n",
    "    )\n",
    "\n",
    "    lines!(ax, t_train, y_train[1, :]; label=\"Train\", color=:black, linewidth=2)\n",
    "    lines!(ax, t_test, y_test[1, :]; label=\"Test\", color=:red, linewidth=2)\n",
    "\n",
    "    fig[1, 2] = Legend(fig, ax)\n",
    "\n",
    "    fig\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X_train, Y_train = windowed_dataset(y_train; input_window=80, output_window=20, stride=5)\n",
    "X_test, Y_test = windowed_dataset(y_test; input_window=80, output_window=20, stride=5)\n",
    "\n",
    "begin\n",
    "    fig = Figure(; size=(1000, 400))\n",
    "    ax = Axis(fig[1, 1]; title=\"Example of Windowed Training Data\", xlabel=\"t\", ylabel=\"y\")\n",
    "\n",
    "    linestyles = [:solid, :dash, :dot, :dashdot, :dashdotdot]\n",
    "\n",
    "    for b in 1:4:16\n",
    "        lines!(\n",
    "            ax,\n",
    "            0:79,\n",
    "            X_train[1, :, b];\n",
    "            label=\"Input\",\n",
    "            color=:black,\n",
    "            linewidth=2,\n",
    "            linestyle=linestyles[mod1(b, 5)],\n",
    "        )\n",
    "        lines!(\n",
    "            ax,\n",
    "            79:99,\n",
    "            vcat(X_train[1, end, b], Y_train[1, :, b]);\n",
    "            label=\"Target\",\n",
    "            color=:red,\n",
    "            linewidth=2,\n",
    "            linestyle=linestyles[mod1(b, 5)],\n",
    "        )\n",
    "    end\n",
    "\n",
    "    fig\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct RNNEncoder{C} <: AbstractLuxWrapperLayer{:cell}\n",
    "    cell::C\n",
    "end\n",
    "\n",
    "function (rnn::RNNEncoder)(x::AbstractArray{T,3}, ps, st) where {T}\n",
    "    (y, carry), st = Lux.apply(rnn.cell, x[:, 1, :], ps, st)\n",
    "    @trace for i in 2:size(x, 2)\n",
    "        (y, carry), st = Lux.apply(rnn.cell, (x[:, i, :], carry), ps, st)\n",
    "    end\n",
    "    return (y, carry), st\n",
    "end\n",
    "\n",
    "struct RNNDecoder{C,L} <: AbstractLuxContainerLayer{(:cell, :linear)}\n",
    "    cell::C\n",
    "    linear::L\n",
    "    training_mode::Symbol\n",
    "    teacher_forcing_ratio::Float32\n",
    "\n",
    "    function RNNDecoder(\n",
    "        cell::C,\n",
    "        linear::L;\n",
    "        training_mode::Symbol=:recursive,\n",
    "        teacher_forcing_ratio::Float32=0.5f0,\n",
    "    ) where {C,L}\n",
    "        @assert training_mode in (:recursive, :teacher_forcing, :mixed_teacher_forcing)\n",
    "        return new{C,L}(cell, linear, training_mode, Float32(teacher_forcing_ratio))\n",
    "    end\n",
    "end\n",
    "\n",
    "function LuxCore.initialstates(rng::AbstractRNG, d::RNNDecoder)\n",
    "    return (;\n",
    "        cell=LuxCore.initialstates(rng, d.cell),\n",
    "        linear=LuxCore.initialstates(rng, d.linear),\n",
    "        training=Val(true),\n",
    "        rng,\n",
    "    )\n",
    "end\n",
    "\n",
    "function _teacher_forcing_condition(::Val{false}, x, mode, rng, ratio, target_len)\n",
    "    res = similar(x, Bool, target_len)\n",
    "    fill!(res, true)\n",
    "    return res\n",
    "end\n",
    "function _teacher_forcing_condition(::Val{true}, x, mode, rng, ratio, target_len)\n",
    "    mode === :recursive &&\n",
    "        return _teacher_forcing_condition(Val(false), x, mode, rng, ratio, target_len)\n",
    "    mode === :teacher_forcing && fill(rand(rng, Float32) < ratio, target_len)\n",
    "    return rand(rng, Float32, target_len) .< ratio\n",
    "end\n",
    "\n",
    "function (rnn::RNNDecoder)((decoder_input, carry, target_len, target), ps, st)\n",
    "    @assert ndims(decoder_input) == 2\n",
    "    rng = Lux.replicate(st.rng)\n",
    "\n",
    "    if target === nothing\n",
    "        ### This will be optimized out by Reactant\n",
    "        target = similar(\n",
    "            decoder_input, size(decoder_input, 1), target_len, size(decoder_input, 2)\n",
    "        )\n",
    "        fill!(target, 0)\n",
    "    else\n",
    "        @assert size(target, 2) ≤ target_len\n",
    "    end\n",
    "\n",
    "    (y_latent, carry), st_cell = Lux.apply(\n",
    "        rnn.cell, (decoder_input, carry), ps.cell, st.cell\n",
    "    )\n",
    "    y_pred, st_linear = Lux.apply(rnn.linear, y_latent, ps.linear, st.linear)\n",
    "\n",
    "    y_full = similar(y_pred, size(y_pred, 1), target_len, size(y_pred, 2))\n",
    "    y_full[:, 1, :] = y_pred\n",
    "\n",
    "    conditions = _teacher_forcing_condition(\n",
    "        st.training,\n",
    "        decoder_input,\n",
    "        rnn.training_mode,\n",
    "        rng,\n",
    "        rnn.teacher_forcing_ratio,\n",
    "        target_len,\n",
    "    )\n",
    "    decoder_input = ifelse.(@allowscalar(conditions[1]), target[:, 1, :], y_pred)\n",
    "\n",
    "    @trace for i in 2:target_len\n",
    "        (y_latent, carry), st_cell = Lux.apply(\n",
    "            rnn.cell, (decoder_input, carry), ps.cell, st_cell\n",
    "        )\n",
    "\n",
    "        y_pred, st_linear = Lux.apply(rnn.linear, y_latent, ps.linear, st_linear)\n",
    "        y_full[:, i, :] = y_pred\n",
    "\n",
    "        decoder_input = ifelse.(@allowscalar(conditions[i]), target[:, i, :], y_pred)\n",
    "    end\n",
    "\n",
    "    return y_full, merge(st, (; cell=st_cell, linear=st_linear, rng))\n",
    "end\n",
    "\n",
    "struct RNNEncoderDecoder{C<:RNNEncoder,L<:RNNDecoder} <:\n",
    "       AbstractLuxContainerLayer{(:encoder, :decoder)}\n",
    "    encoder::C\n",
    "    decoder::L\n",
    "end\n",
    "\n",
    "function (rnn::RNNEncoderDecoder)((x, target_len, target), ps, st)\n",
    "    (y, carry), st_encoder = Lux.apply(rnn.encoder, x, ps.encoder, st.encoder)\n",
    "    pred, st_decoder = Lux.apply(\n",
    "        rnn.decoder, (x[:, end, :], carry, target_len, target), ps.decoder, st.decoder\n",
    "    )\n",
    "    return pred, (; encoder=st_encoder, decoder=st_decoder)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function train(\n",
    "    train_dataset,\n",
    "    validation_dataset;\n",
    "    nepochs=50,\n",
    "    batchsize=32,\n",
    "    hidden_dims=32,\n",
    "    training_mode=:mixed_teacher_forcing,\n",
    "    teacher_forcing_ratio=0.5f0,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "    (X_train, Y_train), (X_test, Y_test) = train_dataset, validation_dataset\n",
    "    in_dims = size(X_train, 1)\n",
    "    @assert size(Y_train, 2) == size(Y_test, 2)\n",
    "    target_len = size(Y_train, 2)\n",
    "\n",
    "    train_dataloader =\n",
    "        DataLoader(\n",
    "            (X_train, Y_train);\n",
    "            batchsize=min(batchsize, size(X_train, 4)),\n",
    "            shuffle=true,\n",
    "            partial=false,\n",
    "        ) |> xdev\n",
    "    X_test, Y_test = (X_test, Y_test) |> xdev\n",
    "\n",
    "    model = RNNEncoderDecoder(\n",
    "        RNNEncoder(LSTMCell(in_dims => hidden_dims)),\n",
    "        RNNDecoder(\n",
    "            LSTMCell(in_dims => hidden_dims),\n",
    "            Dense(hidden_dims => in_dims);\n",
    "            training_mode,\n",
    "            teacher_forcing_ratio,\n",
    "        ),\n",
    "    )\n",
    "    ps, st = Lux.setup(Random.default_rng(), model) |> xdev\n",
    "\n",
    "    train_state = Training.TrainState(model, ps, st, Optimisers.Adam(learning_rate))\n",
    "\n",
    "    stime = time()\n",
    "    model_compiled = @compile model((X_test, target_len, nothing), ps, Lux.testmode(st))\n",
    "    ttime = time() - stime\n",
    "    @printf \"Compilation time: %.4f seconds\\n\\n\" ttime\n",
    "\n",
    "    for epoch in 1:nepochs\n",
    "        stime = time()\n",
    "        for (x, y) in train_dataloader\n",
    "            (_, _, _, train_state) = Training.single_train_step!(\n",
    "                AutoEnzyme(),\n",
    "                MSELoss(),\n",
    "                ((x, target_len, y), y),\n",
    "                train_state;\n",
    "                return_gradients=Val(false),\n",
    "            )\n",
    "        end\n",
    "        ttime = time() - stime\n",
    "\n",
    "        y_pred, _ = model_compiled(\n",
    "            (X_test, target_len, nothing),\n",
    "            train_state.parameters,\n",
    "            Lux.testmode(train_state.states),\n",
    "        )\n",
    "        pred_loss = Float32(@jit(MSELoss()(y_pred, Y_test)))\n",
    "\n",
    "        @printf(\n",
    "            \"[%3d/%3d]\\tTime per epoch: %3.5fs\\tValidation Loss: %.4f\\n\",\n",
    "            epoch,\n",
    "            nepochs,\n",
    "            ttime,\n",
    "            pred_loss,\n",
    "        )\n",
    "    end\n",
    "\n",
    "    return StatefulLuxLayer(\n",
    "        model, train_state.parameters |> cdev, train_state.states |> cdev\n",
    "    )\n",
    "end\n",
    "\n",
    "trained_model = train(\n",
    "    (X_train, Y_train),\n",
    "    (X_test, Y_test);\n",
    "    nepochs=50,\n",
    "    batchsize=4,\n",
    "    hidden_dims=32,\n",
    "    training_mode=:mixed_teacher_forcing,\n",
    "    teacher_forcing_ratio=0.5f0,\n",
    "    learning_rate=3e-4,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making Predictions"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Y_pred = trained_model((X_test, 20, nothing))\n",
    "\n",
    "begin\n",
    "    fig = Figure(; size=(1200, 800))\n",
    "\n",
    "    for i in 1:4, j in 1:2\n",
    "        b = i + j * 4\n",
    "        ax = Axis(fig[i, j]; xlabel=\"t\", ylabel=\"y\")\n",
    "        i != 4 && hidexdecorations!(ax; grid=false)\n",
    "        j != 1 && hideydecorations!(ax; grid=false)\n",
    "\n",
    "        lines!(ax, 0:79, X_test[1, :, b]; label=\"Input\", color=:black, linewidth=2)\n",
    "        lines!(\n",
    "            ax,\n",
    "            79:99,\n",
    "            vcat(X_test[1, end, b], Y_test[1, :, b]);\n",
    "            label=\"Ground Truth\\n(Noisy)\",\n",
    "            color=:red,\n",
    "            linewidth=2,\n",
    "        )\n",
    "        lines!(\n",
    "            ax,\n",
    "            79:99,\n",
    "            vcat(X_test[1, end, b], Y_pred[1, :, b]);\n",
    "            label=\"Prediction\",\n",
    "            color=:blue,\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "        i == 4 && j == 2 && axislegend(ax; position=:lb)\n",
    "    end\n",
    "\n",
    "    fig[0, :] = Label(fig, \"Predictions from Trained Model\"; fontsize=20)\n",
    "\n",
    "    fig\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.8"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.8",
   "language": "julia"
  }
 },
 "nbformat": 4
}