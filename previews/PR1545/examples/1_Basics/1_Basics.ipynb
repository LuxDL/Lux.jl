{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Julia & Lux for the Uninitiated"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a quick intro to [Lux](https://github.com/LuxDL/Lux.jl) loosely based on:\n",
    "\n",
    "1. [PyTorch's tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).\n",
    "2. Flux's tutorial (the link for which has now been lost to abyss).\n",
    "3. [Jax's tutorial](https://jax.readthedocs.io/en/latest/jax-101/index.html).\n",
    "\n",
    "It introduces basic Julia programming, as well `Zygote`, a source-to-source automatic\n",
    "differentiation (AD) framework in Julia. We'll use these tools to build a very simple\n",
    "neural network. Let's start with importing `Lux.jl`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Lux, Random"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let us control the randomness in our code using proper Pseudo Random Number\n",
    "Generator (PRNG)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rng = Random.default_rng()\n",
    "Random.seed!(rng, 0)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arrays"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The starting point for all of our models is the `Array` (sometimes referred to as a\n",
    "`Tensor` in other frameworks). This is really just a list of numbers, which might be\n",
    "arranged into a shape like a square. Let's write down an array with three elements."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = [1, 2, 3]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's a matrix – a square array with four elements."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = [1 2; 3 4]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We often work with arrays of thousands of elements, and don't usually write them down by\n",
    "hand. Here's how we can create an array of 5×3 = 15 elements, each a random number from\n",
    "zero to one."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = rand(rng, 5, 3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "There's a few functions like this; try replacing `rand` with `ones`, `zeros`, or `randn`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "By default, Julia works stores numbers is a high-precision format called `Float64`. In ML\n",
    "we often don't need all those digits, and can ask Julia to work with `Float32` instead.\n",
    "We can even ask for more digits using `BigFloat`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = rand(BigFloat, 5, 3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = rand(Float32, 5, 3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can ask the array how many elements it has."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "length(x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, more specifically, what size it has."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "size(x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We sometimes want to see some elements of the array on their own."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x[2, 3]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This means get the second row and the third column. We can also get every row of the third\n",
    "column."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x[:, 3]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can add arrays, and subtract them, which adds or subtracts each element of the array."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x + x"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x - x"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Julia supports a feature called *broadcasting*, using the `.` syntax. This tiles small\n",
    "arrays (or single numbers) to fill bigger ones."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x .+ 1"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see Julia tile the column vector `1:5` across all rows of the larger array."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "zeros(5, 5) .+ (1:5)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The x' syntax is used to transpose a column `1:5` into an equivalent row, and Julia will\n",
    "tile that across columns."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "zeros(5, 5) .+ (1:5)'"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use this to make a times table."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "(1:5) .* (1:5)'"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, and importantly for machine learning, we can conveniently do things like matrix\n",
    "multiply."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "W = randn(5, 10)\n",
    "x = rand(10)\n",
    "W * x"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Julia's arrays are very powerful, and you can learn more about what they can do\n",
    "[here](https://docs.julialang.org/en/v1/manual/arrays/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CUDA Arrays"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "CUDA functionality is provided separately by the\n",
    "[CUDA.jl package](https://github.com/JuliaGPU/CUDA.jl). If you have a GPU and LuxCUDA\n",
    "is installed, Lux will provide CUDA capabilities. For additional details on backends\n",
    "see the manual section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can manually add `CUDA`. Once CUDA is loaded you can move any array to the GPU with\n",
    "the `cu` function (or the `gpu` function exported by `Lux`), and it supports all of the\n",
    "above operations with the same syntax."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```julia\n",
    "using LuxCUDA\n",
    "\n",
    "if LuxCUDA.functional()\n",
    "    x_cu = cu(rand(5, 3))\n",
    "    @show x_cu\n",
    "end\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (Im)mutability"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lux as you might have read is \"Immutable by convention,\"\n",
    "which means that the core library is built without any form of mutation and all functions\n",
    "are pure. However, we don't enforce it in any form. We do **strongly recommend** that\n",
    "users extending this framework for their respective applications don't mutate their\n",
    "arrays."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = reshape(1:8, 2, 4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To update this array, we should first copy the array."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x_copy = copy(x)\n",
    "view(x_copy, :, 1) .= 0\n",
    "\n",
    "println(\"Original Array \", x)\n",
    "println(\"Mutated Array \", x_copy)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that our current default AD engine (Zygote) is unable to differentiate through this\n",
    "mutation, however, for these specialized cases it is quite trivial to write custom\n",
    "backward passes. (This problem will be fixed once we move towards Enzyme.jl)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Managing Randomness"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We rely on the Julia StdLib `Random` for managing the randomness in our execution. First,\n",
    "we create an PRNG (pseudorandom number generator) and seed it."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rng = Xoshiro(0)     # Creates a Xoshiro PRNG with seed 0"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we call any function that relies on `rng` and uses it via `randn`, `rand`, etc. `rng`\n",
    "will be mutated. As we have already established we care a lot about immutability, hence we\n",
    "should use `Lux.replicate` on PRNGs before using them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let us run a random number generator 3 times with the `replicate`d rng."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "random_vectors = Vector{Vector{Float64}}(undef, 3)\n",
    "for i in 1:3\n",
    "    random_vectors[i] = rand(Lux.replicate(rng), 10)\n",
    "    println(\"Iteration $i \", random_vectors[i])\n",
    "end\n",
    "@assert random_vectors[1] ≈ random_vectors[2] ≈ random_vectors[3]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected we get the same output. We can remove the `replicate` call and we will get\n",
    "different outputs."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for i in 1:3\n",
    "    println(\"Iteration $i \", rand(rng, 10))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Automatic Differentiation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Julia has quite a few (maybe too many) AD tools. For the purpose of this tutorial, we will\n",
    "use:\n",
    "\n",
    "1. [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) -- For Jacobian-Vector\n",
    "   Product (JVP)\n",
    "2. [Zygote.jl](https://github.com/FluxML/Zygote.jl) -- For Vector-Jacobian Product (VJP)\n",
    "\n",
    "*Slight Detour*: We have had several questions regarding if we will be considering any\n",
    "other AD system for the reverse-diff backend. For now we will stick to Zygote.jl, however\n",
    "once we have tested Lux extensively with [Enzyme.jl](https://github.com/EnzymeAD/Enzyme.jl),\n",
    "we will make the switch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Even though, theoretically, a VJP (Vector-Jacobian product - reverse autodiff) and a JVP\n",
    "(Jacobian-Vector product - forward-mode autodiff) are similar—they compute a product of a\n",
    "Jacobian and a vector—they differ by the computational complexity of the operation. In\n",
    "short, when you have a large number of parameters (hence a wide matrix), a JVP is less\n",
    "efficient computationally than a VJP, and, conversely, a JVP is more efficient when the\n",
    "Jacobian matrix is a tall matrix."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ComponentArrays, ForwardDiff, Zygote"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradients"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For our first example, consider a simple function computing $f(x) = \\frac{1}{2}x^T x$,\n",
    "where $\\nabla f(x) = x$"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f(x) = x' * x / 2\n",
    "∇f(x) = x  # `∇` can be typed as `\\nabla<TAB>`\n",
    "v = randn(rng, Float32, 4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's use ForwardDiff and Zygote to compute the gradients."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "println(\"Actual Gradient: \", ∇f(v))\n",
    "println(\"Computed Gradient via Reverse Mode AD (Zygote): \", only(Zygote.gradient(f, v)))\n",
    "println(\"Computed Gradient via Forward Mode AD (ForwardDiff): \", ForwardDiff.gradient(f, v))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that `AD.gradient` will only work for scalar valued outputs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Jacobian-Vector Product"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I will defer the discussion on forward-mode AD to\n",
    "<https://book.sciml.ai/notes/08-Forward-Mode_Automatic_Differentiation_(AD)_via_High_Dimensional_Algebras/>.\n",
    "Here let us just look at a mini example on how to use it."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f(x) = x .* x ./ 2\n",
    "x = randn(rng, Float32, 5)\n",
    "v = ones(Float32, 5)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "!!! warning \"Using DifferentiationInterface\"\n",
    "\n",
    "    While DifferentiationInterface provides these functions for a wider range of backends,\n",
    "    we currently don't recommend using them with Lux models, since the functions presented\n",
    "    here come with additional goodies like\n",
    "    fast second-order derivatives."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute the JVP. `AutoForwardDiff` specifies that we want to use `ForwardDiff.jl` for the\n",
    "Jacobian-Vector Product"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "jvp = jacobian_vector_product(f, AutoForwardDiff(), x, v)\n",
    "println(\"JVP: \", jvp)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vector-Jacobian Product"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the same function and inputs, let us compute the Vector-Jacobian Product (VJP)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "vjp = vector_jacobian_product(f, AutoZygote(), x, v)\n",
    "println(\"VJP: \", vjp)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, now let us consider a linear regression problem. From a set of data-points\n",
    "$\\{ (x_i, y_i), i \\in \\{ 1, \\dots, k \\}, x_i \\in \\mathbb{R}^n, y_i \\in \\mathbb{R}^m \\}$,\n",
    "we try to find a set of parameters $W$ and $b$, such that $f_{W,b}(x) = Wx + b$, which\n",
    "minimizes the mean squared error:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$L(W, b) \\longrightarrow \\sum_{i = 1}^{k} \\frac{1}{2} \\| y_i - f_{W,b}(x_i) \\|_2^2$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can write `f` from scratch, but to demonstrate `Lux`, let us use the `Dense` layer."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Dense(10 => 5)\n",
    "\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(rng, 0)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us initialize the parameters and states (in this case it is empty) for the model."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ps, st = Lux.setup(rng, model)\n",
    "ps = ComponentArray(ps)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set problem dimensions."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We're going to generate a random set of weights `W` and biases `b` that will act as our\n",
    "true model (also known as the ground truth). The neural network we'll train will be to try\n",
    "and approximate `W` and `b` from example data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "W = randn(rng, Float32, y_dim, x_dim)\n",
    "b = randn(rng, Float32, y_dim)\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate samples with additional noise."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x_samples = randn(rng, Float32, x_dim, n_samples)\n",
    "y_samples = W * x_samples .+ b .+ 0.01f0 .* randn(rng, Float32, y_dim, n_samples)\n",
    "println(\"x shape: \", size(x_samples), \"; y shape: \", size(y_samples))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For updating our parameters let's use\n",
    "[Optimisers.jl](https://github.com/FluxML/Optimisers.jl). We will use Stochastic Gradient\n",
    "Descent (SGD) with a learning rate of `0.01`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Optimisers, Printf"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the loss function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lossfn = MSELoss()\n",
    "\n",
    "println(\"Loss Value with ground true parameters: \", lossfn(W * x_samples .+ b, y_samples))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will train the model using our training API."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function train_model!(model, ps, st, opt, nepochs::Int)\n",
    "    tstate = Training.TrainState(model, ps, st, opt)\n",
    "    for i in 1:nepochs\n",
    "        grads, loss, _, tstate = Training.single_train_step!(\n",
    "            AutoZygote(), lossfn, (x_samples, y_samples), tstate\n",
    "        )\n",
    "        if i == 1 || i % 1000 == 0 || i == nepochs\n",
    "            @printf \"Loss Value after %6d iterations: %.8f\\n\" i loss\n",
    "        end\n",
    "    end\n",
    "    return tstate.model, tstate.parameters, tstate.states\n",
    "end\n",
    "\n",
    "model, ps, st = train_model!(model, ps, st, Descent(0.01f0), 10000)\n",
    "\n",
    "println(\"Loss Value after training: \", lossfn(first(model(x_samples, ps, st)), y_samples))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.8"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.8",
   "language": "julia"
  }
 },
 "nbformat": 4
}