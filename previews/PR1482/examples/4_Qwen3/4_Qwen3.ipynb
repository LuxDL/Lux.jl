{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Qwen3 Implementation from Scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is an implementation of Qwen 3 ([blog](https://qwenlm.github.io/blog/qwen3/) and\n",
    "[technical report](https://arxiv.org/abs/2505.09388)) from scratch based on the pytorch\n",
    "[implementation](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/11_qwen3/standalone-qwen3.ipynb)\n",
    "[developed in Pytorch under the Apache License 2.0](https://github.com/rasbt/LLMs-from-scratch/blob/main/LICENSE.txt)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Package Imports"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using BFloat16s, ConcreteStructs, LinearAlgebra, Lux, Random, Reactant\n",
    "using HuggingFaceTokenizers, PythonCall, SafeTensors, Scratch, JSON3\n",
    "\n",
    "const huggingface_hub = pyimport(\"huggingface_hub\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Qwen3 Configuration"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@kwdef struct Qwen3Config{F}\n",
    "    version::String\n",
    "    vocab_size::Int\n",
    "    context_length::Int\n",
    "    emb_dim::Int\n",
    "    n_heads::Int\n",
    "    n_layers::Int\n",
    "    hidden_dim::Int\n",
    "    head_dim::Int\n",
    "    n_kv_groups::Int\n",
    "    rope_base::Float32\n",
    "    dtype::F\n",
    "    reasoning_model::Bool = true\n",
    "end\n",
    "\n",
    "function Qwen3Config(version::String; kwargs...)\n",
    "    if version == \"0.6B\"\n",
    "        return Qwen3Config(;\n",
    "            version,\n",
    "            vocab_size=151_936,\n",
    "            context_length=40_960,\n",
    "            emb_dim=1024,\n",
    "            n_heads=16,\n",
    "            n_layers=28,\n",
    "            hidden_dim=3072,\n",
    "            head_dim=128,\n",
    "            n_kv_groups=8,\n",
    "            rope_base=1.0f6,\n",
    "            dtype=bf16,\n",
    "            kwargs...,\n",
    "        )\n",
    "    elseif version == \"1.7B\"\n",
    "        return Qwen3Config(;\n",
    "            version,\n",
    "            vocab_size=151_936,\n",
    "            context_length=40_960,\n",
    "            emb_dim=2048,\n",
    "            n_heads=16,\n",
    "            n_layers=28,\n",
    "            hidden_dim=6144,\n",
    "            head_dim=128,\n",
    "            n_kv_groups=8,\n",
    "            rope_base=1.0f6,\n",
    "            dtype=bf16,\n",
    "            kwargs...,\n",
    "        )\n",
    "    elseif version == \"4B\"\n",
    "        return Qwen3Config(;\n",
    "            version,\n",
    "            vocab_size=151_936,\n",
    "            context_length=40_960,\n",
    "            emb_dim=2560,\n",
    "            n_heads=32,\n",
    "            n_layers=36,\n",
    "            hidden_dim=9728,\n",
    "            head_dim=128,\n",
    "            n_kv_groups=8,\n",
    "            rope_base=1.0f6,\n",
    "            dtype=bf16,\n",
    "            kwargs...,\n",
    "        )\n",
    "    elseif version == \"8B\"\n",
    "        return Qwen3Config(;\n",
    "            version,\n",
    "            vocab_size=151_936,\n",
    "            context_length=40_960,\n",
    "            emb_dim=4096,\n",
    "            n_heads=32,\n",
    "            n_layers=36,\n",
    "            hidden_dim=12288,\n",
    "            head_dim=128,\n",
    "            n_kv_groups=8,\n",
    "            rope_base=1.0f6,\n",
    "            dtype=bf16,\n",
    "            kwargs...,\n",
    "        )\n",
    "    elseif version == \"14B\"\n",
    "        return Qwen3Config(;\n",
    "            version,\n",
    "            vocab_size=151_936,\n",
    "            context_length=40_960,\n",
    "            emb_dim=5120,\n",
    "            n_heads=40,\n",
    "            n_layers=40,\n",
    "            hidden_dim=17408,\n",
    "            head_dim=128,\n",
    "            n_kv_groups=8,\n",
    "            rope_base=1.0f6,\n",
    "            dtype=bf16,\n",
    "            kwargs...,\n",
    "        )\n",
    "    elseif version == \"32B\"\n",
    "        return Qwen3Config(;\n",
    "            version,\n",
    "            vocab_size=151_936,\n",
    "            context_length=40_960,\n",
    "            emb_dim=5120,\n",
    "            n_heads=64,\n",
    "            n_layers=64,\n",
    "            hidden_dim=25600,\n",
    "            head_dim=128,\n",
    "            n_kv_groups=8,\n",
    "            rope_base=1.0f6,\n",
    "            dtype=bf16,\n",
    "            kwargs...,\n",
    "        )\n",
    "    end\n",
    "\n",
    "    throw(ArgumentError(\"Unknown Qwen3 version $version\"))\n",
    "end\n",
    "\n",
    "fn_to_dtype(::Type{T}) where {T} = T\n",
    "fn_to_dtype(::typeof(f16)) = Float16\n",
    "fn_to_dtype(::typeof(f32)) = Float32\n",
    "fn_to_dtype(::typeof(f64)) = Float64\n",
    "fn_to_dtype(::typeof(bf16)) = BFloat16"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Definition"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function Qwen3MLP(cfg::Qwen3Config)\n",
    "    return Chain(;\n",
    "        proj=Parallel(\n",
    "            .*;\n",
    "            gate_proj=Dense(cfg.emb_dim => cfg.hidden_dim, swish; use_bias=false),\n",
    "            up_proj=Dense(cfg.emb_dim => cfg.hidden_dim; use_bias=false),\n",
    "        ),\n",
    "        down_proj=Dense(cfg.hidden_dim => cfg.emb_dim; use_bias=false),\n",
    "        name=\"Qwen3MLP\",\n",
    "    )\n",
    "end\n",
    "\n",
    "Qwen3RMSNorm(emb_dim::Int, eps) = AlternatePrecision{Float32}(RMSNorm(emb_dim; epsilon=eps))\n",
    "\n",
    "@concrete struct GroupedQueryAttention <: AbstractLuxContainerLayer{(\n",
    "    :q_proj, :k_proj, :v_proj, :o_proj, :q_norm, :k_norm\n",
    ")}\n",
    "    q_proj\n",
    "    k_proj\n",
    "    v_proj\n",
    "    o_proj\n",
    "    q_norm\n",
    "    k_norm\n",
    "    d_in::Int\n",
    "    num_heads::Int\n",
    "    num_kv_groups::Int\n",
    "    head_dim::Int\n",
    "end\n",
    "\n",
    "function GroupedQueryAttention(d_in, num_heads, num_kv_groups; head_dim=nothing)\n",
    "    @assert num_heads % num_kv_groups == 0 \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "    if head_dim === nothing\n",
    "        @assert d_in % num_heads == 0 \"`d_in` must be divisible by `num_heads` if \\\n",
    "                                       `head_dim` is not set\"\n",
    "        head_dim = d_in รท num_heads\n",
    "    end\n",
    "\n",
    "    d_out = num_heads * head_dim\n",
    "\n",
    "    return GroupedQueryAttention(\n",
    "        Dense(d_in, d_out; use_bias=false),\n",
    "        Dense(d_in, num_kv_groups * head_dim; use_bias=false),\n",
    "        Dense(d_in, num_kv_groups * head_dim; use_bias=false),\n",
    "        Dense(d_out, d_in; use_bias=false),\n",
    "        Qwen3RMSNorm(head_dim, 1.0f-6),\n",
    "        Qwen3RMSNorm(head_dim, 1.0f-6),\n",
    "        d_in,\n",
    "        num_heads,\n",
    "        num_kv_groups,\n",
    "        head_dim,\n",
    "    )\n",
    "end\n",
    "\n",
    "function apply_rope(x::AbstractArray{T}, cos_cache, sin_cache) where {T}\n",
    "    return T.(apply_rotary_embedding(x, cos_cache, sin_cache; seq_dim=3))\n",
    "end\n",
    "\n",
    "function (attn::GroupedQueryAttention)((x, cos_cache, sin_cache), ps, st::NamedTuple)\n",
    "    _, num_tokens, B = size(x)\n",
    "\n",
    "    # apply projections\n",
    "    queries, st_q_proj = attn.q_proj(x, ps.q_proj, st.q_proj)\n",
    "    keys, st_k_proj = attn.k_proj(x, ps.k_proj, st.k_proj)\n",
    "    values, st_v_proj = attn.v_proj(x, ps.v_proj, st.v_proj)\n",
    "\n",
    "    # reshape and permute to (head_dim, num_heads/num_kv_groups, num_tokens, batch)\n",
    "    queries = reshape(queries, attn.head_dim, attn.num_heads, num_tokens, B)\n",
    "    keys = reshape(keys, attn.head_dim, attn.num_kv_groups, num_tokens, B)\n",
    "    values = reshape(values, attn.head_dim, attn.num_kv_groups, num_tokens, B)\n",
    "\n",
    "    # apply normalization\n",
    "    queries, st_q_norm = attn.q_norm(queries, ps.q_norm, st.q_norm)\n",
    "    keys, st_k_norm = attn.k_norm(keys, ps.k_norm, st.k_norm)\n",
    "\n",
    "    # apply RoPE\n",
    "    queries = apply_rope(queries, cos_cache, sin_cache)\n",
    "    keys = apply_rope(keys, cos_cache, sin_cache)\n",
    "\n",
    "    # attention\n",
    "    context = reshape(\n",
    "        scaled_dot_product_attention(\n",
    "            queries, keys, values; head_dim=1, token_dim=3, is_causal=true\n",
    "        )[1],\n",
    "        attn.head_dim * attn.num_heads,\n",
    "        num_tokens,\n",
    "        B,\n",
    "    )\n",
    "\n",
    "    # output projection\n",
    "    proj, st_o_proj = attn.o_proj(context, ps.o_proj, st.o_proj)\n",
    "\n",
    "    return (\n",
    "        proj,\n",
    "        (;\n",
    "            q_proj=st_q_proj,\n",
    "            k_proj=st_k_proj,\n",
    "            v_proj=st_v_proj,\n",
    "            o_proj=st_o_proj,\n",
    "            q_norm=st_q_norm,\n",
    "            k_norm=st_k_norm,\n",
    "        ),\n",
    "    )\n",
    "end\n",
    "\n",
    "@concrete struct Qwen3Attention <: AbstractLuxContainerLayer{(\n",
    "    :self_attn, :mlp, :input_layernorm, :post_attention_layernorm\n",
    ")}\n",
    "    self_attn <: GroupedQueryAttention\n",
    "    mlp\n",
    "    input_layernorm\n",
    "    post_attention_layernorm\n",
    "end\n",
    "\n",
    "function Qwen3Attention(cfg::Qwen3Config)\n",
    "    return Qwen3Attention(\n",
    "        GroupedQueryAttention(cfg.emb_dim, cfg.n_heads, cfg.n_kv_groups; cfg.head_dim),\n",
    "        Qwen3MLP(cfg),\n",
    "        Qwen3RMSNorm(cfg.emb_dim, 1.0f-6),\n",
    "        Qwen3RMSNorm(cfg.emb_dim, 1.0f-6),\n",
    "    )\n",
    "end\n",
    "\n",
    "function (block::Qwen3Attention)((x, cos_cache, sin_cache), ps, st::NamedTuple)\n",
    "    # shortcut connection for attention block\n",
    "    shortcut = x\n",
    "    x, st_norm1 = block.input_layernorm(x, ps.input_layernorm, st.input_layernorm)\n",
    "    x, st_attn = block.self_attn((x, cos_cache, sin_cache), ps.self_attn, st.self_attn)\n",
    "    x = x .+ shortcut\n",
    "\n",
    "    # shortcut connection for feed-forward block\n",
    "    shortcut = x\n",
    "    x, st_norm2 = block.post_attention_layernorm(\n",
    "        x, ps.post_attention_layernorm, st.post_attention_layernorm\n",
    "    )\n",
    "    x, st_ff = block.mlp(x, ps.mlp, st.mlp)\n",
    "    x = x .+ shortcut\n",
    "\n",
    "    return (\n",
    "        x,\n",
    "        (;\n",
    "            self_attn=st_attn,\n",
    "            mlp=st_ff,\n",
    "            input_layernorm=st_norm1,\n",
    "            post_attention_layernorm=st_norm2,\n",
    "        ),\n",
    "    )\n",
    "end\n",
    "\n",
    "@concrete struct Qwen3 <:\n",
    "                 AbstractLuxContainerLayer{(:embed_tokens, :blocks, :norm, :lm_head)}\n",
    "    embed_tokens\n",
    "    blocks\n",
    "    norm\n",
    "    lm_head\n",
    "    cfg::Qwen3Config\n",
    "end\n",
    "\n",
    "function Qwen3(cfg::Qwen3Config)\n",
    "    return Qwen3(\n",
    "        Embedding(cfg.vocab_size => cfg.emb_dim),\n",
    "        Tuple([Qwen3Attention(cfg) for _ in 1:(cfg.n_layers)]),\n",
    "        Qwen3RMSNorm(cfg.emb_dim, 1.0f-6),\n",
    "        Dense(cfg.emb_dim, cfg.vocab_size; use_bias=false),\n",
    "        cfg,\n",
    "    )\n",
    "end\n",
    "\n",
    "function LuxCore.initialstates(rng::AbstractRNG, m::Qwen3)\n",
    "    head_dim = m.cfg.head_dim === nothing ? m.cfg.emb_dim รท m.cfg.n_heads : m.cfg.head_dim\n",
    "    (; cos_cache, sin_cache) = compute_rotary_embedding_params(\n",
    "        head_dim, m.cfg.context_length; base=m.cfg.rope_base, dtype=Float32\n",
    "    )\n",
    "    return (;\n",
    "        cos_cache,\n",
    "        sin_cache,\n",
    "        embed_tokens=LuxCore.initialstates(rng, m.embed_tokens),\n",
    "        blocks=LuxCore.initialstates(rng, m.blocks),\n",
    "        norm=LuxCore.initialstates(rng, m.norm),\n",
    "        lm_head=LuxCore.initialstates(rng, m.lm_head),\n",
    "    )\n",
    "end\n",
    "\n",
    "function (qwen3::Qwen3)(in_idx, ps, st::NamedTuple)\n",
    "    x, st_embed_tokens = qwen3.embed_tokens(in_idx, ps.embed_tokens, st.embed_tokens)\n",
    "\n",
    "    st_blocks = ()\n",
    "    for (i, block) in enumerate(qwen3.blocks)\n",
    "        x, st_block_new = block((x, st.cos_cache, st.sin_cache), ps.blocks[i], st.blocks[i])\n",
    "        st_blocks = (st_blocks..., st_block_new)\n",
    "    end\n",
    "    x, st_norm = qwen3.norm(x, ps.norm, st.norm)\n",
    "    logits, st_lm_head = qwen3.lm_head(\n",
    "        fn_to_dtype(qwen3.cfg.dtype).(x), ps.lm_head, st.lm_head\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        logits,\n",
    "        (;\n",
    "            cos_cache=st.cos_cache,\n",
    "            sin_cache=st.sin_cache,\n",
    "            embed_tokens=st_embed_tokens,\n",
    "            blocks=st_blocks,\n",
    "            norm=st_norm,\n",
    "            lm_head=st_lm_head,\n",
    "        ),\n",
    "    )\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Weights and Tokenizer from HuggingFace"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function download_qwen3_weights_from_huggingface(cfg::Qwen3Config)\n",
    "    return download_qwen3_weights_from_huggingface(cfg.reasoning_model, cfg.version)\n",
    "end\n",
    "\n",
    "function download_qwen3_weights_from_huggingface(use_reasoning_model::Bool, version::String)\n",
    "    repo_id = \"Qwen/Qwen3-$(version)\" * (use_reasoning_model ? \"\" : \"-Base\")\n",
    "    local_dir = @get_scratch!(\"Qwen3-$(version)-$(use_reasoning_model)\")\n",
    "\n",
    "    tokenizer_file = huggingface_hub.hf_hub_download(;\n",
    "        repo_id=repo_id, filename=\"tokenizer.json\", local_dir=local_dir\n",
    "    )\n",
    "\n",
    "    if version == \"0.6B\"\n",
    "        weights_file = huggingface_hub.hf_hub_download(;\n",
    "            repo_id=repo_id, filename=\"model.safetensors\", local_dir=local_dir\n",
    "        )\n",
    "        weights_dict = load_safetensors(string(weights_file))\n",
    "    else\n",
    "        repo_dir = huggingface_hub.snapshot_download(; repo_id=repo_id, local_dir=local_dir)\n",
    "        index_path = joinpath(string(repo_dir), \"model.safetensors.index.json\")\n",
    "\n",
    "        index = JSON3.read(index_path)\n",
    "\n",
    "        weights_dict = Dict()\n",
    "        for filename in Set(values(index[\"weight_map\"]))\n",
    "            shard_path = joinpath(string(repo_dir), filename)\n",
    "            shard = load_safetensors(shard_path)\n",
    "            merge!(weights_dict, shard)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return weights_dict, string(tokenizer_file), repo_id\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Qwen3 Tokenizer"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct Qwen3Tokenizer\n",
    "    tokenizer::Tokenizer\n",
    "    special_to_id::Dict{String,Int32}\n",
    "    pad_token_id::Int32\n",
    "    eos_token_id::Int32\n",
    "    apply_chat_template::Bool\n",
    "    add_generation_prompt::Bool\n",
    "    add_thinking::Bool\n",
    "end\n",
    "\n",
    "function Base.show(io::IO, tokenizer::Qwen3Tokenizer)\n",
    "    return print(\n",
    "        io,\n",
    "        \"Qwen3Tokenizer(apply_chat_template=$(tokenizer.apply_chat_template), add_generation_prompt=$(tokenizer.add_generation_prompt), add_thinking=$(tokenizer.add_thinking))\",\n",
    "    )\n",
    "end\n",
    "\n",
    "const SPECIALS = [\n",
    "    \"<|endoftext|>\",\n",
    "    \"<|im_start|>\",\n",
    "    \"<|im_end|>\",\n",
    "    \"<|object_ref_start|>\",\n",
    "    \"<|object_ref_end|>\",\n",
    "    \"<|box_start|>\",\n",
    "    \"<|box_end|>\",\n",
    "    \"<|quad_start|>\",\n",
    "    \"<|quad_end|>\",\n",
    "    \"<|vision_start|>\",\n",
    "    \"<|vision_end|>\",\n",
    "    \"<|vision_pad|>\",\n",
    "    \"<|image_pad|>\",\n",
    "    \"<|video_pad|>\",\n",
    "]\n",
    "\n",
    "const SPLIT_RE = r\"(<\\|[^>]+?\\|>)\"\n",
    "\n",
    "token_to_id(tokenizer::Qwen3Tokenizer, s) = token_to_id(tokenizer.tokenizer, s)\n",
    "function token_to_id(tokenizer::Tokenizer, s)\n",
    "    return pyconvert(Int32, tokenizer.py_tokenizer.token_to_id(s)) + Int32(1)\n",
    "end\n",
    "\n",
    "function split_with_delims(text::String, re::Regex)\n",
    "    parts = String[]\n",
    "    last_end = 1\n",
    "    for m in eachmatch(re, text)\n",
    "        if m.offset > last_end\n",
    "            push!(parts, text[last_end:(m.offset - 1)])\n",
    "        elseif m.offset == 1\n",
    "            push!(parts, \"\")\n",
    "        end\n",
    "        push!(parts, m.match)\n",
    "        last_end = m.offset + length(m.match)\n",
    "    end\n",
    "    if last_end โค lastindex(text)\n",
    "        push!(parts, text[last_end:end])\n",
    "    end\n",
    "    return parts\n",
    "end\n",
    "\n",
    "function Qwen3Tokenizer(\n",
    "    tokenizer_file_path::String;\n",
    "    repo_id=nothing,\n",
    "    apply_chat_template::Bool=true,\n",
    "    add_generation_prompt::Bool=false,\n",
    "    add_thinking::Bool=false,\n",
    ")\n",
    "    tok = HuggingFaceTokenizers.from_file(Tokenizer, tokenizer_file_path)\n",
    "    special_to_id = Dict(s => token_to_id(tok, s) for s in SPECIALS)\n",
    "    pad_token_id = special_to_id[\"<|endoftext|>\"]\n",
    "    eos_token_id = pad_token_id\n",
    "    if repo_id !== nothing && !occursin(\"Base\", repo_id)\n",
    "        eos_token = \"<|im_end|>\"\n",
    "    else\n",
    "        eos_token = \"<|endoftext|>\"\n",
    "    end\n",
    "    if haskey(special_to_id, eos_token)\n",
    "        eos_token_id = special_to_id[eos_token]\n",
    "    end\n",
    "    return Qwen3Tokenizer(\n",
    "        tok,\n",
    "        special_to_id,\n",
    "        pad_token_id,\n",
    "        eos_token_id,\n",
    "        apply_chat_template,\n",
    "        add_generation_prompt,\n",
    "        add_thinking,\n",
    "    )\n",
    "end\n",
    "\n",
    "function wrap_chat(tokenizer::Qwen3Tokenizer, user_msg::AbstractString)\n",
    "    s = \"<|im_start|>user\\n$(user_msg)<|im_end|>\\n\"\n",
    "    if tokenizer.add_generation_prompt\n",
    "        s *= \"<|im_start|>assistant\"\n",
    "        if tokenizer.add_thinking\n",
    "            s *= \"\\n\"\n",
    "        else\n",
    "            s *= \"\\n<think>\\n\\n</think>\\n\\n\"\n",
    "        end\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "function HuggingFaceTokenizers.encode(\n",
    "    tok::Qwen3Tokenizer, text; chat_wrapped::Bool=tok.apply_chat_template\n",
    ")\n",
    "    stripped = strip(text)\n",
    "    if haskey(tok.special_to_id, stripped) && !occursin('\\n', stripped)\n",
    "        return [tok.special_to_id[stripped]]\n",
    "    end\n",
    "\n",
    "    chat_wrapped && (text = wrap_chat(tok, text))\n",
    "\n",
    "    ids = Int32[]\n",
    "    for part in filter(!isempty, split_with_delims(text, SPLIT_RE))\n",
    "        if haskey(tok.special_to_id, part)\n",
    "            push!(ids, tok.special_to_id[part])\n",
    "        else\n",
    "            append!(ids, encode(tok.tokenizer, string(part)).ids .+ Int16(1))\n",
    "        end\n",
    "    end\n",
    "    return ids\n",
    "end\n",
    "\n",
    "function HuggingFaceTokenizers.decode(tok::Qwen3Tokenizer, ids::Vector{<:Integer})\n",
    "    return decode(tok.tokenizer, ids .- Int16(1); skip_special_tokens=false)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pretrained Model Weights"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "get_weights_tensor(tensor::AbstractArray, ::Type{T}) where {T} = collect(T, tensor)\n",
    "\n",
    "function get_weights_tensor(dict, key, dtype::Type{T}, dev; permute::Bool=false) where {T}\n",
    "    tensor = dict[key]\n",
    "    if permute\n",
    "        tensor = permutedims(tensor, Tuple(reverse(1:ndims(tensor))))\n",
    "    end\n",
    "    return get_weights_tensor(tensor, dtype) |> dev\n",
    "end\n",
    "\n",
    "function load_weights_from_dict(weights_dict, cfg::Qwen3Config, dev)\n",
    "    dtype = fn_to_dtype(cfg.dtype)\n",
    "\n",
    "    function get_tensor(key; kwargs...)\n",
    "        return get_weights_tensor(weights_dict, key, dtype, dev; kwargs...)\n",
    "    end\n",
    "\n",
    "    embed_tokens = (; weight=get_tensor(\"model.embed_tokens.weight\"; permute=true))\n",
    "\n",
    "    blocks = Vector{Any}(undef, cfg.n_layers)\n",
    "    for l in 1:(cfg.n_layers)\n",
    "        prefix = \"model.layers.$(l - 1)\"\n",
    "        sa_prefix = \"$(prefix).self_attn\"\n",
    "\n",
    "        blocks[l] = (;\n",
    "            self_attn=merge(\n",
    "                NamedTuple(\n",
    "                    k => (; weight=get_tensor(\"$(sa_prefix).$k.weight\")) for\n",
    "                    k in (:q_proj, :k_proj, :v_proj, :o_proj)\n",
    "                ),\n",
    "                NamedTuple(\n",
    "                    k => (; scale=get_tensor(\"$(sa_prefix).$k.weight\")) for\n",
    "                    k in (:q_norm, :k_norm)\n",
    "                ),\n",
    "            ),\n",
    "            mlp=(;\n",
    "                proj=(;\n",
    "                    gate_proj=(; weight=get_tensor(\"$(prefix).mlp.gate_proj.weight\")),\n",
    "                    up_proj=(; weight=get_tensor(\"$(prefix).mlp.up_proj.weight\")),\n",
    "                ),\n",
    "                down_proj=(; weight=get_tensor(\"$(prefix).mlp.down_proj.weight\")),\n",
    "            ),\n",
    "            input_layernorm=(; scale=get_tensor(\"$(prefix).input_layernorm.weight\")),\n",
    "            post_attention_layernorm=(;\n",
    "                scale=get_tensor(\"$(prefix).post_attention_layernorm.weight\")\n",
    "            ),\n",
    "        )\n",
    "    end\n",
    "    blocks = Tuple(blocks)\n",
    "\n",
    "    norm = (; scale=get_weights_tensor(weights_dict, \"model.norm.weight\", dtype, dev))\n",
    "\n",
    "    if haskey(weights_dict, \"lm_head.weight\")\n",
    "        lm_head = (; weight=get_weights_tensor(weights_dict, \"lm_head.weight\", dtype, dev))\n",
    "    else\n",
    "        # Weight tying with the embedding matrix. We will share the weights here to\n",
    "        # reduce memory usage.\n",
    "        lm_head = (; weight=transpose(embed_tokens.weight))\n",
    "    end\n",
    "\n",
    "    return (; embed_tokens, blocks, norm, lm_head)\n",
    "end\n",
    "\n",
    "function setup_model(\n",
    "    version::String, dev; weights_dict::Union{Nothing,Dict}=nothing, kwargs...\n",
    ")\n",
    "    return setup_model(Qwen3Config(version; kwargs...), dev; weights_dict)\n",
    "end\n",
    "\n",
    "function setup_model(cfg::Qwen3Config, dev; weights_dict::Union{Nothing,Dict}=nothing)\n",
    "    model = Qwen3(cfg)\n",
    "\n",
    "    st = Lux.initialstates(Random.default_rng(), model) |> dev\n",
    "\n",
    "    if weights_dict !== nothing\n",
    "        ps = load_weights_from_dict(weights_dict, cfg, dev)\n",
    "    else\n",
    "        ps = Lux.initialparameters(Random.default_rng(), model) |> dev\n",
    "        ps = ps |> cfg.dtype\n",
    "    end\n",
    "\n",
    "    return model, ps, st\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running the model without dynamic sizes"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function get_padded_size(seq_len::Int, context_length::Int)\n",
    "    return min(max(512, nextpow(2, seq_len)), context_length)\n",
    "end\n",
    "\n",
    "function padded_input_and_mask_len(x::AbstractMatrix, v, cfg::Qwen3Config, pad_token_id)\n",
    "    return padded_input_and_mask_len(\n",
    "        x, v, get_padded_size(size(x, 1) + v !== nothing, cfg.context_length), pad_token_id\n",
    "    )\n",
    "end\n",
    "\n",
    "function padded_input_and_mask_len(x::AbstractMatrix, v, padded_sz::Int, pad_token_id)\n",
    "    if padded_sz > size(x, 1)\n",
    "        x_padded = similar(x, (padded_sz, size(x, 2)))\n",
    "        x_padded[1:size(x, 1), :] .= x\n",
    "        if v === nothing\n",
    "            x_padded[(size(x, 1) + 1):end, :] .= pad_token_id\n",
    "        else\n",
    "            x_padded[(size(x, 1) + 1), :] = v[1, :]\n",
    "            x_padded[(size(x, 1) + 2):end, :] .= pad_token_id\n",
    "        end\n",
    "    else\n",
    "        x_padded = x\n",
    "    end\n",
    "    return (\n",
    "        x_padded,\n",
    "        Reactant.TracedUtils.promote_to(\n",
    "            Reactant.TracedRNumber{Int32}, padded_sz - (size(x, 1) + (v !== nothing))\n",
    "        ),\n",
    "    )\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helpers to generate text"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function predict_next_token(\n",
    "    model, token_ids::AbstractMatrix{T}, input_mask_len, ps, st\n",
    ") where {T}\n",
    "    logits, stโ = model(token_ids, ps, st)\n",
    "    predictions = T.(argmax(logits[:, end - input_mask_len, :]; dims=1))\n",
    "    predictions = mod1.(predictions, T(size(logits, 1)))\n",
    "    return predictions, stโ\n",
    "end\n",
    "\n",
    "function update_token_ids_and_mask!(\n",
    "    padded_token_ids, input_mask_len, cur_num_tokens, next_token\n",
    ")\n",
    "    next_token_idx = safe_increment(cur_num_tokens)\n",
    "    padded_token_ids[next_token_idx, :] = next_token[1, :]\n",
    "    return input_mask_len - eltype(input_mask_len)(1), next_token_idx\n",
    "end\n",
    "\n",
    "function update_token_ids_with_shift!(token_ids, next_token)\n",
    "    token_ids[1:(end - 1), :] = token_ids[2:end, :]\n",
    "    token_ids[end, :] = next_token[1, :]\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "safe_increment(x) = x + one(x)\n",
    "\n",
    "mutable struct CachedReactantThunks\n",
    "    cache::Dict{Qwen3Config,Dict{Int,NTuple{3,Reactant.Compiler.Thunk}}}\n",
    "    increment_fn::Union{Nothing,Reactant.Compiler.Thunk}\n",
    "end\n",
    "\n",
    "function CachedReactantThunks()\n",
    "    return CachedReactantThunks(\n",
    "        Dict{Qwen3Config,Dict{Int,NTuple{3,Reactant.Compiler.Thunk}}}(), nothing\n",
    "    )\n",
    "end\n",
    "\n",
    "function cache_and_retrieve!(\n",
    "    cache::CachedReactantThunks,\n",
    "    len::Integer,\n",
    "    model::Qwen3,\n",
    "    padded_token_ids,\n",
    "    input_mask_len,\n",
    "    ps,\n",
    "    st,\n",
    "    next_token,\n",
    "    cur_num_tokens_traced,\n",
    ")\n",
    "    if haskey(cache.cache, model.cfg) && haskey(cache.cache[model.cfg], len)\n",
    "        return cache.cache[model.cfg][len]\n",
    "    end\n",
    "\n",
    "    println()\n",
    "    @warn \"Compiling Qwen3 generation loop for $(model.cfg.version) with $(len) tokens. \\\n",
    "           This might take a while... (However this is only done once per model per length)\"\n",
    "\n",
    "    predict_next_token_compiled = @compile predict_next_token(\n",
    "        model, padded_token_ids, input_mask_len, ps, st\n",
    "    )\n",
    "    update_fn1! = @compile update_token_ids_and_mask!(\n",
    "        padded_token_ids, input_mask_len, cur_num_tokens_traced, next_token\n",
    "    )\n",
    "    update_fn2! = @compile update_token_ids_with_shift!(padded_token_ids, next_token)\n",
    "\n",
    "    if !haskey(cache.cache, model.cfg)\n",
    "        cache.cache[model.cfg] = Dict{Int,NTuple{3,Reactant.Compiler.Thunk}}()\n",
    "    end\n",
    "\n",
    "    return cache.cache[model.cfg][len] = (\n",
    "        predict_next_token_compiled, update_fn1!, update_fn2!\n",
    "    )\n",
    "end\n",
    "\n",
    "const CACHED_THUNKS = CachedReactantThunks()\n",
    "\n",
    "generate_text(args...; kwargs...) = generate_text!(CACHED_THUNKS, args...; kwargs...)\n",
    "\n",
    "function generate_text!(\n",
    "    compile_cache::CachedReactantThunks,\n",
    "    model::Qwen3,\n",
    "    prompt::String,\n",
    "    ps,\n",
    "    st,\n",
    "    max_new_tokens,\n",
    "    tokenizer,\n",
    ")\n",
    "    token_ids = Reactant.to_rarray(reshape(encode(tokenizer, prompt), :, 1))\n",
    "\n",
    "    # TODO: compile the generation loop with Reactant\n",
    "    # TODO: implement some simple KV caching\n",
    "    cur_num_tokens = size(token_ids, 1)\n",
    "    max_context_length = model.cfg.context_length\n",
    "    cur_compiled_fn_token_len = get_padded_size(cur_num_tokens, max_context_length)\n",
    "\n",
    "    padded_token_ids, input_mask_len = @jit padded_input_and_mask_len(\n",
    "        token_ids, nothing, cur_compiled_fn_token_len, tokenizer.pad_token_id\n",
    "    )\n",
    "    cur_num_tokens_traced = ConcreteRNumber{Int32}(cur_num_tokens)\n",
    "\n",
    "    next_token = get_device(ps)(rand(Int32, 1, size(padded_token_ids, 2)))\n",
    "\n",
    "    (predict_next_token_compiled, update_fn1!, update_fn2!) = cache_and_retrieve!(\n",
    "        compile_cache,\n",
    "        cur_compiled_fn_token_len,\n",
    "        model,\n",
    "        padded_token_ids,\n",
    "        input_mask_len,\n",
    "        ps,\n",
    "        st,\n",
    "        next_token,\n",
    "        cur_num_tokens_traced,\n",
    "    )\n",
    "\n",
    "    if compile_cache.increment_fn === nothing\n",
    "        compile_cache.increment_fn = @compile safe_increment(cur_num_tokens_traced)\n",
    "    end\n",
    "\n",
    "    start_time = time()\n",
    "    compile_time = 0.0\n",
    "    ntokens_generated = 0\n",
    "\n",
    "    for _ in 1:max_new_tokens\n",
    "        new_compiled_fn_token_len = get_padded_size(cur_num_tokens, max_context_length)\n",
    "        if new_compiled_fn_token_len != cur_compiled_fn_token_len\n",
    "            compile_start_time = time()\n",
    "            cur_compiled_fn_token_len = new_compiled_fn_token_len\n",
    "            padded_token_ids, input_mask_len = @jit padded_input_and_mask_len(\n",
    "                padded_token_ids,\n",
    "                next_token,\n",
    "                cur_compiled_fn_token_len,\n",
    "                tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "            (predict_next_token_compiled, update_fn1!, update_fn2!) = cache_and_retrieve!(\n",
    "                compile_cache,\n",
    "                cur_compiled_fn_token_len,\n",
    "                model,\n",
    "                padded_token_ids,\n",
    "                input_mask_len,\n",
    "                ps,\n",
    "                st,\n",
    "                next_token,\n",
    "                cur_num_tokens_traced,\n",
    "            )\n",
    "            compile_time += time() - compile_start_time\n",
    "        end\n",
    "\n",
    "        next_token, st = predict_next_token_compiled(\n",
    "            model, padded_token_ids, input_mask_len, ps, st\n",
    "        )\n",
    "\n",
    "        ntokens_generated += 1\n",
    "\n",
    "        next_token_jl = vec(Array(next_token))\n",
    "\n",
    "        if tokenizer.eos_token_id !== nothing &&\n",
    "            all(next_token_jl .== tokenizer.eos_token_id)\n",
    "            break\n",
    "        end\n",
    "\n",
    "        print(decode(tokenizer, next_token_jl))\n",
    "\n",
    "        if cur_num_tokens >= max_context_length\n",
    "            update_fn2!(padded_token_ids, next_token)\n",
    "        elseif new_compiled_fn_token_len > cur_num_tokens\n",
    "            input_mask_len, cur_num_tokens_traced = update_fn1!(\n",
    "                padded_token_ids, input_mask_len, cur_num_tokens_traced, next_token\n",
    "            )\n",
    "        else\n",
    "            cur_num_tokens_traced = compile_cache.increment_fn(cur_num_tokens_traced)\n",
    "        end\n",
    "        cur_num_tokens += 1\n",
    "    end\n",
    "    total_time = time() - start_time\n",
    "\n",
    "    println()\n",
    "    return ntokens_generated / (total_time - compile_time)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entry Point"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function run_model_selection()\n",
    "    printstyled(\"Which model do you want to run? \\n\"; color=:cyan, bold=true)\n",
    "    choices = [\"0.6B\", \"1.7B\", \"4B\", \"8B\", \"14B\", \"32B\"]\n",
    "    for (i, choice) in enumerate(choices)\n",
    "        printstyled(\"    $(i). $(choice)\\n\"; color=:light_blue)\n",
    "    end\n",
    "    printstyled(\"  Enter your choice: \"; color=:cyan)\n",
    "    choice = parse(Int, readline(stdin))\n",
    "    if choice โ 1:length(choices)\n",
    "        error(\"Invalid choice: $(choice). Expected an integer between 1 and \\\n",
    "               $(length(choices))\")\n",
    "    end\n",
    "\n",
    "    printstyled(\"Do you want to use the reasoning model? [y/N] \"; color=:cyan)\n",
    "    reasoning = readline(stdin) == \"y\"\n",
    "    println()\n",
    "    return choices[choice], reasoning\n",
    "end\n",
    "\n",
    "function get_model_and_tokenizer(version, reasoning)\n",
    "    cfg = Qwen3Config(version; reasoning_model=reasoning)\n",
    "    rdev = reactant_device(; force=true)\n",
    "    weights_dict, tokenizer_file, repo_id = download_qwen3_weights_from_huggingface(cfg)\n",
    "    tokenizer = Qwen3Tokenizer(\n",
    "        tokenizer_file;\n",
    "        repo_id,\n",
    "        add_generation_prompt=cfg.reasoning_model,\n",
    "        add_thinking=cfg.reasoning_model,\n",
    "    )\n",
    "    model, ps, st = setup_model(cfg, rdev; weights_dict)\n",
    "    return model, ps, st, tokenizer\n",
    "end\n",
    "\n",
    "function main()\n",
    "    @info \"Text Generation with Qwen-3 powered by Lux, Reactant & XLA.\"\n",
    "\n",
    "    version, reasoning = run_model_selection()\n",
    "    model, ps, st, tokenizer = get_model_and_tokenizer(version, reasoning)\n",
    "\n",
    "    while true\n",
    "        printstyled(\n",
    "            \"Prompt (type \\\"exit\\\" to quit the program or \\\n",
    "             \\\"model selection\\\" to change the model): \";\n",
    "            color=:cyan,\n",
    "            bold=true,\n",
    "        )\n",
    "        prompt = readline(stdin)\n",
    "\n",
    "        prompt == \"exit\" && break\n",
    "\n",
    "        if prompt == \"model selection\"\n",
    "            version, reasoning = run_model_selection()\n",
    "            model, ps, st, tokenizer = get_model_and_tokenizer(version, reasoning)\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        tokens_per_second = generate_text(model, prompt, ps, st, 100_000, tokenizer)\n",
    "        println(\"\\nTokens per second: $tokens_per_second\\n\\n\")\n",
    "    end\n",
    "\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "if abspath(PROGRAM_FILE) == @__FILE__\n",
    "    main()\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.7",
   "language": "julia"
  }
 },
 "nbformat": 4
}
