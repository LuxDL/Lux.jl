{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ResNet20 on CIFAR-10"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Package Imports"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Comonicon, Lux, Optimisers, Printf, Random, Statistics"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set some global flags that will improve performance"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "XLA_FLAGS = get(ENV, \"XLA_FLAGS\", \"\")\n",
    "ENV[\"XLA_FLAGS\"] = \"$(XLA_FLAGS) --xla_gpu_enable_cublaslt=true\""
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Common Packages"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ConcreteStructs,\n",
    "    DataAugmentation,\n",
    "    ImageShow,\n",
    "    Lux,\n",
    "    MLDatasets,\n",
    "    MLUtils,\n",
    "    OneHotArrays,\n",
    "    Printf,\n",
    "    ProgressTables,\n",
    "    Random,\n",
    "    BFloat16s\n",
    "using Reactant"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading Functionality"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@concrete struct TensorDataset\n",
    "    dataset\n",
    "    transform\n",
    "end\n",
    "\n",
    "Base.length(ds::TensorDataset) = length(ds.dataset)\n",
    "\n",
    "function Base.getindex(ds::TensorDataset, idxs::Union{Vector{<:Integer},AbstractRange})\n",
    "    img = Image.(eachslice(convert2image(ds.dataset, idxs); dims=3))\n",
    "    y = onehotbatch(ds.dataset.targets[idxs], 0:9)\n",
    "    return stack(parent ∘ itemdata ∘ Base.Fix1(apply, ds.transform), img), y\n",
    "end\n",
    "\n",
    "function get_cifar10_dataloaders(::Type{T}, batchsize; kwargs...) where {T}\n",
    "    cifar10_mean = T.((0.4914, 0.4822, 0.4465))\n",
    "    cifar10_std = T.((0.2471, 0.2435, 0.2616))\n",
    "\n",
    "    train_transform =\n",
    "        RandomResizeCrop((32, 32)) |>\n",
    "        Maybe(FlipX{2}()) |>\n",
    "        ImageToTensor() |>\n",
    "        Normalize(cifar10_mean, cifar10_std) |>\n",
    "        ToEltype(T)\n",
    "\n",
    "    test_transform = ImageToTensor() |> Normalize(cifar10_mean, cifar10_std) |> ToEltype(T)\n",
    "\n",
    "    trainset = TensorDataset(CIFAR10(; Tx=T, split=:train), train_transform)\n",
    "    trainloader = DataLoader(trainset; batchsize, shuffle=true, kwargs...)\n",
    "\n",
    "    testset = TensorDataset(CIFAR10(; Tx=T, split=:test), test_transform)\n",
    "    testloader = DataLoader(testset; batchsize, shuffle=false, kwargs...)\n",
    "\n",
    "    return trainloader, testloader\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utility Functions"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function accuracy(model, ps, st, dataloader)\n",
    "    total_correct, total = 0, 0\n",
    "    cdev = cpu_device()\n",
    "    for (x, y) in dataloader\n",
    "        target_class = onecold(cdev(y))\n",
    "        predicted_class = onecold(cdev(first(model(x, ps, st))))\n",
    "        total_correct += sum(target_class .== predicted_class)\n",
    "        total += length(target_class)\n",
    "    end\n",
    "    return total_correct / total\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function train_model(\n",
    "    model,\n",
    "    opt,\n",
    "    scheduler=nothing;\n",
    "    batchsize::Int=512,\n",
    "    seed::Int=1234,\n",
    "    epochs::Int=25,\n",
    "    bfloat16::Bool=false,\n",
    ")\n",
    "    rng = Random.default_rng()\n",
    "    Random.seed!(rng, seed)\n",
    "\n",
    "    prec = bfloat16 ? bf16 : f32\n",
    "    prec_jl = bfloat16 ? BFloat16 : Float32\n",
    "    prec_str = bfloat16 ? \"BFloat16\" : \"Float32\"\n",
    "    @printf \"[Info] Using %s precision\\n\" prec_str\n",
    "\n",
    "    dev = reactant_device(; force=true)\n",
    "\n",
    "    trainloader, testloader =\n",
    "        get_cifar10_dataloaders(prec_jl, batchsize; partial=false) |> dev\n",
    "\n",
    "    ps, st = prec(Lux.setup(rng, model)) |> dev\n",
    "\n",
    "    train_state = Training.TrainState(model, ps, st, opt)\n",
    "\n",
    "    x_ra = rand(rng, prec_jl, size(first(trainloader)[1])) |> dev\n",
    "    @printf \"[Info] Compiling model with Reactant.jl\\n\"\n",
    "    model_compiled = Reactant.with_config(;\n",
    "        dot_general_precision=PrecisionConfig.HIGH,\n",
    "        convolution_precision=PrecisionConfig.HIGH,\n",
    "    ) do\n",
    "        @compile model(x_ra, ps, Lux.testmode(st))\n",
    "    end\n",
    "    @printf \"[Info] Model compiled!\\n\"\n",
    "\n",
    "    loss_fn = CrossEntropyLoss(; logits=Val(true))\n",
    "\n",
    "    pt = ProgressTable(;\n",
    "        header=[\n",
    "            \"Epoch\", \"Learning Rate\", \"Train Accuracy (%)\", \"Test Accuracy (%)\", \"Time (s)\"\n",
    "        ],\n",
    "        widths=[24, 24, 24, 24, 24],\n",
    "        format=[\"%3d\", \"%.6f\", \"%.6f\", \"%.6f\", \"%.6f\"],\n",
    "        color=[:normal, :normal, :blue, :blue, :normal],\n",
    "        border=true,\n",
    "        alignment=[:center, :center, :center, :center, :center],\n",
    "    )\n",
    "\n",
    "    @printf \"[Info] Training model\\n\"\n",
    "    initialize(pt)\n",
    "\n",
    "    for epoch in 1:epochs\n",
    "        stime = time()\n",
    "        lr = 0\n",
    "        for (i, (x, y)) in enumerate(trainloader)\n",
    "            if scheduler !== nothing\n",
    "                lr = scheduler((epoch - 1) + (i + 1) / length(trainloader))\n",
    "                train_state = Optimisers.adjust!(train_state, lr)\n",
    "            end\n",
    "            (_, loss, _, train_state) = Training.single_train_step!(\n",
    "                AutoEnzyme(), loss_fn, (x, y), train_state; return_gradients=Val(false)\n",
    "            )\n",
    "            isnan(loss) && error(\"NaN loss encountered!\")\n",
    "        end\n",
    "        ttime = time() - stime\n",
    "\n",
    "        train_acc =\n",
    "            accuracy(\n",
    "                model_compiled,\n",
    "                train_state.parameters,\n",
    "                Lux.testmode(train_state.states),\n",
    "                trainloader,\n",
    "            ) * 100\n",
    "        test_acc =\n",
    "            accuracy(\n",
    "                model_compiled,\n",
    "                train_state.parameters,\n",
    "                Lux.testmode(train_state.states),\n",
    "                testloader,\n",
    "            ) * 100\n",
    "\n",
    "        scheduler === nothing && (lr = NaN32)\n",
    "        next(pt, [epoch, lr, train_acc, test_acc, ttime])\n",
    "    end\n",
    "\n",
    "    finalize(pt)\n",
    "    return @printf \"[Info] Finished training\\n\"\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Definition"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function ConvBN(kernel_size, (in_chs, out_chs), act; kwargs...)\n",
    "    return Chain(Conv(kernel_size, in_chs => out_chs, act; kwargs...), BatchNorm(out_chs))\n",
    "end\n",
    "\n",
    "function BasicBlock(in_channels, out_channels; stride=1)\n",
    "    connection = if (stride == 1 && in_channels == out_channels)\n",
    "        NoOpLayer()\n",
    "    else\n",
    "        Conv((3, 3), in_channels => out_channels, identity; stride=stride, pad=SamePad())\n",
    "    end\n",
    "    return Chain(\n",
    "        Parallel(\n",
    "            +,\n",
    "            connection,\n",
    "            Chain(\n",
    "                ConvBN((3, 3), in_channels => out_channels, relu; stride, pad=SamePad()),\n",
    "                ConvBN((3, 3), out_channels => out_channels, identity; pad=SamePad()),\n",
    "            ),\n",
    "        ),\n",
    "        Base.BroadcastFunction(relu),\n",
    "    )\n",
    "end\n",
    "\n",
    "function ResNet20(; num_classes=10)\n",
    "    layers = []\n",
    "\n",
    "    # Initial Conv Layer\n",
    "    push!(layers, Chain(Conv((3, 3), 3 => 16, relu; pad=SamePad()), BatchNorm(16)))\n",
    "\n",
    "    # Residual Blocks\n",
    "    block_configs = [\n",
    "        # (in_channels, out_channels, num_blocks, stride)\n",
    "        (16, 16, 3, 1),\n",
    "        (16, 32, 3, 2),\n",
    "        (32, 64, 3, 2),\n",
    "    ]\n",
    "\n",
    "    for (in_channels, out_channels, num_blocks, stride) in block_configs\n",
    "        for i in 1:num_blocks\n",
    "            push!(\n",
    "                layers,\n",
    "                BasicBlock(\n",
    "                    i == 1 ? in_channels : out_channels,\n",
    "                    out_channels;\n",
    "                    stride=(i == 1 ? stride : 1),\n",
    "                ),\n",
    "            )\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Global Pooling and Final Dense Layer\n",
    "    push!(layers, GlobalMeanPool())\n",
    "    push!(layers, FlattenLayer())\n",
    "    push!(layers, Dense(64 => num_classes))\n",
    "\n",
    "    return Chain(layers...)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entry Point"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Comonicon.@main function main(;\n",
    "    batchsize::Int=512,\n",
    "    weight_decay::Float64=0.0001,\n",
    "    clip_norm::Bool=false,\n",
    "    seed::Int=1234,\n",
    "    epochs::Int=100,\n",
    "    lr::Float64=0.001,\n",
    "    bfloat16::Bool=false,\n",
    ")\n",
    "    model = ResNet20()\n",
    "\n",
    "    opt = AdamW(; eta=lr, lambda=weight_decay)\n",
    "    clip_norm && (opt = OptimiserChain(ClipNorm(), opt))\n",
    "\n",
    "    lr_schedule = nothing\n",
    "\n",
    "    return train_model(model, opt, lr_schedule; batchsize, seed, epochs, bfloat16)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.7",
   "language": "julia"
  }
 },
 "nbformat": 4
}
