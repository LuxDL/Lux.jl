import{_ as t,c as r,o as a,al as o}from"./chunks/framework.BPWKQDOF.js";const m=JSON.parse('{"title":"Why we wrote Lux?","description":"","frontmatter":{},"headers":[],"relativePath":"introduction/overview.md","filePath":"introduction/overview.md","lastUpdated":null}'),i={name:"introduction/overview.md"};function s(n,e,l,p,d,u){return a(),r("div",null,[...e[0]||(e[0]=[o('<h1 id="Why-we-wrote-Lux?" tabindex="-1">Why we wrote Lux? <a class="header-anchor" href="#Why-we-wrote-Lux?" aria-label="Permalink to &quot;Why we wrote Lux? {#Why-we-wrote-Lux?}&quot;">​</a></h1><p>Julia already has quite a few well established Neural Network Frameworks – <a href="https://fluxml.ai/" target="_blank" rel="noreferrer">Flux</a> &amp; <a href="https://denizyuret.github.io/Knet.jl/latest/" target="_blank" rel="noreferrer">KNet</a>. However, certain design elements – <strong>Coupled Model and Parameters</strong> &amp; <strong>Internal Mutations</strong> – associated with these frameworks make them less compiler and user friendly. Making changes to address these problems in the respective frameworks would be too disruptive for users. Here comes in <code>Lux</code>: a neural network framework built completely using pure functions to make it both compiler and autodiff friendly.</p><h2 id="Performance-and-Deployment-with-Reactant" tabindex="-1">Performance and Deployment with Reactant <a class="header-anchor" href="#Performance-and-Deployment-with-Reactant" aria-label="Permalink to &quot;Performance and Deployment with Reactant {#Performance-and-Deployment-with-Reactant}&quot;">​</a></h2><p>Lux.jl takes a <strong>Reactant-first approach</strong> to deliver exceptional performance and seamless deployment capabilities:</p><ul><li><p><strong>XLA Compilation</strong> – Lux models compile to highly optimized XLA code via <a href="https://github.com/EnzymeAD/Reactant.jl" target="_blank" rel="noreferrer">Reactant.jl</a>, delivering significant speedups on CPU, GPU, and TPU.</p></li><li><p><strong>Cross-Platform Performance</strong> – Run the same Lux model with optimal performance across different hardware backends (CPU, NVIDIA GPUs, AMD GPUs, TPUs) without code changes, simply by switching the Reactant backend.</p></li><li><p><strong>Production Deployment</strong> – Compiled models can be exported and deployed to production servers and edge devices by leveraging the rich TensorFlow ecosystem, making Lux suitable for real-world applications.</p></li><li><p><strong>Large Model Support</strong> – With Reactant compilation, Lux now excels at training very large models that were previously challenging, making it competitive with other frameworks for large-scale deep learning.</p></li></ul><h2 id="Design-Principles" tabindex="-1">Design Principles <a class="header-anchor" href="#Design-Principles" aria-label="Permalink to &quot;Design Principles {#Design-Principles}&quot;">​</a></h2><ul><li><p><strong>Layers must be immutable</strong> – cannot store any parameter/state but rather store the information to construct them</p></li><li><p><strong>Layers are pure functions</strong></p></li><li><p><strong>Layers return a Tuple containing the result and the updated state</strong></p></li><li><p><strong>Given same inputs the outputs must be same</strong> – yes this must hold true even for stochastic functions. Randomness must be controlled using <code>rng</code>s passed in the state.</p></li><li><p><strong>Easily extensible</strong></p></li><li><p><strong>Extensive Testing</strong> – All layers and features are tested across all supported AD backends across all supported hardware backends.</p></li></ul><h2 id="Why-use-Lux-over-Flux?" tabindex="-1">Why use Lux over Flux? <a class="header-anchor" href="#Why-use-Lux-over-Flux?" aria-label="Permalink to &quot;Why use Lux over Flux? {#Why-use-Lux-over-Flux?}&quot;">​</a></h2><ul><li><p><strong>High-Performance XLA Compilation</strong> – Lux&#39;s Reactant-first approach enables XLA compilation for dramatic performance improvements across CPU, GPU, and TPU. Models compile to highly optimized code that eliminates Julia overhead and leverages hardware-specific optimizations.</p></li><li><p><strong>Production-Ready Deployment</strong> – Deploy Lux models to production environments using the mature TensorFlow ecosystem. Compiled models can be exported and run on servers, edge devices, and mobile platforms.</p></li><li><p><strong>Neural Networks for SciML</strong>: For SciML Applications (Neural ODEs, Deep Equilibrium Models) solvers typically expect a monolithic parameter vector. Flux enables this via its <code>destructure</code> mechanism, but <code>destructure</code> comes with various <a href="https://fluxml.ai/Optimisers.jl/dev/api/#Optimisers.destructure" target="_blank" rel="noreferrer">edge cases and limitations</a>. Lux forces users to make an explicit distinction between state variables and parameter variables to avoid these issues. Also, it comes battery-included for distributed training.</p></li><li><p><strong>Sensible display of Custom Layers</strong> – Ever wanted to see Pytorch like Network printouts or wondered how to extend the pretty printing of Flux&#39;s layers? Lux handles all of that by default.</p></li><li><p><strong>Truly immutable models</strong> - No <em>unexpected internal mutations</em> since all layers are implemented as pure functions. All layers are also <em>deterministic</em> given the parameters and state: if a layer is supposed to be stochastic (say <a href="/previews/PR1628/api/Lux/layers#Lux.Dropout"><code>Lux.Dropout</code></a>), the state must contain a seed which is then updated after the function call.</p></li><li><p><strong>Easy Parameter Manipulation</strong> – By separating parameter data and layer structures, Lux makes implementing <a href="/previews/PR1628/api/Lux/layers#Lux.WeightNorm"><code>WeightNorm</code></a>, <code>SpectralNorm</code>, etc. downright trivial. Without this separation, it is much harder to pass such parameters around without mutations which AD systems don&#39;t like.</p></li><li><p><strong>Wider AD Support</strong> – Lux has extensive support for most <a href="/previews/PR1628/manual/autodiff#autodiff-lux">AD systems in julia</a>, while Flux is mostly tied to Zygote (with some initial support for Enzyme).</p></li><li><p><strong>Optimized for All Model Sizes</strong> – Whether you&#39;re working with small prototypes or large production models, Lux delivers optimal performance. For the smallest networks where minimal overhead is critical, you can use <a href="/previews/PR1628/api/Lux/interop#Lux.ToSimpleChainsAdaptor"><code>ToSimpleChainsAdaptor</code></a> to leverage SimpleChains.jl&#39;s specialized CPU optimizations.</p></li><li><p><strong>Reliability</strong> – We have learned from the mistakes of the past with Flux and everything in our core framework is extensively tested, along with downstream CI to ensure that everything works as expected.</p></li></ul>',9)])])}const h=t(i,[["render",s]]);export{m as __pageData,h as default};
