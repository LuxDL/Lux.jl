     2006 ```@meta
    32448 EditURL = "../../../../examples/NeuralODE/main.jl"
    38444 ```
    44066 
     1010 # MNIST Classification using Neural ODEs
     1918 
     1272 To understand Neural ODEs, users should look up
      185 [these lecture notes](https://book.sciml.ai/notes/11-Differentiable_Programming_and_Neural_Differential_Equations/). We recommend users to directly use
      105 [DiffEqFlux.jl](https://docs.sciml.ai/DiffEqFlux/stable/), instead of implementing
     1699 Neural ODEs from scratch.
       95 
      107 ## Package Imports
      604 
      590 ````julia
      607 using Lux, ComponentArrays, SciMLSensitivity, LuxAMDGPU, LuxCUDA, Optimisers,
      721       OrdinaryDiffEq, Random, Statistics, Zygote, OneHotArrays, InteractiveUtils
      910 import MLDatasets: MNIST
    11430 import MLUtils: DataLoader, splitobs
      430 
      323 CUDA.allowscalar(false)
       45 ````
       45 
      495 ## Loading MNIST
      495 
     2475 ````julia
        - function loadmnist(batchsize, train_split)
      495     # Load MNIST: Only 1500 for demonstration purposes
      495     N = 1500
      495     dataset = MNIST(; split=:train)
       45     imgs = dataset.features[:, :, 1:N]
        -     labels_raw = dataset.targets[1:N]
       90 
        -     # Process images into (H,W,C,BS) batches
        -     x_data = Float32.(reshape(imgs, size(imgs, 1), size(imgs, 2), 1, size(imgs, 3)))
       45     y_data = onehotbatch(labels_raw, 0:9)
        -     (x_train, y_train), (x_test, y_test) = splitobs((x_data, y_data); at=train_split)
        - 
        -     return (
        -         # Use DataLoader to automatically minibatch and shuffle the data
        -         DataLoader(collect.((x_train, y_train)); batchsize, shuffle=true),
        -         # Don't shuffle the test data
        -         DataLoader(collect.((x_test, y_test)); batchsize, shuffle=false))
        - end
        - ````
        - 
        - ````
        - loadmnist (generic function with 1 method)
        - ````
        - 
        - ## Define the Neural ODE Layer
        - 
        - The NeuralODE is a ContainerLayer, which stores a `model`. The parameters and states of
        - the NeuralODE are same as those of the underlying model.
        - 
        - ````julia
        - struct NeuralODE{M <: Lux.AbstractExplicitLayer, So, Se, T, K} <:
        -        Lux.AbstractExplicitContainerLayer{(:model,)}
        -     model::M
        -     solver::So
        -     sensealg::Se
        -     tspan::T
        -     kwargs::K
        - end
        - 
        - function NeuralODE(model::Lux.AbstractExplicitLayer; solver=Tsit5(), tspan=(0.0f0, 1.0f0),
        -         sensealg=InterpolatingAdjoint(; autojacvec=ZygoteVJP()), kwargs...)
        -     return NeuralODE(model, solver, sensealg, tspan, kwargs)
        - end
        - ````
        - 
        - ````
        - Main.var"##225".NeuralODE
        - ````
        - 
        - OrdinaryDiffEq.jl can deal with non-Vector Inputs! However, certain discrete sensitivities
        - like `ReverseDiffAdjoint` can't handle non-Vector inputs. Hence, we need to convert the
        - input and output of the ODE solver to a Vector.
        - 
        - ````julia
        - function (n::NeuralODE)(x, ps, st)
        -     function dudt(u, p, t)
        -         u_, st = n.model(reshape(u, size(x)), p, st)
        -         return vec(u_)
        -     end
        -     prob = ODEProblem{false}(ODEFunction{false}(dudt), vec(x), n.tspan, ps)
        -     return solve(prob, n.solver; sensealg=n.sensealg, n.kwargs...), st
        - end
        - 
        - @views diffeqsol_to_array(l::Int, x::ODESolution) = reshape(last(x.u), (l, :))
        - @views diffeqsol_to_array(l::Int, x::AbstractMatrix) = reshape(x[:, end], (l, :))
        - ````
        - 
        - ````
        - diffeqsol_to_array (generic function with 2 methods)
        - ````
        - 
        - ## Create and Initialize the Neural ODE Layer
        - 
        - ````julia
        - function create_model(model_fn=NeuralODE; dev=gpu_device(), use_named_tuple::Bool=false,
        -         sensealg=InterpolatingAdjoint(; autojacvec=ZygoteVJP()))
        -     # Construct the Neural ODE Model
        -     model = Chain(FlattenLayer(),
        -         Dense(784 => 20, tanh),
        -         model_fn(Chain(Dense(20 => 10, tanh), Dense(10 => 10, tanh), Dense(10 => 20, tanh));
        -             save_everystep=false, reltol=1.0f-3,
        -             abstol=1.0f-3, save_start=false, sensealg),
        -         Base.Fix1(diffeqsol_to_array, 20),
        -         Dense(20 => 10))
        - 
        -     rng = Random.default_rng()
        -     Random.seed!(rng, 0)
        - 
        -     ps, st = Lux.setup(rng, model)
        -     ps = (use_named_tuple ? ps : ComponentArray(ps)) |> dev
        -     st = st |> dev
        - 
        -     return model, ps, st
        - end
        - ````
        - 
        - ````
        - create_model (generic function with 2 methods)
        - ````
        - 
        - ## Define Utility Functions
        - 
        - ````julia
        - logitcrossentropy(y_pred, y) = mean(-sum(y .* logsoftmax(y_pred); dims=1))
        - 
        - function loss(x, y, model, ps, st)
        -     y_pred, st = model(x, ps, st)
        -     return logitcrossentropy(y_pred, y), st
        - end
        - 
        - function accuracy(model, ps, st, dataloader; dev=gpu_device())
        -     total_correct, total = 0, 0
        -     st = Lux.testmode(st)
        -     cpu_dev = cpu_device()
        -     for (x, y) in dataloader
        -         target_class = onecold(y)
        -         predicted_class = onecold(cpu_dev(first(model(dev(x), ps, st))))
        -         total_correct += sum(target_class .== predicted_class)
        -         total += length(target_class)
        -     end
        -     return total_correct / total
        - end
        - ````
        - 
        - ````
        - accuracy (generic function with 1 method)
        - ````
        - 
        - ## Training
        - 
        - ````julia
        - function train(model_function; cpu::Bool=false, kwargs...)
        -     dev = cpu ? cpu_device() : gpu_device()
        -     model, ps, st = create_model(model_function; dev, kwargs...)
        - 
        -     # Training
        -     train_dataloader, test_dataloader = loadmnist(128, 0.9)
        - 
        -     opt = Adam(0.001f0)
        -     st_opt = Optimisers.setup(opt, ps)
        - 
        -     ### Warmup the Model
        -     img = dev(train_dataloader.data[1][:, :, :, 1:1])
        -     lab = dev(train_dataloader.data[2][:, 1:1])
        -     loss(img, lab, model, ps, st)
        -     (l, _), back = pullback(p -> loss(img, lab, model, p, st), ps)
        -     back((one(l), nothing))
        - 
        -     ### Lets train the model
        -     nepochs = 9
        -     for epoch in 1:nepochs
        -         stime = time()
        -         for (x, y) in train_dataloader
        -             x = dev(x)
        -             y = dev(y)
        -             (l, st), back = pullback(p -> loss(x, y, model, p, st), ps)
        -             ### We need to add `nothing`s equal to the number of returned values - 1
        -             gs = back((one(l), nothing))[1]
        -             st_opt, ps = Optimisers.update(st_opt, ps, gs)
        -         end
        -         ttime = time() - stime
        - 
        -         println("[$epoch/$nepochs] \t Time $(round(ttime; digits=2))s \t Training Accuracy: " *
        -                 "$(round(accuracy(model, ps, st, train_dataloader; dev) * 100; digits=2))% \t " *
        -                 "Test Accuracy: $(round(accuracy(model, ps, st, test_dataloader; dev) * 100; digits=2))%")
        -     end
        - end
        - 
        - train(NeuralODE)
        - ````
        - 
        - ````
        - [1/9] 	 Time 5.6s 	 Training Accuracy: 50.74% 	 Test Accuracy: 45.33%
        - [2/9] 	 Time 0.37s 	 Training Accuracy: 70.74% 	 Test Accuracy: 66.67%
        - [3/9] 	 Time 0.59s 	 Training Accuracy: 77.85% 	 Test Accuracy: 71.33%
        - [4/9] 	 Time 0.58s 	 Training Accuracy: 81.04% 	 Test Accuracy: 75.33%
        - [5/9] 	 Time 0.5s 	 Training Accuracy: 82.67% 	 Test Accuracy: 78.0%
        - [6/9] 	 Time 0.75s 	 Training Accuracy: 84.15% 	 Test Accuracy: 78.67%
        - [7/9] 	 Time 0.35s 	 Training Accuracy: 85.48% 	 Test Accuracy: 80.67%
        - [8/9] 	 Time 0.46s 	 Training Accuracy: 86.81% 	 Test Accuracy: 82.0%
        - [9/9] 	 Time 0.61s 	 Training Accuracy: 87.41% 	 Test Accuracy: 84.0%
        - 
        - ````
        - 
        - We can also change the sensealg and train the model! `GaussAdjoint` allows you to use
        - any arbitrary parameter structure and not just a flat vector (`ComponentArray`).
        - 
        - ````julia
        - train(NeuralODE; sensealg=GaussAdjoint(; autojacvec=ZygoteVJP()), use_named_tuple=true)
        - ````
        - 
        - ````
        - [1/9] 	 Time 4.29s 	 Training Accuracy: 47.33% 	 Test Accuracy: 39.33%
        - [2/9] 	 Time 0.71s 	 Training Accuracy: 69.19% 	 Test Accuracy: 63.33%
        - [3/9] 	 Time 0.32s 	 Training Accuracy: 75.78% 	 Test Accuracy: 71.33%
        - [4/9] 	 Time 0.28s 	 Training Accuracy: 79.63% 	 Test Accuracy: 75.33%
        - [5/9] 	 Time 0.28s 	 Training Accuracy: 80.89% 	 Test Accuracy: 76.67%
        - [6/9] 	 Time 0.31s 	 Training Accuracy: 82.74% 	 Test Accuracy: 79.33%
        - [7/9] 	 Time 0.61s 	 Training Accuracy: 84.22% 	 Test Accuracy: 81.33%
        - [8/9] 	 Time 0.32s 	 Training Accuracy: 84.96% 	 Test Accuracy: 82.67%
        - [9/9] 	 Time 0.42s 	 Training Accuracy: 85.11% 	 Test Accuracy: 81.33%
        - 
        - ````
        - 
        - But remember some AD backends like `ReverseDiff` is not GPU compatible.
        - For a model this size, you will notice that training time is significantly lower for
        - training on CPU than on GPU.
        - 
        - ````julia
        - train(NeuralODE; sensealg=InterpolatingAdjoint(; autojacvec=ReverseDiffVJP()), cpu=true)
        - ````
        - 
        - ````
        - [1/9] 	 Time 1.2s 	 Training Accuracy: 50.96% 	 Test Accuracy: 43.33%
        - [2/9] 	 Time 0.27s 	 Training Accuracy: 69.63% 	 Test Accuracy: 66.0%
        - [3/9] 	 Time 0.25s 	 Training Accuracy: 77.93% 	 Test Accuracy: 71.33%
        - [4/9] 	 Time 0.25s 	 Training Accuracy: 80.74% 	 Test Accuracy: 76.67%
        - [5/9] 	 Time 0.26s 	 Training Accuracy: 82.52% 	 Test Accuracy: 78.0%
        - [6/9] 	 Time 0.26s 	 Training Accuracy: 84.07% 	 Test Accuracy: 78.67%
        - [7/9] 	 Time 0.46s 	 Training Accuracy: 85.33% 	 Test Accuracy: 80.67%
        - [8/9] 	 Time 0.32s 	 Training Accuracy: 86.59% 	 Test Accuracy: 81.33%
        - [9/9] 	 Time 0.48s 	 Training Accuracy: 87.7% 	 Test Accuracy: 82.0%
        - 
        - ````
        - 
        - For completeness, let's also test out discrete sensitivities!
        - 
        - ````julia
        - train(NeuralODE; sensealg=ReverseDiffAdjoint(), cpu=true)
        - ````
        - 
        - ````
        - [1/9] 	 Time 9.44s 	 Training Accuracy: 50.96% 	 Test Accuracy: 43.33%
        - [2/9] 	 Time 8.46s 	 Training Accuracy: 69.63% 	 Test Accuracy: 66.0%
        - [3/9] 	 Time 8.6s 	 Training Accuracy: 77.93% 	 Test Accuracy: 71.33%
        - [4/9] 	 Time 11.36s 	 Training Accuracy: 80.74% 	 Test Accuracy: 76.67%
        - [5/9] 	 Time 12.5s 	 Training Accuracy: 82.52% 	 Test Accuracy: 78.0%
        - [6/9] 	 Time 13.04s 	 Training Accuracy: 84.07% 	 Test Accuracy: 78.67%
        - [7/9] 	 Time 13.38s 	 Training Accuracy: 85.33% 	 Test Accuracy: 80.67%
        - [8/9] 	 Time 13.18s 	 Training Accuracy: 86.59% 	 Test Accuracy: 81.33%
        - [9/9] 	 Time 13.4s 	 Training Accuracy: 87.7% 	 Test Accuracy: 82.0%
        - 
        - ````
        - 
        - ## Alternate Implementation using Stateful Layer
        - 
        - Starting `v0.5.5`, Lux provides a `Lux.Experimental.StatefulLuxLayer` which can be used
        - to avoid the [`Box`ing of `st`](https://github.com/JuliaLang/julia/issues/15276).
        - 
        - ````julia
        - struct StatefulNeuralODE{M <: Lux.AbstractExplicitLayer, So, Se, T, K} <:
        -        Lux.AbstractExplicitContainerLayer{(:model,)}
        -     model::M
        -     solver::So
        -     sensealg::Se
        -     tspan::T
        -     kwargs::K
        - end
        - 
        - function StatefulNeuralODE(
        -         model::Lux.AbstractExplicitLayer; solver=Tsit5(), tspan=(0.0f0, 1.0f0),
        -         sensealg=InterpolatingAdjoint(; autojacvec=ZygoteVJP()), kwargs...)
        -     return StatefulNeuralODE(model, solver, sensealg, tspan, kwargs)
        - end
        - 
        - function (n::StatefulNeuralODE)(x, ps, st)
        -     st_model = Lux.StatefulLuxLayer(n.model, ps, st)
        -     dudt(u, p, t) = st_model(u, p)
        -     prob = ODEProblem{false}(ODEFunction{false}(dudt), x, n.tspan, ps)
        -     return solve(prob, n.solver; sensealg=n.sensealg, n.kwargs...), st_model.st
        - end
        - ````
        - 
        - ## Train the new Stateful Neural ODE
        - 
        - ````julia
        - train(StatefulNeuralODE)
        - ````
        - 
        - ````
        - [1/9] 	 Time 2.08s 	 Training Accuracy: 49.85% 	 Test Accuracy: 40.67%
        - [2/9] 	 Time 0.27s 	 Training Accuracy: 70.3% 	 Test Accuracy: 66.67%
        - [3/9] 	 Time 0.3s 	 Training Accuracy: 78.07% 	 Test Accuracy: 71.33%
        - [4/9] 	 Time 0.32s 	 Training Accuracy: 80.74% 	 Test Accuracy: 76.0%
        - [5/9] 	 Time 0.34s 	 Training Accuracy: 82.0% 	 Test Accuracy: 78.0%
        - [6/9] 	 Time 0.3s 	 Training Accuracy: 84.44% 	 Test Accuracy: 79.33%
        - [7/9] 	 Time 0.3s 	 Training Accuracy: 85.7% 	 Test Accuracy: 82.0%
        - [8/9] 	 Time 0.31s 	 Training Accuracy: 87.04% 	 Test Accuracy: 80.67%
        - [9/9] 	 Time 0.3s 	 Training Accuracy: 88.0% 	 Test Accuracy: 82.67%
        - 
        - ````
        - 
        - We might not see a significant difference in the training time, but let us investigate
        - the type stabilities of the layers.
        - 
        - ## Type Stability
        - 
        - ````julia
        - model, ps, st = create_model(NeuralODE)
        - 
        - model_stateful, ps_stateful, st_stateful = create_model(StatefulNeuralODE)
        - 
        - x = gpu_device()(ones(Float32, 28, 28, 1, 3));
        - ````
        - 
        - NeuralODE is not type stable due to the boxing of `st`
        - 
        - ````julia
        - @code_warntype model(x, ps, st)
        - ````
        - 
        - ````
        - MethodInstance for (::Lux.Chain{@NamedTuple{layer_1::Lux.FlattenLayer{Nothing}, layer_2::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_3::Main.var"##225".NeuralODE{Lux.Chain{@NamedTuple{layer_1::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_2::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_3::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}}, Nothing}, OrdinaryDiffEq.Tsit5{typeof(OrdinaryDiffEq.trivial_limiter!), typeof(OrdinaryDiffEq.trivial_limiter!), Static.False}, SciMLSensitivity.InterpolatingAdjoint{0, true, Val{:central}, SciMLSensitivity.ZygoteVJP}, Tuple{Float32, Float32}, Base.Pairs{Symbol, Real, NTuple{4, Symbol}, @NamedTuple{save_everystep::Bool, reltol::Float32, abstol::Float32, save_star
        -   from (c::Lux.Chain)(x, ps, st::NamedTuple) @ Lux /var/lib/buildkite-agent/builds/gpuci-14/julialang/lux-dot-jl/src/layers/containers.jl:479
        - Arguments
        -   c::Lux.Chain{@NamedTuple{layer_1::Lux.FlattenLayer{Nothing}, layer_2::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_3::Main.var"##225".NeuralODE{Lux.Chain{@NamedTuple{layer_1::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_2::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_3::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}}, Nothing}, OrdinaryDiffEq.Tsit5{typeof(OrdinaryDiffEq.trivial_limiter!), typeof(OrdinaryDiffEq.trivial_limiter!), Static.False}, SciMLSensitivity.InterpolatingAdjoint{0, true, Val{:central}, SciMLSensitivity.ZygoteVJP}, Tuple{Float32, Float32}, Base.Pairs{Symbol, Real, NTuple{4, Symbol}, @NamedTuple{save_everystep::Bool, reltol::Float32, abstol::Float32, save_start::Bool}}}, layer
        -   x::CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}
        -   ps::ComponentArrays.ComponentVector{Float32, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Tuple{ComponentArrays.Axis{(layer_1 = 1:0, layer_2 = ViewAxis(1:15700, Axis(weight = ViewAxis(1:15680, ShapedAxis((20, 784))), bias = ViewAxis(15681:15700, ShapedAxis((20, 1))))), layer_3 = ViewAxis(15701:16240, Axis(layer_1 = ViewAxis(1:210, Axis(weight = ViewAxis(1:200, ShapedAxis((10, 20))), bias = ViewAxis(201:210, ShapedAxis((10, 1))))), layer_2 = ViewAxis(211:320, Axis(weight = ViewAxis(1:100, ShapedAxis((10, 10))), bias = ViewAxis(101:110, ShapedAxis((10, 1))))), layer_3 = ViewAxis(321:540, Axis(weight = ViewAxis(1:200, ShapedAxis((20, 10))), bias = ViewAxis(201:220, ShapedAxis((20, 1))))))), layer_4 = 16241:16240, layer_5 = ViewAxis(16241:16450, Axis(weight = ViewAxis(1:200, ShapedAxis((10, 20))), bias = ViewAxis(201:210, ShapedAxis((10, 1))))))}}}
        -   st::Core.Const((layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple()), layer_4 = NamedTuple(), layer_5 = NamedTuple()))
        - Body::TUPLE{CUDA.CUARRAY{FLOAT32, 2, CUDA.MEM.DEVICEBUFFER}, NAMEDTUPLE{(:LAYER_1, :LAYER_2, :LAYER_3, :LAYER_4, :LAYER_5), <:TUPLE{@NAMEDTUPLE{}, @NAMEDTUPLE{}, ANY, @NAMEDTUPLE{}, @NAMEDTUPLE{}}}}
        - 1 ─ %1 = Base.getproperty(c, :layers)::@NamedTuple{layer_1::Lux.FlattenLayer{Nothing}, layer_2::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_3::Main.var"##225".NeuralODE{Lux.Chain{@NamedTuple{layer_1::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_2::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_3::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}}, Nothing}, OrdinaryDiffEq.Tsit5{typeof(OrdinaryDiffEq.trivial_limiter!), typeof(OrdinaryDiffEq.trivial_limiter!), Static.False}, SciMLSensitivity.InterpolatingAdjoint{0, true, Val{:central}, SciMLSensitivity.ZygoteVJP}, Tuple{Float32, Float32}, Base.Pairs{Symbol, Real, NTuple{4, Symbol}, @NamedTuple{save_everystep::Bool, reltol::Float32, abstol::Float32, 
        - │   %2 = Lux.applychain(%1, x, ps, st)::TUPLE{CUDA.CUARRAY{FLOAT32, 2, CUDA.MEM.DEVICEBUFFER}, NAMEDTUPLE{(:LAYER_1, :LAYER_2, :LAYER_3, :LAYER_4, :LAYER_5), <:TUPLE{@NAMEDTUPLE{}, @NAMEDTUPLE{}, ANY, @NAMEDTUPLE{}, @NAMEDTUPLE{}}}}
        - └──      return %2
        - 
        - 
        - ````
        - 
        - We avoid the problem entirely by using `StatefulNeuralODE`
        - 
        - ````julia
        - @code_warntype model_stateful(x, ps_stateful, st_stateful)
        - ````
        - 
        - ````
        - MethodInstance for (::Lux.Chain{@NamedTuple{layer_1::Lux.FlattenLayer{Nothing}, layer_2::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_3::Main.var"##225".StatefulNeuralODE{Lux.Chain{@NamedTuple{layer_1::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_2::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_3::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}}, Nothing}, OrdinaryDiffEq.Tsit5{typeof(OrdinaryDiffEq.trivial_limiter!), typeof(OrdinaryDiffEq.trivial_limiter!), Static.False}, SciMLSensitivity.InterpolatingAdjoint{0, true, Val{:central}, SciMLSensitivity.ZygoteVJP}, Tuple{Float32, Float32}, Base.Pairs{Symbol, Real, NTuple{4, Symbol}, @NamedTuple{save_everystep::Bool, reltol::Float32, abstol::Float32, s
        -   from (c::Lux.Chain)(x, ps, st::NamedTuple) @ Lux /var/lib/buildkite-agent/builds/gpuci-14/julialang/lux-dot-jl/src/layers/containers.jl:479
        - Arguments
        -   c::Lux.Chain{@NamedTuple{layer_1::Lux.FlattenLayer{Nothing}, layer_2::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_3::Main.var"##225".StatefulNeuralODE{Lux.Chain{@NamedTuple{layer_1::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_2::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_3::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}}, Nothing}, OrdinaryDiffEq.Tsit5{typeof(OrdinaryDiffEq.trivial_limiter!), typeof(OrdinaryDiffEq.trivial_limiter!), Static.False}, SciMLSensitivity.InterpolatingAdjoint{0, true, Val{:central}, SciMLSensitivity.ZygoteVJP}, Tuple{Float32, Float32}, Base.Pairs{Symbol, Real, NTuple{4, Symbol}, @NamedTuple{save_everystep::Bool, reltol::Float32, abstol::Float32, save_start::Bool}}
        -   x::CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}
        -   ps::ComponentArrays.ComponentVector{Float32, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Tuple{ComponentArrays.Axis{(layer_1 = 1:0, layer_2 = ViewAxis(1:15700, Axis(weight = ViewAxis(1:15680, ShapedAxis((20, 784))), bias = ViewAxis(15681:15700, ShapedAxis((20, 1))))), layer_3 = ViewAxis(15701:16240, Axis(layer_1 = ViewAxis(1:210, Axis(weight = ViewAxis(1:200, ShapedAxis((10, 20))), bias = ViewAxis(201:210, ShapedAxis((10, 1))))), layer_2 = ViewAxis(211:320, Axis(weight = ViewAxis(1:100, ShapedAxis((10, 10))), bias = ViewAxis(101:110, ShapedAxis((10, 1))))), layer_3 = ViewAxis(321:540, Axis(weight = ViewAxis(1:200, ShapedAxis((20, 10))), bias = ViewAxis(201:220, ShapedAxis((20, 1))))))), layer_4 = 16241:16240, layer_5 = ViewAxis(16241:16450, Axis(weight = ViewAxis(1:200, ShapedAxis((10, 20))), bias = ViewAxis(201:210, ShapedAxis((10, 1))))))}}}
        -   st::Core.Const((layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple()), layer_4 = NamedTuple(), layer_5 = NamedTuple()))
        - Body::Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}, layer_4::@NamedTuple{}, layer_5::@NamedTuple{}}}
        - 1 ─ %1 = Base.getproperty(c, :layers)::@NamedTuple{layer_1::Lux.FlattenLayer{Nothing}, layer_2::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_3::Main.var"##225".StatefulNeuralODE{Lux.Chain{@NamedTuple{layer_1::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_2::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}, layer_3::Lux.Dense{true, typeof(NNlib.tanh_fast), typeof(WeightInitializers.glorot_uniform), typeof(WeightInitializers.zeros32)}}, Nothing}, OrdinaryDiffEq.Tsit5{typeof(OrdinaryDiffEq.trivial_limiter!), typeof(OrdinaryDiffEq.trivial_limiter!), Static.False}, SciMLSensitivity.InterpolatingAdjoint{0, true, Val{:central}, SciMLSensitivity.ZygoteVJP}, Tuple{Float32, Float32}, Base.Pairs{Symbol, Real, NTuple{4, Symbol}, @NamedTuple{save_everystep::Bool, reltol::Float32, abstol::F
        - │   %2 = Lux.applychain(%1, x, ps, st)::Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}, layer_4::@NamedTuple{}, layer_5::@NamedTuple{}}}
        - └──      return %2
        - 
        - 
        - ````
        - 
        - Note, that we still recommend using this layer internally and not exposing this as the
        - default API to the users.
        - 
        - ## Appendix
        - 
        - ````julia
        - using InteractiveUtils
        - InteractiveUtils.versioninfo()
        - if @isdefined(LuxCUDA) && CUDA.functional(); println(); CUDA.versioninfo(); end
        - if @isdefined(LuxAMDGPU) && LuxAMDGPU.functional(); println(); AMDGPU.versioninfo(); end
        - ````
        - 
        - ````
        - Julia Version 1.10.2
        - Commit bd47eca2c8a (2024-03-01 10:14 UTC)
        - Build Info:
        -   Official https://julialang.org/ release
        - Platform Info:
        -   OS: Linux (x86_64-linux-gnu)
        -   CPU: 48 × AMD EPYC 7402 24-Core Processor
        -   WORD_SIZE: 64
        -   LIBM: libopenlibm
        -   LLVM: libLLVM-15.0.7 (ORCJIT, znver2)
        - Threads: 48 default, 0 interactive, 24 GC (on 2 virtual cores)
        - Environment:
        -   LD_LIBRARY_PATH = /usr/local/nvidia/lib:/usr/local/nvidia/lib64
        -   JULIA_DEPOT_PATH = /root/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6
        -   JULIA_PROJECT = /var/lib/buildkite-agent/builds/gpuci-14/julialang/lux-dot-jl/docs/Project.toml
        -   JULIA_AMDGPU_LOGGING_ENABLED = true
        -   JULIA_DEBUG = Literate
        -   JULIA_CPU_THREADS = 2
        -   JULIA_NUM_THREADS = 48
        -   JULIA_LOAD_PATH = @:@v#.#:@stdlib
        -   JULIA_CUDA_HARD_MEMORY_LIMIT = 25%
        - 
        - CUDA runtime 12.3, artifact installation
        - CUDA driver 12.3
        - NVIDIA driver 545.23.8
        - 
        - CUDA libraries: 
        - - CUBLAS: 12.3.4
        - - CURAND: 10.3.4
        - - CUFFT: 11.0.12
        - - CUSOLVER: 11.5.4
        - - CUSPARSE: 12.2.0
        - - CUPTI: 21.0.0
        - - NVML: 12.0.0+545.23.8
        - 
        - Julia packages: 
        - - CUDA: 5.2.0
        - - CUDA_Driver_jll: 0.7.0+1
        - - CUDA_Runtime_jll: 0.11.1+0
        - 
        - Toolchain:
        - - Julia: 1.10.2
        - - LLVM: 15.0.7
        - 
        - Environment:
        - - JULIA_CUDA_HARD_MEMORY_LIMIT: 25%
        - 
        - 1 device:
        -   0: NVIDIA A100-PCIE-40GB MIG 1g.5gb (sm_80, 3.787 GiB / 4.750 GiB available)
        - ┌ Warning: LuxAMDGPU is loaded but the AMDGPU is not functional.
        - └ @ LuxAMDGPU ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/LuxAMDGPU/sGa0S/src/LuxAMDGPU.jl:19
        - 
        - ````
        - 
        - ---
        - 
        - *This page was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*
        - 
