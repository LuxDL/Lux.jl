import { _ as _export_sfc, c as createElementBlock, o as openBlock, a3 as createStaticVNode } from "./chunks/framework.BksySIuR.js";
const __pageData = JSON.parse('{"title":"Distributed Data Parallel Training","description":"","frontmatter":{},"headers":[],"relativePath":"manual/distributed_utils.md","filePath":"manual/distributed_utils.md","lastUpdated":null}');
const _sfc_main = { name: "manual/distributed_utils.md" };
const _hoisted_1 = /* @__PURE__ */ createStaticVNode('<h1 id="Distributed-Data-Parallel-Training" tabindex="-1">Distributed Data Parallel Training <a class="header-anchor" href="#Distributed-Data-Parallel-Training" aria-label="Permalink to &quot;Distributed Data Parallel Training {#Distributed-Data-Parallel-Training}&quot;">​</a></h1><div class="tip custom-block"><p class="custom-block-title">Tip</p><p>For a fully functional example, see the <a href="https://github.com/LuxDL/Lux.jl/tree/main/examples/ImageNet" target="_blank" rel="noreferrer">ImageNet Training Example</a>.</p></div><p>DDP Training using <code>Lux.DistributedUtils</code> is a spiritual successor to <a href="https://github.com/avik-pal/FluxMPI.jl" target="_blank" rel="noreferrer">FluxMPI.jl</a>, but has some key differences.</p><h2 id="Guide-to-Integrating-DistributedUtils-into-your-code" tabindex="-1">Guide to Integrating DistributedUtils into your code <a class="header-anchor" href="#Guide-to-Integrating-DistributedUtils-into-your-code" aria-label="Permalink to &quot;Guide to Integrating DistributedUtils into your code {#Guide-to-Integrating-DistributedUtils-into-your-code}&quot;">​</a></h2><ul><li>Initialize the respective backend with <a href="/v0.5.40/api/Lux/distributed_utils#Lux.DistributedUtils.initialize"><code>DistributedUtils.initialize</code></a>, by passing in a backend type. It is important that you pass in the type, i.e. <code>NCCLBackend</code> and not the object <code>NCCLBackend()</code>.</li></ul><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">DistributedUtils</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">initialize</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(NCCLBackend)</span></span></code></pre></div><ul><li>Obtain the backend via <a href="/v0.5.40/api/Lux/distributed_utils#Lux.DistributedUtils.get_distributed_backend"><code>DistributedUtils.get_distributed_backend</code></a> by passing in the type of the backend (same note as last point applies here again).</li></ul><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">backend </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DistributedUtils</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">get_distributed_backend</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(NCCLBackend)</span></span></code></pre></div><p>It is important that you use this function instead of directly constructing the backend, since there are certain internal states that need to be synchronized.</p><ul><li>Next synchronize the parameters and states of the model. This is done by calling <a href="/v0.5.40/api/Lux/distributed_utils#Lux.DistributedUtils.synchronize!!"><code>DistributedUtils.synchronize!!</code></a> with the backend and the respective input.</li></ul><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ps </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DistributedUtils</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">synchronize!!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(backend, ps)</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">st </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DistributedUtils</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">synchronize!!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(backend, st)</span></span></code></pre></div><ul><li>To split the data uniformly across the processes use <a href="/v0.5.40/api/Lux/distributed_utils#Lux.DistributedUtils.DistributedDataContainer"><code>DistributedUtils.DistributedDataContainer</code></a>. Alternatively, one can manually split the data. For the provided container to work <a href="https://github.com/JuliaML/MLUtils.jl" target="_blank" rel="noreferrer"><code>MLUtils.jl</code></a> must be installed and loaded.</li></ul><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">data </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DistributedUtils</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">DistributedDataContainer</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(backend, data)</span></span></code></pre></div><ul><li>Wrap the optimizer in <a href="/v0.5.40/api/Lux/distributed_utils#Lux.DistributedUtils.DistributedOptimizer"><code>DistributedUtils.DistributedOptimizer</code></a> to ensure that the optimizer is correctly synchronized across all processes before parameter updates. After initializing the state of the optimizer, synchronize the state across all processes.</li></ul><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">opt </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DistributedUtils</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">DistributedOptimizer</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(backend, opt)</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">opt_state </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Optimisers</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">setup</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(opt, ps)</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">opt_state </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DistributedUtils</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">synchronize!!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(backend, opt_state)</span></span></code></pre></div><ul><li>Finally change all logging and serialization code to trigger on <code>local_rank(backend) == 0</code>. This ensures that only the master process logs and serializes the model.</li></ul><h2 id="gpu-aware-mpi" tabindex="-1">GPU-Aware MPI <a class="header-anchor" href="#gpu-aware-mpi" aria-label="Permalink to &quot;GPU-Aware MPI {#gpu-aware-mpi}&quot;">​</a></h2><p>If you are using a custom MPI build that supports CUDA or ROCM, you can use the following preferences with <a href="https://github.com/JuliaPackaging/Preferences.jl" target="_blank" rel="noreferrer">Preferences.jl</a>:</p><ol><li><p><code>LuxDistributedMPICUDAAware</code> - Set this to <code>true</code> if your MPI build is CUDA aware.</p></li><li><p><code>LuxDistributedMPIROCMAware</code> - Set this to <code>true</code> if your MPI build is ROCM aware.</p></li></ol><p>By default, both of these values are set to <code>false</code>.</p><h2 id="Migration-Guide-from-FluxMPI.jl" tabindex="-1">Migration Guide from <code>FluxMPI.jl</code> <a class="header-anchor" href="#Migration-Guide-from-FluxMPI.jl" aria-label="Permalink to &quot;Migration Guide from `FluxMPI.jl` {#Migration-Guide-from-FluxMPI.jl}&quot;">​</a></h2><p>Let&#39;s compare the changes we need to make wrt the <a href="https://avik-pal.github.io/FluxMPI.jl/dev/guide/" target="_blank" rel="noreferrer">FluxMPI.jl integration guide</a>.</p><ol><li><p><code>FluxMPI.Init</code> is now <a href="/v0.5.40/api/Lux/distributed_utils#Lux.DistributedUtils.initialize"><code>DistributedUtils.initialize</code></a>.</p></li><li><p><code>FluxMPI.synchronize!(x)</code> needs to be changed to <code>x_new = DistributedUtils.synchronize!!(backend, x)</code>.</p></li><li><p><a href="/v0.5.40/api/Lux/distributed_utils#Lux.DistributedUtils.DistributedDataContainer"><code>DistributedUtils.DistributedDataContainer</code></a>, <a href="/v0.5.40/api/Lux/distributed_utils#Lux.DistributedUtils.local_rank"><code>DistributedUtils.local_rank</code></a>, and <a href="/v0.5.40/api/Lux/distributed_utils#Lux.DistributedUtils.DistributedOptimizer"><code>DistributedUtils.DistributedOptimizer</code></a> need <code>backend</code> as the first input.</p></li></ol><p>And that&#39;s pretty much it!</p><h3 id="Removed-Functionality" tabindex="-1">Removed Functionality <a class="header-anchor" href="#Removed-Functionality" aria-label="Permalink to &quot;Removed Functionality {#Removed-Functionality}&quot;">​</a></h3><ol><li><p><code>FluxMPI.allreduce_gradients</code> no longer exists. Previously this was needed when CUDA communication was flaky, with <code>NCCL.jl</code> this is no longer the case.</p></li><li><p><code>FluxMPIFluxModel</code> has been removed. <code>DistributedUtils</code> no longer works with <code>Flux</code>.</p></li></ol><h3 id="Key-Differences" tabindex="-1">Key Differences <a class="header-anchor" href="#Key-Differences" aria-label="Permalink to &quot;Key Differences {#Key-Differences}&quot;">​</a></h3><ol><li><p><code>FluxMPI.synchronize!</code> is now <code>DistributedUtils.synchronize!!</code> to highlight the fact that some of the inputs are not updated in-place.</p></li><li><p>All of the functions now require a <a href="/v0.5.40/api/Lux/distributed_utils#communication-backends">communication backend</a> as input.</p></li><li><p>We don&#39;t automatically determine if the MPI Implementation is CUDA or ROCM aware. See <a href="/v0.5.40/manual/distributed_utils#gpu-aware-mpi">GPU-aware MPI</a> for more information.</p></li><li><p>Older <a href="/v0.5.40/api/Lux/utilities#Lux.gpu"><code>Lux.gpu</code></a> implementations used to &quot;just work&quot; with <code>FluxMPI.jl</code>. We expect <a href="/v0.5.40/api/Accelerator_Support/LuxDeviceUtils#LuxDeviceUtils.gpu_device"><code>gpu_device</code></a> to continue working as expected, however, we recommend using <a href="/v0.5.40/api/Accelerator_Support/LuxDeviceUtils#LuxDeviceUtils.gpu_device"><code>gpu_device</code></a> after calling <a href="/v0.5.40/api/Lux/distributed_utils#Lux.DistributedUtils.initialize"><code>DistributedUtils.initialize</code></a> to avoid any mismatch between the device set via <code>DistributedUtils</code> and the device stores in <code>LuxCUDADevice</code> or <code>LuxAMDGPUDevice</code>.</p></li></ol><h2 id="Known-Shortcomings" tabindex="-1">Known Shortcomings <a class="header-anchor" href="#Known-Shortcomings" aria-label="Permalink to &quot;Known Shortcomings {#Known-Shortcomings}&quot;">​</a></h2><ol><li><p>Currently we don&#39;t run tests with CUDA or ROCM aware MPI, use those features at your own risk. We are working on adding tests for these features.</p></li><li><p>AMDGPU support is mostly experimental and causes deadlocks in certain situations, this is being investigated. If you have a minimal reproducer for this, please open an issue.</p></li></ol>', 30);
const _hoisted_31 = [
  _hoisted_1
];
function _sfc_render(_ctx, _cache, $props, $setup, $data, $options) {
  return openBlock(), createElementBlock("div", null, _hoisted_31);
}
const distributed_utils = /* @__PURE__ */ _export_sfc(_sfc_main, [["render", _sfc_render]]);
export {
  __pageData,
  distributed_utils as default
};
