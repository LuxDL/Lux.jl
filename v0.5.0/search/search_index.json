{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lux.jl: Explicitly Parameterized Neural Networks","text":"<p>Welcome to the documentation of Lux!</p> <p></p> <p></p>"},{"location":"#what-is-lux","title":"What is Lux?","text":"<p><code>Lux</code> is a julia deep learning framework which decouples models and parameterization using deeply nested named tuples.</p> <ul> <li>Functional Design \u2013 Pure Functions and Deterministic Function Calls.</li> <li>No more implicit parameterization.</li> <li>Compiler and AD-friendly Neural Networks</li> </ul> <p></p> <p></p>"},{"location":"#installation-guide","title":"Installation Guide","text":"<p>Install julia v1.6 or above.</p> <pre><code>using Pkg\nPkg.add(\"Lux\")\n</code></pre> <p></p> <p></p>"},{"location":"#resources-to-get-started","title":"Resources to Get Started","text":"<ul> <li>Go through the Quickstart Example.</li> <li>Read the introductory tutorials on julia and Lux</li> <li>Go through the examples sorted based on their complexity in the documentation</li> </ul> <p>Tip</p> <p>For usage related questions, please use Github Discussions or JuliaLang Discourse (machine learning domain) which allows questions and answers to be indexed. To report bugs use github issues or even better send in a pull request.</p> <p></p> <p></p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Tip</p> <p>You need to install <code>Optimisers</code> and <code>Zygote</code> if not done already. <code>Pkg.add([\"Optimisers\", \"Zygote\"])</code></p> <pre><code>using Lux, Random, Optimisers, Zygote\n# using LuxCUDA, LuxAMDGPU # Optional packages for GPU support\n</code></pre> <p>We take randomness very seriously</p> <pre><code># Seeding\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n</code></pre> <p>Build the model</p> <pre><code># Construct the layer\nmodel = Chain(BatchNorm(128), Dense(128, 256, tanh), BatchNorm(256),\nChain(Dense(256, 1, tanh), Dense(1, 10)))\n</code></pre> <p>Models don't hold parameters and states so initialize them. From there on, we just use our standard AD and Optimisers API.</p> <pre><code># Get the device determined by Lux\ndevice = gpu_device()\n\n# Parameter and State Variables\nps, st = Lux.setup(rng, model) .|&gt; device\n\n# Dummy Input\nx = rand(rng, Float32, 128, 2) |&gt; device\n\n# Run the model\ny, st = Lux.apply(model, x, ps, st)\n\n# Gradients\n## Pullback API to capture change in state\n(l, st_), pb = pullback(p -&gt; Lux.apply(model, x, p, st), ps)\ngs = pb((one.(l), nothing))[1]\n\n# Optimization\nst_opt = Optimisers.setup(Optimisers.ADAM(0.0001), ps)\nst_opt, ps = Optimisers.update(st_opt, ps, gs)\n</code></pre> <p></p> <p></p>"},{"location":"#how-the-documentation-is-structured","title":"How the documentation is structured","text":"<p>Having a high-level overview of how this documentation is structured will help you know where to look for certain things.</p> <ul> <li><code>Introduction</code> \u2013 Talks about why we wrote Lux and has pointers to frameworks in the extended julia ecosystem which might help users to get started with deep learning</li> <li><code>Tutorials</code> \u2013 Contain tutorials of varying complexity. These contain worked examples of solving problems with Lux. Start here if you are new to Lux, or you have a particular problem class you want to model.</li> <li><code>Manual</code> \u2013 Contains guides to some common problems encountered by users.</li> <li><code>API Reference</code> \u2013 Contains a complete list of the functions you can use in Lux. Look here if you want to know how to use a particular function.</li> <li><code>Development Documentation</code> \u2013 Contains information for people contributing to Lux development or writing Lux extensions. Don't worry about this section if you are using Lux to formulate and solve problems as a user.</li> </ul> <p></p> <p></p>"},{"location":"#citation","title":"Citation","text":"<p>If you found this library to be useful in academic work, then please cite:</p> <pre><code>@software{pal2023lux,\nauthor       = {Pal, Avik},\ntitle        = {{Lux: Explicit Parameterization of Deep Neural Networks in Julia}},\nmonth        = April,\nyear         = 2023,\nnote         = {If you use this software, please cite it as below.},\npublisher    = {Zenodo},\nversion      = {v0.4.50},\ndoi          = {10.5281/zenodo.7808904},\nurl          = {https://doi.org/10.5281/zenodo.7808904}\n}\n</code></pre> <p>Also consider starring our github repo</p>"},{"location":"api/contrib/","title":"Experimental","text":""},{"location":"api/contrib/#experimental-features","title":"Experimental Features","text":"<p>All features listed on this page are experimental which means:</p> <ol> <li>No SemVer Guarantees. We use code here to iterate fast and most users should wait for these features to be marked non-experimental.</li> <li>The code will probably be moved into a separate repository in the future.</li> <li>Expect edge-cases and report them. It will help us move these features out of experimental sooner.</li> <li>None of the features are exported.</li> </ol> <p></p> <p></p>"},{"location":"api/contrib/#index","title":"Index","text":"<ul> <li><code>Lux.FrozenLayer</code></li> <li><code>Lux.Training.TrainState</code></li> <li><code>Lux.Training.apply_gradients</code></li> <li><code>Lux.Training.compute_gradients</code></li> <li><code>Lux.freeze</code></li> <li><code>Lux.layer_map</code></li> <li><code>Lux.share_parameters</code></li> <li><code>Lux.unfreeze</code></li> <li><code>Lux.@layer_map</code></li> </ul>"},{"location":"api/contrib/#training","title":"Training","text":"<p>Helper Functions making it easier to train <code>Lux.jl</code> models.</p> <p>Lux.Training is meant to be simple and provide extremely basic functionality. We provide basic building blocks which can be seamlessly composed to create complex training pipelines.</p> <p># <code>Lux.Training.TrainState</code> \u2014 Type.</p> <pre><code>TrainState\n</code></pre> <p>Training State containing:</p> <ul> <li><code>model</code>: <code>Lux</code> model.</li> <li><code>parameters</code>: Trainable Variables of the <code>model</code>.</li> <li><code>states</code>: Non-trainable Variables of the <code>model</code>.</li> <li><code>optimizer_state</code>: Optimizer State.</li> <li><code>step</code>: Number of updates of the parameters made.</li> </ul> <p>source</p> <p># <code>Lux.Training.compute_gradients</code> \u2014 Function.</p> <pre><code>compute_gradients(ad::ADTypes.AbstractADType, objective_function::Function, data,\nts::TrainState)\n</code></pre> <p>Compute the gradients of the objective function wrt parameters stored in <code>ts</code>.</p> <p>Arguments</p> <ul> <li><code>ad</code>: Backend (from ADTypes.jl) used to compute the gradients.</li> <li><code>objective_function</code>: Objective function. The function must take 4 inputs \u2013 model, parameters, states and data. The function must return 3 values \u2013 loss, updated_state, and any computed statistics.</li> <li><code>data</code>: Data used to compute the gradients.</li> <li><code>ts</code>: Current Training State. See <code>TrainState</code>.</li> </ul> <p>Return</p> <p>A 4-Tuple containing:</p> <ul> <li><code>grads</code>: Computed Gradients.</li> <li><code>loss</code>: Loss from the objective function.</li> <li><code>stats</code>: Any computed statistics from the objective function.</li> <li><code>ts</code>: Updated Training State.</li> </ul> <p>source</p> <p># <code>Lux.Training.apply_gradients</code> \u2014 Function.</p> <pre><code>apply_gradients(ts::TrainState, grads)\n</code></pre> <p>Update the parameters stored in <code>ts</code> using the gradients <code>grads</code>.</p> <p>Arguments</p> <ul> <li><code>ts</code>: <code>TrainState</code> object.</li> <li><code>grads</code>: Gradients of the loss function wrt <code>ts.params</code>.</li> </ul> <p>Returns</p> <p>Updated <code>TrainState</code> object.</p> <p>source</p> <p></p> <p></p>"},{"location":"api/contrib/#parameter-freezing","title":"Parameter Freezing","text":"<p>Note</p> <p>In the long term, this will be supported via Optimisers.jl.</p> <p># <code>Lux.FrozenLayer</code> \u2014 Type.</p> <pre><code>FrozenLayer(l::AbstractExplicitLayer, which_params::Union{Tuple, Nothing})\n</code></pre> <p>Freeze the parameters with name <code>which_params</code> of the layer <code>l</code>.</p> <p>Tip</p> <p>It is always recommended to use the <code>Lux.freeze</code> function instead of directly using the <code>FrozenLayer</code> constructor.</p> <p>Warning</p> <p>There are no checks for <code>which_params</code>. For example, if the original layer has parameters named <code>(:weight, :bias)``, and</code>which_params<code>is set to</code>(:myweight,)` then none of the parameters are frozen and no error is thrown.</p> <p>Arguments</p> <ul> <li><code>l</code>: Lux AbstractExplicitLayer.</li> <li><code>which_params</code>: Parameter Names to be Frozen. Can be set to <code>nothing</code>, in which case all parameters are frozen.</li> </ul> <p>Input</p> <ul> <li><code>x</code>: Input to the layer <code>l</code>.</li> </ul> <p>Returns</p> <ul> <li>Output of the inner layer <code>l</code></li> <li>Updated State</li> </ul> <p>Parameters</p> <ul> <li>Parameters of the layer <code>l</code> excluding <code>which_params</code>.</li> </ul> <p>States</p> <ul> <li><code>frozen_params</code>: Parameters that are frozen, i.e., <code>which_params</code>.</li> <li><code>states</code>: The state of the inner layer <code>l</code>.</li> </ul> <p>Note on Internal Layer Implementation</p> <p>The inner layer should work with <code>NamedTuple</code> parameters. In order to support custom parameter types, users need to implement <code>Lux._merge(::CustomParamType, ::NamedTuple)</code>.</p> <p>Example</p> <pre><code>m = Lux.FrozenLayer(Dense(2 =&gt; 2), (:weight,))\n</code></pre> <p>See also <code>Lux.freeze</code>, <code>Lux.unfreeze</code>.</p> <p>source</p> <p># <code>Lux.freeze</code> \u2014 Function.</p> <pre><code>freeze(l::AbstractExplicitLayer, which_params::Union{Tuple, Nothing} = nothing)\n</code></pre> <p>Constructs a version of <code>l</code> with <code>which_params</code> frozen. If <code>which_params</code> is nothing, then all parameters are frozen.</p> <p>source</p> <pre><code>freeze(l::AbstractExplicitLayer, ps, st::NamedTuple,\n       which_params::Union{Tuple, Nothing} = nothing)\n</code></pre> <p>Construct a <code>Lux.FrozenLayer</code> for <code>l</code> with the current parameters and states. If <code>which_params</code> is nothing, then all parameters are frozen.</p> <p>source</p> <p># <code>Lux.unfreeze</code> \u2014 Function.</p> <pre><code>unfreeze(l::FrozenLayer)\n</code></pre> <p>Unfreezes the layer <code>l</code>.</p> <p>source</p> <pre><code>unfreeze(l::FrozenLayer, ps, st::NamedTuple)\n</code></pre> <p>Unwraps a <code>Lux.FrozenLayer</code> <code>l</code> with the current parameters and states.</p> <p>source</p> <p>For detailed usage example look at the manual page.</p> <p></p> <p></p>"},{"location":"api/contrib/#map-over-layer","title":"Map over Layer","text":"<p># <code>Lux.layer_map</code> \u2014 Function.</p> <pre><code>layer_map(f::Function, l::AbstractExplicitLayer, ps, st::NamedTuple,\nname::String=\"model\")\n</code></pre> <p>Map the function <code>f</code> over the model <code>l</code>, with the parameters <code>ps</code> and states <code>st</code>. This is different from <code>Functors.fmap</code> since it zips the layers, parameters, and states and invokes the function on all of them together.</p> <p>Call Signature for <code>f</code></p> <ul> <li>Must take 4 inputs \u2013 <code>AbstractExplicitLayer</code>, Corresponding Parameters, Corresponding States, and the name of the layer.</li> <li>Must return a tuple of 3 elements \u2013 <code>AbstractExplicitLayer</code>, new parameters and the new states.</li> </ul> <p>Tip</p> <p>We recommend using the macro <code>Lux.@layer_map</code> instead of this function. It automatically sets the <code>name</code> of the layer to be the variable name.</p> <p>Example</p> <pre><code>using Lux, Random, Setfield\n\nc = Parallel(+; chain=Chain(; dense_1=Dense(2 =&gt; 3), bn=BatchNorm(3),\ndense_2=Dense(3 =&gt; 5)),\ndense_3=Dense(5 =&gt; 1))\n\nrng = Random.default_rng()\nps, st = Lux.setup(rng, c)\n\n# Makes parameters of Dense Layers inside Chain zero\nfunction zero_dense_params(l, ps, st, name)\nif l isa Dense\nprintln(\"zeroing params of $name\")\n@set! ps.weight = zero.(ps.weight)\n@set! ps.bias = zero.(ps.bias)\nend\nreturn l, ps, st\nend\n\nLux.layer_map(zero_dense_params, c, ps, st)\n</code></pre> <p>source</p> <p># <code>Lux.@layer_map</code> \u2014 Macro.</p> <pre><code>@layer_map func layer ps st\n</code></pre> <p>See the documentation of <code>Lux.layer_map</code> for more details. This macro eliminates the need to the set the layer name, and uses the variable name as the starting point.</p> <p>Example</p> <pre><code>using Lux, Random, Setfield\n\nc = Parallel(+; chain=Chain(; dense_1=Dense(2 =&gt; 3), bn=BatchNorm(3),\ndense_2=Dense(3 =&gt; 5)),\ndense_3=Dense(5 =&gt; 1))\n\nrng = Random.default_rng()\nps, st = Lux.setup(rng, c)\n\n# Makes parameters of Dense Layers inside Chain zero\nfunction zero_dense_params(l, ps, st, name)\nif l isa Dense\nprintln(\"zeroing params of $name\")\n@set! ps.weight = zero.(ps.weight)\n@set! ps.bias = zero.(ps.bias)\nend\nreturn l, ps, st\nend\n\nLux.@layer_map zero_dense_params c ps st\n</code></pre> <p>source</p> <p></p> <p></p>"},{"location":"api/contrib/#tied-parameters","title":"Tied Parameters","text":"<p># <code>Lux.share_parameters</code> \u2014 Function.</p> <pre><code>share_parameters(ps, sharing)\nshare_parameters(ps, sharing, new_parameters)\n</code></pre> <p>Updates the parameters in <code>ps</code> with a common set of parameters <code>new_parameters</code> that are shared between each list in the nested list <code>sharing</code>. (That was kind of a mouthful, the example should make it clear).</p> <p>Arguments</p> <ul> <li><code>ps</code>: Original parameters.</li> <li><code>sharing</code>: A nested list of lists of accessors of <code>ps</code> which need to shate the parameters (See the example for details). (Each list in the list must be disjoint)</li> <li><code>new_parameters</code>: If passed the length of <code>new_parameters</code> must be equal to the length of <code>sharing</code>. For each vector in <code>sharing</code> the corresponding parameter in <code>new_parameters</code> will be used. (If not passed, the parameters corresponding to the first element of each vector in <code>sharing</code> will be used).</li> </ul> <p>Returns</p> <p>Updated Parameters having the same structure as <code>ps</code>.</p> <p>Example</p> <pre><code>model = Chain(;\nd1=Dense(2 =&gt; 4, tanh),\nd3=Chain(; l1=Dense(4 =&gt; 2), l2=Dense(2 =&gt; 4)),\nd2=Dense(4 =&gt; 2))\n\nps, st = Lux.setup(Xoshiro(0), model)\n\n# share parameters of (d1 and d3.l1) and (d3.l2 and d2)\nps = Lux.share_parameters(ps, ((\"d3.l2\", \"d1\"), (\"d2\", \"d3.l1\")))\n</code></pre> <p>source</p>"},{"location":"api/flux2lux/","title":"Flux & Lux InterOp","text":""},{"location":"api/flux2lux/#flux-models-to-lux-models","title":"Flux Models to Lux Models","text":"<p>Accessing these functions require manually loading <code>Flux</code>, i.e., <code>using Flux</code> must be present somewhere in the code for these to be used.</p> <p></p> <p></p>"},{"location":"api/flux2lux/#index","title":"Index","text":"<ul> <li><code>Lux.FluxLayer</code></li> <li><code>Lux.transform</code></li> </ul>"},{"location":"api/flux2lux/#functions","title":"Functions","text":"<p># <code>Lux.transform</code> \u2014 Function.</p> <pre><code>transform(l; preserve_ps_st::Bool=false, force_preserve::Bool=false)\n</code></pre> <p>Convert a Flux Model to Lux Model.</p> <p>Warning</p> <p><code>transform</code> always ingores the <code>active</code> field of some of the Flux layers. This is almost never going to be supported.</p> <p>Arguments</p> <ul> <li><code>l</code>: Flux l or any generic Julia function / object.</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>preserve_ps_st</code>: Set to <code>true</code> to preserve the states and parameters of the l. This attempts the best possible way to preserve the original model. But it might fail. If you need to override possible failures, set <code>force_preserve</code> to <code>true</code>.</li> <li><code>force_preserve</code>: Some of the transformations with state and parameters preservation haven't been implemented yet, in these cases, if <code>force_transform</code> is <code>false</code> a warning will be printed and a core Lux layer will be returned. Else, it will create a <code>FluxLayer</code>.</li> </ul> <p>Examples</p> <pre><code>import Flux\nusing Lux, Metalhead, Random\n\nm = ResNet(18)\nm2 = transform(m.layers)\n\nx = randn(Float32, 224, 224, 3, 1);\n\nps, st = Lux.setup(Random.default_rng(), m2);\n\nm2(x, ps, st)\n</code></pre> <p>source</p> <p></p> <p></p>"},{"location":"api/flux2lux/#layers","title":"Layers","text":"<p># <code>Lux.FluxLayer</code> \u2014 Type.</p> <pre><code>FluxLayer(layer)\n</code></pre> <p>Serves as a compatibility layer between Flux and Lux. This uses <code>Optimisers.destructure</code> API internally.</p> <p>Warning</p> <p>Lux was written to overcome the limitations of <code>destructure</code> + <code>Flux</code>. It is recommended to rewrite your l in Lux instead of using this layer.</p> <p>Warning</p> <p>Introducing this Layer in your model will lead to type instabilities, given the way <code>Optimisers.destructure</code> works.</p> <p>Arguments</p> <ul> <li><code>layer</code>: Flux layer</li> </ul> <p>Parameters</p> <ul> <li><code>p</code>: Flattened parameters of the <code>layer</code></li> </ul> <p>source</p>"},{"location":"api/layers/","title":"Layers","text":""},{"location":"api/layers/#layers","title":"Layers","text":""},{"location":"api/layers/#index","title":"Index","text":"<ul> <li><code>Lux.AdaptiveMaxPool</code></li> <li><code>Lux.AdaptiveMeanPool</code></li> <li><code>Lux.AlphaDropout</code></li> <li><code>Lux.BatchNorm</code></li> <li><code>Lux.Bilinear</code></li> <li><code>Lux.BranchLayer</code></li> <li><code>Lux.Chain</code></li> <li><code>Lux.Conv</code></li> <li><code>Lux.ConvTranspose</code></li> <li><code>Lux.CrossCor</code></li> <li><code>Lux.Dense</code></li> <li><code>Lux.Dropout</code></li> <li><code>Lux.Embedding</code></li> <li><code>Lux.FlattenLayer</code></li> <li><code>Lux.GRUCell</code></li> <li><code>Lux.GlobalMaxPool</code></li> <li><code>Lux.GlobalMeanPool</code></li> <li><code>Lux.GroupNorm</code></li> <li><code>Lux.InstanceNorm</code></li> <li><code>Lux.LSTMCell</code></li> <li><code>Lux.LayerNorm</code></li> <li><code>Lux.MaxPool</code></li> <li><code>Lux.Maxout</code></li> <li><code>Lux.MeanPool</code></li> <li><code>Lux.NoOpLayer</code></li> <li><code>Lux.PairwiseFusion</code></li> <li><code>Lux.Parallel</code></li> <li><code>Lux.RNNCell</code></li> <li><code>Lux.Recurrence</code></li> <li><code>Lux.ReshapeLayer</code></li> <li><code>Lux.Scale</code></li> <li><code>Lux.SelectDim</code></li> <li><code>Lux.SkipConnection</code></li> <li><code>Lux.StatefulRecurrentCell</code></li> <li><code>Lux.Upsample</code></li> <li><code>Lux.VariationalHiddenDropout</code></li> <li><code>Lux.WeightNorm</code></li> <li><code>Lux.WrappedFunction</code></li> <li><code>Lux.PixelShuffle</code></li> </ul>"},{"location":"api/layers/#containers","title":"Containers","text":"<p># <code>Lux.BranchLayer</code> \u2014 Type.</p> <pre><code>BranchLayer(layers...)\nBranchLayer(; name=nothing, layers...)\n</code></pre> <p>Takes an input <code>x</code> and passes it through all the <code>layers</code> and returns a tuple of the outputs.</p> <p>Arguments</p> <ul> <li> <p>Layers can be specified in two formats:</p> <ul> <li>A list of <code>N</code> Lux layers</li> <li>Specified as <code>N</code> keyword arguments.</li> </ul> </li> </ul> <p>Keyword Arguments</p> <ul> <li><code>name</code>: Name of the layer (optional)</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Will be directly passed to each of the <code>layers</code></li> </ul> <p>Returns</p> <ul> <li>Tuple: <code>(layer_1(x), layer_2(x), ..., layer_N(x))</code>  (naming changes if using the kwargs API)</li> <li>Updated state of the <code>layers</code></li> </ul> <p>Parameters</p> <ul> <li>Parameters of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>States</p> <ul> <li>States of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>Comparison with Parallel</p> <p>This is slightly different from <code>Parallel(nothing, layers...)</code></p> <ul> <li>If the input is a tuple, <code>Parallel</code> will pass each element individually to each layer.</li> <li><code>BranchLayer</code> essentially assumes 1 input comes in and is branched out into <code>N</code> outputs.</li> </ul> <p>Example</p> <p>An easy way to replicate an input to an NTuple is to do</p> <pre><code>l = BranchLayer(NoOpLayer(), NoOpLayer(), NoOpLayer())\n</code></pre> <p>source</p> <p># <code>Lux.Chain</code> \u2014 Type.</p> <pre><code>Chain(layers...; name=nothing, disable_optimizations::Bool = false)\nChain(; layers..., name=nothing, disable_optimizations::Bool = false)\n</code></pre> <p>Collects multiple layers / functions to be called in sequence on a given input.</p> <p>Arguments</p> <ul> <li> <p>Layers can be specified in two formats:</p> <ul> <li>A list of <code>N</code> Lux layers</li> <li>Specified as <code>N</code> keyword arguments.</li> </ul> </li> </ul> <p>Keyword Arguments</p> <ul> <li><code>disable_optimizations</code>: Prevents any structural optimization</li> <li><code>name</code>: Name of the layer (optional)</li> </ul> <p>Inputs</p> <p>Input <code>x</code> is passed sequentially to each layer, and must conform to the input requirements of the internal layers.</p> <p>Returns</p> <ul> <li>Output after sequentially applying all the layers to <code>x</code></li> <li>Updated model states</li> </ul> <p>Parameters</p> <ul> <li>Parameters of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>States</p> <ul> <li>States of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>Optimizations</p> <p>Performs a few optimizations to generate reasonable architectures. Can be disabled using keyword argument <code>disable_optimizations</code>.</p> <ul> <li>All sublayers are recursively optimized.</li> <li>If a function <code>f</code> is passed as a layer and it doesn't take 3 inputs, it is converted to a <code>WrappedFunction</code>(<code>f</code>) which takes only one input.</li> <li>If the layer is a Chain, it is flattened.</li> <li><code>NoOpLayer</code>s are removed.</li> <li>If there is only 1 layer (left after optimizations), then it is returned without the <code>Chain</code> wrapper.</li> <li>If there are no layers (left after optimizations), a <code>NoOpLayer</code> is returned.</li> </ul> <p>Miscellaneous Properties</p> <ul> <li>Allows indexing. We can access the <code>i</code>th layer using <code>m[i]</code>. We can also index using ranges or arrays.</li> </ul> <p>Example</p> <pre><code>c = Chain(Dense(2, 3, relu), BatchNorm(3), Dense(3, 2))\n</code></pre> <p>source</p> <p># <code>Lux.PairwiseFusion</code> \u2014 Type.</p> <pre><code>PairwiseFusion(connection, layers...; name=nothing)\nPairwiseFusion(connection; name=nothing, layers...)\n</code></pre> <pre><code>x1 \u2192 layer1 \u2192 y1 \u2198\n                  connection \u2192 layer2 \u2192 y2 \u2198\n              x2 \u2197                          connection \u2192 y3\n                                        x3 \u2197\n</code></pre> <p>Arguments</p> <ul> <li><code>connection</code>: Takes 2 inputs and combines them</li> <li> <p><code>layers</code>: <code>AbstractExplicitLayer</code>s. Layers can be specified in two formats:</p> <ul> <li>A list of <code>N</code> Lux layers</li> <li>Specified as <code>N</code> keyword arguments.</li> </ul> </li> </ul> <p>Keyword Arguments</p> <ul> <li><code>name</code>: Name of the layer (optional)</li> </ul> <p>Inputs</p> <p>Layer behaves differently based on input type:</p> <ol> <li>If the input <code>x</code> is a tuple of length <code>N + 1</code>, then the <code>layers</code> must be a tuple of length <code>N</code>. The computation is as follows</li> </ol> <pre><code>y = x[1]\nfor i in 1:N\ny = connection(x[i + 1], layers[i](y))\nend\n</code></pre> <ol> <li>Any other kind of input</li> </ol> <pre><code>y = x\nfor i in 1:N\ny = connection(x, layers[i](y))\nend\n</code></pre> <p>Returns</p> <ul> <li>See Inputs section for how the return value is computed</li> <li>Updated model state for all the contained layers</li> </ul> <p>Parameters</p> <ul> <li>Parameters of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>States</p> <ul> <li>States of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>source</p> <p># <code>Lux.Parallel</code> \u2014 Type.</p> <pre><code>Parallel(connection, layers...; name=nothing)\nParallel(connection; name=nothing, layers...)\n</code></pre> <p>Create a layer which passes an input to each path in <code>layers</code>, before reducing the output with <code>connection</code>.</p> <p>Arguments</p> <ul> <li><code>connection</code>: An <code>N</code>-argument function that is called after passing the input through each layer. If <code>connection = nothing</code>, we return a tuple <code>Parallel(nothing, f, g)(x, y) = (f(x), g(y))</code></li> <li> <p>Layers can be specified in two formats:</p> <ul> <li>A list of <code>N</code> Lux layers</li> <li>Specified as <code>N</code> keyword arguments.</li> </ul> </li> </ul> <p>Keyword Arguments</p> <ul> <li><code>name</code>: Name of the layer (optional)</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: If <code>x</code> is not a tuple, then return is computed as <code>connection([l(x) for l in layers]...)</code>. Else one is passed to each layer, thus <code>Parallel(+, f, g)(x, y) = f(x) + g(y)</code>.</li> </ul> <p>Returns</p> <ul> <li>See the Inputs section for how the output is computed</li> <li>Updated state of the <code>layers</code></li> </ul> <p>Parameters</p> <ul> <li>Parameters of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>States</p> <ul> <li>States of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>See also <code>SkipConnection</code> which is <code>Parallel</code> with one identity.</p> <p>source</p> <p># <code>Lux.SkipConnection</code> \u2014 Type.</p> <pre><code>SkipConnection(layer, connection; name=nothing)\n</code></pre> <p>Create a skip connection which consists of a layer or <code>Chain</code> of consecutive layers and a shortcut connection linking the block's input to the output through a user-supplied 2-argument callable. The first argument to the callable will be propagated through the given <code>layer</code> while the second is the unchanged, \"skipped\" input.</p> <p>The simplest \"ResNet\"-type connection is just <code>SkipConnection(layer, +)</code>.</p> <p>Arguments</p> <ul> <li><code>layer</code>: Layer or <code>Chain</code> of layers to be applied to the input</li> <li> <p><code>connection</code>:</p> <ul> <li>A 2-argument function that takes <code>layer(input)</code> and the input OR</li> <li>An AbstractExplicitLayer that takes <code>(layer(input), input)</code> as input</li> </ul> </li> </ul> <p>Keyword Arguments</p> <ul> <li><code>name</code>: Name of the layer (optional)</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Will be passed directly to <code>layer</code></li> </ul> <p>Returns</p> <ul> <li>Output of <code>connection(layer(input), input)</code></li> <li>Updated state of <code>layer</code></li> </ul> <p>Parameters</p> <ul> <li>Parameters of <code>layer</code> OR</li> <li>If <code>connection</code> is an AbstractExplicitLayer, then NamedTuple with fields <code>:layers</code> and <code>:connection</code></li> </ul> <p>States</p> <ul> <li>States of <code>layer</code> OR</li> <li>If <code>connection</code> is an AbstractExplicitLayer, then NamedTuple with fields <code>:layers</code> and <code>:connection</code></li> </ul> <p>See <code>Parallel</code> for a more general implementation.</p> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#convolutional-layers","title":"Convolutional Layers","text":"<p># <code>Lux.Conv</code> \u2014 Type.</p> <pre><code>Conv(k::NTuple{N,Integer}, (in_chs =&gt; out_chs)::Pair{&lt;:Integer,&lt;:Integer},\nactivation=identity; init_weight=glorot_uniform, init_bias=zeros32, stride=1,\npad=0, dilation=1, groups=1, use_bias=true)\n</code></pre> <p>Standard convolutional layer.</p> <p>Image data should be stored in WHCN order (width, height, channels, batch). In other words, a <code>100 x 100</code> RGB image would be a <code>100 x 100 x 3 x 1</code> array, and a batch of 50 would be a <code>100 x 100 x 3 x 50</code> array. This has <code>N = 2</code> spatial dimensions, and needs a kernel size like <code>(5, 5)</code>, a 2-tuple of integers. To take convolutions along <code>N</code> feature dimensions, this layer expects as input an array with <code>ndims(x) == N + 2</code>, where <code>size(x, N + 1) == in_chs</code> is the number of input channels, and <code>size(x, ndims(x))</code> is the number of observations in a batch.</p> <p>Note</p> <p>Frameworks like <code>Pytorch</code> perform cross-correlation in their convolution layers</p> <p>Arguments</p> <ul> <li><code>k</code>: Tuple of integers specifying the size of the convolutional kernel. Eg, for 2D      convolutions <code>length(k) == 2</code></li> <li><code>in_chs</code>: Number of input channels</li> <li><code>out_chs</code>: Number of input and output channels</li> <li><code>activation</code>: Activation Function</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>init_weight</code>: Controls the initialization of the weight parameter</li> <li><code>init_bias</code>: Controls the initialization of the bias parameter</li> <li><code>stride</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li><code>dilation</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li> <p><code>pad</code>: Specifies the number of elements added to the borders of the data array. It can        be</p> <ul> <li>a single integer for equal padding all around,</li> <li>a tuple of <code>N</code> integers, to apply the same padding at begin/end of each spatial dimension,</li> <li>a tuple of <code>2*N</code> integers, for asymmetric padding, or</li> <li>the singleton <code>SamePad()</code>, to calculate padding such that <code>size(output,d) == size(x,d) / stride</code> (possibly rounded) for each spatial dimension.</li> <li><code>groups</code>: Expected to be an <code>Int</code>. It specifies the number of groups to divide a           convolution into (set <code>groups = in_chs</code> for Depthwise Convolutions). <code>in_chs</code>           and <code>out_chs</code> must be divisible by <code>groups</code>.</li> <li><code>use_bias</code>: Trainable bias can be disabled entirely by setting this to <code>false</code>.</li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Data satisfying <code>ndims(x) == N + 2 &amp;&amp; size(x, N - 1) == in_chs</code>, i.e.      <code>size(x) = (I_N, ..., I_1, C_in, N)</code></li> </ul> <p>Returns</p> <ul> <li>Output of the convolution <code>y</code> of size <code>(O_N, ..., O_1, C_out, N)</code> where</li> </ul> \\[ O_i = floor\\left(\\frac{I_i + pad[i] + pad[(i + N) \\% length(pad)] - dilation[i] \\times (k[i] - 1)}{stride[i]} + 1\\right) \\] <ul> <li>Empty <code>NamedTuple()</code></li> </ul> <p>Parameters</p> <ul> <li><code>weight</code>: Convolution kernel</li> <li><code>bias</code>: Bias (present if <code>use_bias=true</code>)</li> </ul> <p>source</p> <p># <code>Lux.ConvTranspose</code> \u2014 Type.</p> <pre><code>ConvTranspose(k::NTuple{N,Integer}, (in_chs =&gt; out_chs)::Pair{&lt;:Integer,&lt;:Integer},\nactivation=identity; init_weight=glorot_uniform, init_bias=zeros32,\nstride=1, pad=0, dilation=1, groups=1, use_bias=true)\n</code></pre> <p>Standard convolutional transpose layer.</p> <p>Arguments</p> <ul> <li><code>k</code>: Tuple of integers specifying the size of the convolutional kernel. Eg, for 2D      convolutions <code>length(k) == 2</code></li> <li><code>in_chs</code>: Number of input channels</li> <li><code>out_chs</code>: Number of input and output channels</li> <li><code>activation</code>: Activation Function</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>init_weight</code>: Controls the initialization of the weight parameter</li> <li><code>init_bias</code>: Controls the initialization of the bias parameter</li> <li><code>stride</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li><code>dilation</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li> <p><code>pad</code>: Specifies the number of elements added to the borders of the data array. It can        be</p> <ul> <li>a single integer for equal padding all around,</li> <li>a tuple of <code>N</code> integers, to apply the same padding at begin/end of each spatial dimension,</li> <li>a tuple of <code>2*N</code> integers, for asymmetric padding, or</li> <li>the singleton <code>SamePad()</code>, to calculate padding such that <code>size(output,d) == size(x,d) * stride</code> (possibly rounded) for each spatial dimension.</li> <li><code>groups</code>: Expected to be an <code>Int</code>. It specifies the number of groups to divide a           convolution into (set <code>groups = in_chs</code> for Depthwise Convolutions). <code>in_chs</code>           and <code>out_chs</code> must be divisible by <code>groups</code>.</li> <li><code>use_bias</code>: Trainable bias can be disabled entirely by setting this to <code>false</code>.</li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Data satisfying <code>ndims(x) == N + 2 &amp;&amp; size(x, N - 1) == in_chs</code>, i.e.      <code>size(x) = (I_N, ..., I_1, C_in, N)</code></li> </ul> <p>Returns</p> <ul> <li>Output of the convolution transpose <code>y</code> of size <code>(O_N, ..., O_1, C_out, N)</code> where</li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>Parameters</p> <ul> <li><code>weight</code>: Convolution Transpose kernel</li> <li><code>bias</code>: Bias (present if <code>use_bias=true</code>)</li> </ul> <p>source</p> <p># <code>Lux.CrossCor</code> \u2014 Type.</p> <pre><code>CrossCor(k::NTuple{N,Integer}, (in_chs =&gt; out_chs)::Pair{&lt;:Integer,&lt;:Integer},\nactivation=identity; init_weight=glorot_uniform, init_bias=zeros32, stride=1,\npad=0, dilation=1, use_bias=true)\n</code></pre> <p>Cross Correlation layer.</p> <p>Image data should be stored in WHCN order (width, height, channels, batch). In other words, a <code>100 x 100</code> RGB image would be a <code>100 x 100 x 3 x 1</code> array, and a batch of 50 would be a <code>100 x 100 x 3 x 50</code> array. This has <code>N = 2</code> spatial dimensions, and needs a kernel size like <code>(5, 5)</code>, a 2-tuple of integers. To take convolutions along <code>N</code> feature dimensions, this layer expects as input an array with <code>ndims(x) == N + 2</code>, where <code>size(x, N + 1) == in_chs</code> is the number of input channels, and <code>size(x, ndims(x))</code> is the number of observations in a batch.</p> <p>Arguments</p> <ul> <li><code>k</code>: Tuple of integers specifying the size of the convolutional kernel. Eg, for 2D      convolutions <code>length(k) == 2</code></li> <li><code>in_chs</code>: Number of input channels</li> <li><code>out_chs</code>: Number of input and output channels</li> <li><code>activation</code>: Activation Function</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>init_weight</code>: Controls the initialization of the weight parameter</li> <li><code>init_bias</code>: Controls the initialization of the bias parameter</li> <li><code>stride</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li><code>dilation</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li> <p><code>pad</code>: Specifies the number of elements added to the borders of the data array. It can        be</p> <ul> <li>a single integer for equal padding all around,</li> <li>a tuple of <code>N</code> integers, to apply the same padding at begin/end of each spatial dimension,</li> <li>a tuple of <code>2*N</code> integers, for asymmetric padding, or</li> <li>the singleton <code>SamePad()</code>, to calculate padding such that <code>size(output,d) == size(x,d) / stride</code> (possibly rounded) for each spatial dimension.</li> <li><code>use_bias</code>: Trainable bias can be disabled entirely by setting this to <code>false</code>.</li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Data satisfying <code>ndims(x) == N + 2 &amp;&amp; size(x, N - 1) == in_chs</code>, i.e.      <code>size(x) = (I_N, ..., I_1, C_in, N)</code></li> </ul> <p>Returns</p> <ul> <li>Output of the convolution <code>y</code> of size <code>(O_N, ..., O_1, C_out, N)</code> where</li> </ul> \\[ O_i = floor\\left(\\frac{I_i + pad[i] + pad[(i + N) \\% length(pad)] - dilation[i] \\times (k[i] - 1)}{stride[i]} + 1\\right) \\] <ul> <li>Empty <code>NamedTuple()</code></li> </ul> <p>Parameters</p> <ul> <li><code>weight</code>: Convolution kernel</li> <li><code>bias</code>: Bias (present if <code>use_bias=true</code>)</li> </ul> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#dropout-layers","title":"Dropout Layers","text":"<p># <code>Lux.AlphaDropout</code> \u2014 Type.</p> <pre><code>AlphaDropout(p::Real)\n</code></pre> <p>AlphaDropout layer.</p> <p>Arguments</p> <ul> <li> <p><code>p</code>: Probability of Dropout</p> <ul> <li>if <code>p = 0</code> then <code>NoOpLayer</code> is returned.</li> <li>if <code>p = 1</code> then <code>WrappedLayer(Base.Fix1(broadcast, zero))</code> is returned.</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Must be an AbstractArray</li> </ul> <p>Returns</p> <ul> <li><code>x</code> with dropout mask applied if <code>training=Val(true)</code> else just <code>x</code></li> <li>State with updated <code>rng</code></li> </ul> <p>States</p> <ul> <li><code>rng</code>: Pseudo Random Number Generator</li> <li><code>training</code>: Used to check if training/inference mode</li> </ul> <p>Call <code>Lux.testmode</code> to switch to test mode.</p> <p>See also <code>Dropout</code>, <code>VariationalHiddenDropout</code></p> <p>source</p> <p># <code>Lux.Dropout</code> \u2014 Type.</p> <pre><code>Dropout(p; dims=:)\n</code></pre> <p>Dropout layer.</p> <p>Arguments</p> <ul> <li><code>p</code>: Probability of Dropout (if <code>p = 0</code> then <code>NoOpLayer</code> is returned)</li> </ul> <p>Keyword Arguments</p> <ul> <li>To apply dropout along certain dimension(s), specify the <code>dims</code> keyword. e.g. <code>Dropout(p; dims = 3)</code> will randomly zero out entire channels on WHCN input (also called 2D dropout).</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Must be an AbstractArray</li> </ul> <p>Returns</p> <ul> <li><code>x</code> with dropout mask applied if <code>training=Val(true)</code> else just <code>x</code></li> <li>State with updated <code>rng</code></li> </ul> <p>States</p> <ul> <li><code>rng</code>: Pseudo Random Number Generator</li> <li><code>training</code>: Used to check if training/inference mode</li> </ul> <p>Call <code>Lux.testmode</code> to switch to test mode.</p> <p>See also <code>AlphaDropout</code>, <code>VariationalHiddenDropout</code></p> <p>source</p> <p># <code>Lux.VariationalHiddenDropout</code> \u2014 Type.</p> <pre><code>VariationalHiddenDropout(p; dims=:)\n</code></pre> <p>VariationalHiddenDropout layer. The only difference from Dropout is that the <code>mask</code> is retained until <code>Lux.update_state(l, :update_mask, Val(true))</code> is called.</p> <p>Arguments</p> <ul> <li><code>p</code>: Probability of Dropout (if <code>p = 0</code> then <code>NoOpLayer</code> is returned)</li> </ul> <p>Keyword Arguments</p> <ul> <li>To apply dropout along certain dimension(s), specify the <code>dims</code> keyword. e.g. <code>VariationalHiddenDropout(p; dims = 3)</code> will randomly zero out entire channels on WHCN input (also called 2D dropout).</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Must be an AbstractArray</li> </ul> <p>Returns</p> <ul> <li><code>x</code> with dropout mask applied if <code>training=Val(true)</code> else just <code>x</code></li> <li>State with updated <code>rng</code></li> </ul> <p>States</p> <ul> <li><code>rng</code>: Pseudo Random Number Generator</li> <li><code>training</code>: Used to check if training/inference mode</li> <li><code>mask</code>: Dropout mask. Initilly set to nothing. After every run, contains the mask applied in that call</li> <li><code>update_mask</code>: Stores whether new mask needs to be generated in the current call</li> </ul> <p>Call <code>Lux.testmode</code> to switch to test mode.</p> <p>See also <code>AlphaDropout</code>, <code>Dropout</code></p> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#pooling-layers","title":"Pooling Layers","text":"<p># <code>Lux.AdaptiveMaxPool</code> \u2014 Type.</p> <pre><code>AdaptiveMaxPool(out::NTuple)\n</code></pre> <p>Adaptive Max Pooling layer. Calculates the necessary window size such that its output has <code>size(y)[1:N] == out</code>.</p> <p>Arguments</p> <ul> <li><code>out</code>: Size of the first <code>N</code> dimensions for the output</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Expects as input an array with <code>ndims(x) == N+2</code>, i.e. channel and batch dimensions, after the <code>N</code> feature dimensions, where <code>N = length(out)</code>.</li> </ul> <p>Returns</p> <ul> <li>Output of size <code>(out..., C, N)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>See also <code>MaxPool</code>, <code>AdaptiveMeanPool</code>.</p> <p>source</p> <p># <code>Lux.AdaptiveMeanPool</code> \u2014 Type.</p> <pre><code>AdaptiveMeanPool(out::NTuple)\n</code></pre> <p>Adaptive Mean Pooling layer. Calculates the necessary window size such that its output has <code>size(y)[1:N] == out</code>.</p> <p>Arguments</p> <ul> <li><code>out</code>: Size of the first <code>N</code> dimensions for the output</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Expects as input an array with <code>ndims(x) == N+2</code>, i.e. channel and batch dimensions, after the <code>N</code> feature dimensions, where <code>N = length(out)</code>.</li> </ul> <p>Returns</p> <ul> <li>Output of size <code>(out..., C, N)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>See also <code>MeanPool</code>, <code>AdaptiveMaxPool</code>.</p> <p>source</p> <p># <code>Lux.GlobalMaxPool</code> \u2014 Type.</p> <pre><code>GlobalMaxPool()\n</code></pre> <p>Global Mean Pooling layer. Transforms (w,h,c,b)-shaped input into (1,1,c,b)-shaped output, by performing max pooling on the complete (w,h)-shaped feature maps.</p> <p>Inputs</p> <ul> <li><code>x</code>: Data satisfying <code>ndims(x) &gt; 2</code>, i.e. <code>size(x) = (I_N, ..., I_1, C, N)</code></li> </ul> <p>Returns</p> <ul> <li>Output of the pooling <code>y</code> of size <code>(1, ..., 1, C, N)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>See also <code>MaxPool</code>, <code>AdaptiveMaxPool</code>, <code>GlobalMeanPool</code></p> <p>source</p> <p># <code>Lux.GlobalMeanPool</code> \u2014 Type.</p> <pre><code>GlobalMeanPool()\n</code></pre> <p>Global Mean Pooling layer. Transforms (w,h,c,b)-shaped input into (1,1,c,b)-shaped output, by performing mean pooling on the complete (w,h)-shaped feature maps.</p> <p>Inputs</p> <ul> <li><code>x</code>: Data satisfying <code>ndims(x) &gt; 2</code>, i.e. <code>size(x) = (I_N, ..., I_1, C, N)</code></li> </ul> <p>Returns</p> <ul> <li>Output of the pooling <code>y</code> of size <code>(1, ..., 1, C, N)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>See also <code>MeanPool</code>, <code>AdaptiveMeanPool</code>, <code>GlobalMaxPool</code></p> <p>source</p> <p># <code>Lux.MaxPool</code> \u2014 Type.</p> <pre><code>MaxPool(window::NTuple; pad=0, stride=window)\n</code></pre> <p>Max pooling layer, which replaces all pixels in a block of size <code>window</code> with the maximum value.</p> <p>Arguments</p> <ul> <li><code>window</code>: Tuple of integers specifying the size of the window. Eg, for 2D pooling           <code>length(window) == 2</code></li> </ul> <p>Keyword Arguments</p> <ul> <li><code>stride</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li> <p><code>pad</code>: Specifies the number of elements added to the borders of the data array. It can        be</p> <ul> <li>a single integer for equal padding all around,</li> <li>a tuple of <code>N</code> integers, to apply the same padding at begin/end of each spatial dimension,</li> <li>a tuple of <code>2*N</code> integers, for asymmetric padding, or</li> <li>the singleton <code>SamePad()</code>, to calculate padding such that <code>size(output,d) == size(x,d) / stride</code> (possibly rounded) for each spatial dimension.</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Data satisfying <code>ndims(x) == N + 2</code>, i.e. <code>size(x) = (I_N, ..., I_1, C, N)</code></li> </ul> <p>Returns</p> <ul> <li>Output of the pooling <code>y</code> of size <code>(O_N, ..., O_1, C, N)</code> where</li> </ul> \\[   O_i = floor\\left(\\frac{I_i + pad[i] + pad[(i + N) \\% length(pad)] - dilation[i] \\times (k[i] - 1)}{stride[i]} + 1\\right) \\] <ul> <li>Empty <code>NamedTuple()</code></li> </ul> <p>See also <code>Conv</code>, <code>MeanPool</code>, <code>GlobalMaxPool</code>, <code>AdaptiveMaxPool</code></p> <p>source</p> <p># <code>Lux.MeanPool</code> \u2014 Type.</p> <pre><code>MeanPool(window::NTuple; pad=0, stride=window)\n</code></pre> <p>Mean pooling layer, which replaces all pixels in a block of size <code>window</code> with the mean value.</p> <p>Arguments</p> <ul> <li><code>window</code>: Tuple of integers specifying the size of the window. Eg, for 2D pooling           <code>length(window) == 2</code></li> </ul> <p>Keyword Arguments</p> <ul> <li><code>stride</code>: Should each be either single integer, or a tuple with <code>N</code> integers</li> <li> <p><code>pad</code>: Specifies the number of elements added to the borders of the data array. It can        be</p> <ul> <li>a single integer for equal padding all around,</li> <li>a tuple of <code>N</code> integers, to apply the same padding at begin/end of each spatial dimension,</li> <li>a tuple of <code>2*N</code> integers, for asymmetric padding, or</li> <li>the singleton <code>SamePad()</code>, to calculate padding such that <code>size(output,d) == size(x,d) / stride</code> (possibly rounded) for each spatial dimension.</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Data satisfying <code>ndims(x) == N + 2</code>, i.e. <code>size(x) = (I_N, ..., I_1, C, N)</code></li> </ul> <p>Returns</p> <ul> <li>Output of the pooling <code>y</code> of size <code>(O_N, ..., O_1, C, N)</code> where</li> </ul> \\[   O_i = floor\\left(\\frac{I_i + pad[i] + pad[(i + N) \\% length(pad)] - dilation[i] \\times (k[i] - 1)}{stride[i]} + 1\\right) \\] <ul> <li>Empty <code>NamedTuple()</code></li> </ul> <p>See also <code>Conv</code>, <code>MaxPool</code>, <code>GlobalMeanPool</code>, <code>AdaptiveMeanPool</code></p> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#recurrent-layers","title":"Recurrent Layers","text":"<p># <code>Lux.GRUCell</code> \u2014 Type.</p> <pre><code>GRUCell((in_dims, out_dims)::Pair{&lt;:Int,&lt;:Int}; use_bias=true, train_state::Bool=false,\ninit_weight::Tuple{Function,Function,Function}=(glorot_uniform, glorot_uniform,\nglorot_uniform),\ninit_bias::Tuple{Function,Function,Function}=(zeros32, zeros32, zeros32),\ninit_state::Function=zeros32)\n</code></pre> <p>Gated Recurrent Unit (GRU) Cell</p> \\[ \\begin{align}   r &amp;= \\sigma(W_{ir} \\times x + W_{hr} \\times h_{prev} + b_{hr})\\\\   z &amp;= \\sigma(W_{iz} \\times x + W_{hz} \\times h_{prev} + b_{hz})\\\\   n &amp;= \\tanh(W_{in} \\times x + b_{in} + r \\cdot (W_{hn} \\times h_{prev} + b_{hn}))\\\\   h_{new} &amp;= (1 - z) \\cdot n + z \\cdot h_{prev} \\end{align} \\] <p>Arguments</p> <ul> <li><code>in_dims</code>: Input Dimension</li> <li><code>out_dims</code>: Output (Hidden State) Dimension</li> <li><code>use_bias</code>: Set to false to deactivate bias</li> <li><code>train_state</code>: Trainable initial hidden state can be activated by setting this to <code>true</code></li> <li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 3 functions</li> <li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 3 functions</li> <li><code>init_state</code>: Initializer for hidden state</li> </ul> <p>Inputs</p> <ul> <li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li> <li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li> <li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the          updated hidden state is returned.</li> </ul> <p>Returns</p> <ul> <li> <p>Tuple containing</p> <ul> <li>Output \\(h_{new}\\) of shape <code>(out_dims, batch_size)</code></li> <li>Tuple containing new hidden state \\(h_{new}\\)</li> <li>Updated model state</li> </ul> </li> </ul> <p>Parameters</p> <ul> <li><code>weight_i</code>: Concatenated Weights to map from input space             \\(\\\\left\\\\\\{ W_{ir}, W_{iz}, W_{in} \\\\right\\\\\\}\\).</li> <li><code>weight_h</code>: Concatenated Weights to map from hidden space             \\(\\\\left\\\\\\{ W_{hr}, W_{hz}, W_{hn} \\\\right\\\\\\}\\).</li> <li><code>bias_i</code>: Bias vector (\\(b_{in}\\); not present if <code>use_bias=false</code>).</li> <li><code>bias_h</code>: Concatenated Bias vector for the hidden space           \\(\\\\left\\\\\\{ b_{hr}, b_{hz}, b_{hn} \\\\right\\\\\\}\\) (not present if           <code>use_bias=false</code>).</li> <li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)           \\(\\\\left\\\\\\{ b_{hr}, b_{hz}, b_{hn} \\\\right\\\\\\}\\).</li> </ul> <p>States</p> <ul> <li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li> </ul> <p>source</p> <p># <code>Lux.LSTMCell</code> \u2014 Type.</p> <pre><code>LSTMCell(in_dims =&gt; out_dims; use_bias::Bool=true, train_state::Bool=false,\ntrain_memory::Bool=false,\ninit_weight=(glorot_uniform, glorot_uniform, glorot_uniform, glorot_uniform),\ninit_bias=(zeros32, zeros32, ones32, zeros32), init_state=zeros32,\ninit_memory=zeros32)\n</code></pre> <p>Long Short-Term (LSTM) Cell</p> \\[ \\begin{align}   i &amp;= \\sigma(W_{ii} \\times x + W_{hi} \\times h_{prev} + b_{i})\\\\   f &amp;= \\sigma(W_{if} \\times x + W_{hf} \\times h_{prev} + b_{f})\\\\   g &amp;= tanh(W_{ig} \\times x + W_{hg} \\times h_{prev} + b_{g})\\\\   o &amp;= \\sigma(W_{io} \\times x + W_{ho} \\times h_{prev} + b_{o})\\\\   c_{new} &amp;= f \\cdot c_{prev} + i \\cdot g\\\\   h_{new} &amp;= o \\cdot tanh(c_{new}) \\end{align} \\] <p>Arguments</p> <ul> <li><code>in_dims</code>: Input Dimension</li> <li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li> <li><code>use_bias</code>: Set to false to deactivate bias</li> <li><code>train_state</code>: Trainable initial hidden state can be activated by setting this to <code>true</code></li> <li><code>train_memory</code>: Trainable initial memory can be activated by setting this to <code>true</code></li> <li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 4 functions</li> <li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 4 functions</li> <li><code>init_state</code>: Initializer for hidden state</li> <li><code>init_memory</code>: Initializer for memory</li> </ul> <p>Inputs</p> <ul> <li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li> <li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li> <li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li> <li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li> <li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li> </ul> <p>Returns</p> <ul> <li> <p>Tuple Containing</p> <ul> <li>Output \\(h_{new}\\) of shape <code>(out_dims, batch_size)</code></li> <li>Tuple containing new hidden state \\(h_{new}\\) and new memory \\(c_{new}\\)</li> <li>Updated model state</li> </ul> </li> </ul> <p>Parameters</p> <ul> <li><code>weight_i</code>: Concatenated Weights to map from input space             \\(\\left\\{ W_{ii}, W_{if}, W_{ig}, W_{io} \\right\\}\\).</li> <li><code>weight_h</code>: Concatenated Weights to map from hidden space             \\(\\left\\{ W_{hi}, W_{hf}, W_{hg}, W_{ho} \\right\\}\\)</li> <li><code>bias</code>: Bias vector (not present if <code>use_bias=false</code>)</li> <li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li> <li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li> </ul> <p>States</p> <ul> <li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li> </ul> <p>source</p> <p># <code>Lux.RNNCell</code> \u2014 Type.</p> <pre><code>RNNCell(in_dims =&gt; out_dims, activation=tanh; bias::Bool=true,\ntrain_state::Bool=false, init_bias=zeros32, init_weight=glorot_uniform,\ninit_state=ones32)\n</code></pre> <p>An Elman RNNCell cell with <code>activation</code> (typically set to <code>tanh</code> or <code>relu</code>).</p> <p>\\(h_{new} = activation(weight_{ih} \\times x + weight_{hh} \\times h_{prev} + bias)\\)</p> <p>Arguments</p> <ul> <li><code>in_dims</code>: Input Dimension</li> <li><code>out_dims</code>: Output (Hidden State) Dimension</li> <li><code>activation</code>: Activation function</li> <li><code>bias</code>: Set to false to deactivate bias</li> <li><code>train_state</code>: Trainable initial hidden state can be activated by setting this to <code>true</code></li> <li><code>init_bias</code>: Initializer for bias</li> <li><code>init_weight</code>: Initializer for weight</li> <li><code>init_state</code>: Initializer for hidden state</li> </ul> <p>Inputs</p> <ul> <li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li> <li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li> <li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the updated hidden state is returned.</li> </ul> <p>Returns</p> <ul> <li> <p>Tuple containing</p> <ul> <li>Output \\(h_{new}\\) of shape <code>(out_dims, batch_size)</code></li> <li>Tuple containing new hidden state \\(h_{new}\\)</li> <li>Updated model state</li> </ul> </li> </ul> <p>Parameters</p> <ul> <li><code>weight_ih</code>: Maps the input to the hidden state.</li> <li><code>weight_hh</code>: Maps the hidden state to the hidden state.</li> <li><code>bias</code>: Bias vector (not present if <code>use_bias=false</code>)</li> <li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li> </ul> <p>States</p> <ul> <li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li> </ul> <p>source</p> <p># <code>Lux.Recurrence</code> \u2014 Type.</p> <pre><code>Recurrence(cell;\nordering::AbstractTimeSeriesDataBatchOrdering=BatchLastIndex(),\nreturn_sequence::Bool=false)\n</code></pre> <p>Wraps a recurrent cell (like <code>RNNCell</code>, <code>LSTMCell</code>, <code>GRUCell</code>) to automatically operate over a sequence of inputs.</p> <p>Warning</p> <p>This is completely distinct from <code>Flux.Recur</code>. It doesn't make the <code>cell</code> stateful, rather allows operating on an entire sequence of inputs at once. See <code>StatefulRecurrentCell</code> for functionality similar to <code>Flux.Recur</code>.</p> <p>Arguments</p> <ul> <li><code>cell</code>: A recurrent cell. See <code>RNNCell</code>, <code>LSTMCell</code>, <code>GRUCell</code>, for how the inputs/outputs of a recurrent cell must be structured.</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>return_sequence</code>: If <code>true</code> returns the entire sequence of outputs, else returns only the last output. Defaults to <code>false</code>.</li> <li><code>ordering</code>: The ordering of the batch and time dimensions in the input. Defaults to <code>BatchLastIndex()</code>. Alternatively can be set to <code>TimeLastIndex()</code>.</li> </ul> <p>Inputs</p> <ul> <li> <p>If <code>x</code> is a</p> <ul> <li>Tuple or Vector: Each element is fed to the <code>cell</code> sequentially.</li> <li>Array (except a Vector): It is spliced along the penultimate dimension and each slice is fed to the <code>cell</code> sequentially.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>Output of the <code>cell</code> for the entire sequence.</li> <li>Update state of the <code>cell</code>.</li> </ul> <p>Parameters</p> <ul> <li>Same as <code>cell</code>.</li> </ul> <p>States</p> <ul> <li>Same as <code>cell</code>.</li> </ul> <p>source</p> <p># <code>Lux.StatefulRecurrentCell</code> \u2014 Type.</p> <pre><code>StatefulRecurrentCell(cell)\n</code></pre> <p>Wraps a recurrent cell (like <code>RNNCell</code>, <code>LSTMCell</code>, <code>GRUCell</code>) and makes it stateful.</p> <p>Tip</p> <p>This is very similar to <code>Flux.Recur</code></p> <p>To avoid undefined behavior, once the processing of a single sequence of data is complete, update the state with <code>Lux.update_state(st, :carry, nothing)</code>.</p> <p>Arguments</p> <ul> <li><code>cell</code>: A recurrent cell. See <code>RNNCell</code>, <code>LSTMCell</code>, <code>GRUCell</code>, for how the inputs/outputs of a recurrent cell must be structured.</li> </ul> <p>Inputs</p> <ul> <li>Input to the <code>cell</code>.</li> </ul> <p>Returns</p> <ul> <li>Output of the <code>cell</code> for the entire sequence.</li> <li>Update state of the <code>cell</code> and updated <code>carry</code>.</li> </ul> <p>Parameters</p> <ul> <li>Same as <code>cell</code>.</li> </ul> <p>States</p> <ul> <li> <p>NamedTuple containing:</p> <ul> <li><code>cell</code>: Same as <code>cell</code>.</li> <li><code>carry</code>: The carry state of the <code>cell</code>.</li> </ul> </li> </ul> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#linear-layers","title":"Linear Layers","text":"<p># <code>Lux.Bilinear</code> \u2014 Type.</p> <pre><code>Bilinear((in1_dims, in2_dims) =&gt; out, activation=identity; init_weight=glorot_uniform,\ninit_bias=zeros32, use_bias::Bool=true, allow_fast_activation::Bool=true)\nBilinear(in12_dims =&gt; out, activation=identity; init_weight=glorot_uniform,\ninit_bias=zeros32, use_bias::Bool=true, allow_fast_activation::Bool=true)\n</code></pre> <p>Create a fully connected layer between two inputs and an output, and otherwise similar to <code>Dense</code>. Its output, given vectors <code>x</code> &amp; <code>y</code>, is another vector <code>z</code> with, for all <code>i in 1:out</code>:</p> <p><code>z[i] = activation(x' * W[i, :, :] * y + bias[i])</code></p> <p>If <code>x</code> and <code>y</code> are matrices, then each column of the output <code>z = B(x, y)</code> is of this form, with <code>B</code> the Bilinear layer.</p> <p>Arguments</p> <ul> <li><code>in1_dims</code>: number of input dimensions of <code>x</code></li> <li><code>in2_dims</code>: number of input dimensions of <code>y</code></li> <li><code>in12_dims</code>: If specified, then <code>in1_dims = in2_dims = in12_dims</code></li> <li><code>out</code>: number of output dimensions</li> <li><code>activation</code>: activation function</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>init_weight</code>: initializer for the weight matrix (<code>weight = init_weight(rng, out_dims, in1_dims, in2_dims)</code>)</li> <li><code>init_bias</code>: initializer for the bias vector (ignored if <code>use_bias=false</code>)</li> <li><code>use_bias</code>: Trainable bias can be disabled entirely by setting this to <code>false</code></li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> </ul> <p>Input</p> <ul> <li> <p>A 2-Tuple containing</p> <ul> <li><code>x</code> must be an AbstractArray with <code>size(x, 1) == in1_dims</code></li> <li><code>y</code> must be an AbstractArray with <code>size(y, 1) == in2_dims</code></li> <li>If the input is an AbstractArray, then <code>x = y</code></li> </ul> </li> </ul> <p>Returns</p> <ul> <li>AbstractArray with dimensions <code>(out_dims, size(x, 2))</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>Parameters</p> <ul> <li><code>weight</code>: Weight Matrix of size <code>(out_dims, in1_dims, in2_dims)</code></li> <li><code>bias</code>: Bias of size <code>(out_dims, 1)</code> (present if <code>use_bias=true</code>)</li> </ul> <p>source</p> <p># <code>Lux.Dense</code> \u2014 Type.</p> <pre><code>Dense(in_dims =&gt; out_dims, activation=identity; init_weight=glorot_uniform,\ninit_bias=zeros32, bias::Bool=true)\n</code></pre> <p>Create a traditional fully connected layer, whose forward pass is given by: <code>y = activation.(weight * x .+ bias)</code></p> <p>Arguments</p> <ul> <li><code>in_dims</code>: number of input dimensions</li> <li><code>out_dims</code>: number of output dimensions</li> <li><code>activation</code>: activation function</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>init_weight</code>: initializer for the weight matrix (<code>weight = init_weight(rng, out_dims, in_dims)</code>)</li> <li><code>init_bias</code>: initializer for the bias vector (ignored if <code>use_bias=false</code>)</li> <li><code>use_bias</code>: Trainable bias can be disabled entirely by setting this to <code>false</code></li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> </ul> <p>Input</p> <ul> <li><code>x</code> must be an AbstractArray with <code>size(x, 1) == in_dims</code></li> </ul> <p>Returns</p> <ul> <li>AbstractArray with dimensions <code>(out_dims, ...)</code> where <code>...</code> are the dimensions of <code>x</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>Parameters</p> <ul> <li><code>weight</code>: Weight Matrix of size <code>(out_dims, in_dims)</code></li> <li><code>bias</code>: Bias of size <code>(out_dims, 1)</code> (present if <code>use_bias=true</code>)</li> </ul> <p>source</p> <p># <code>Lux.Embedding</code> \u2014 Type.</p> <pre><code>Embedding(in_dims =&gt; out_dims; init_weight=randn32)\n</code></pre> <p>A lookup table that stores embeddings of dimension <code>out_dims</code> for a vocabulary of size <code>in_dims</code>.</p> <p>This layer is often used to store word embeddings and retrieve them using indices.</p> <p>Warning</p> <p>Unlike <code>Flux.Embedding</code>, this layer does not support using <code>OneHotArray</code> as an input.</p> <p>Arguments</p> <ul> <li><code>in_dims</code>: number of input dimensions</li> <li><code>out_dims</code>: number of output dimensions</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>init_weight</code>: initializer for the weight matrix (<code>weight = init_weight(rng, out_dims, in_dims)</code>)</li> </ul> <p>Input</p> <ul> <li>Integer OR</li> <li>Abstract Vector of Integers OR</li> <li>Abstract Array of Integers</li> </ul> <p>Returns</p> <ul> <li>Returns the embedding corresponding to each index in the input. For an N dimensional input, an N + 1 dimensional output is returned.</li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>source</p> <p># <code>Lux.Scale</code> \u2014 Type.</p> <pre><code>Scale(dims, activation=identity; init_weight=ones32, init_bias=zeros32, bias::Bool=true)\n</code></pre> <p>Create a Sparsely Connected Layer with a very specific structure (only Diagonal Elements are non-zero). The forward pass is given by: <code>y = activation.(weight .* x .+ bias)</code></p> <p>Arguments</p> <ul> <li><code>dims</code>: size of the learnable scale and bias parameters.</li> <li><code>activation</code>: activation function</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>init_weight</code>: initializer for the weight matrix (<code>weight = init_weight(rng, out_dims, in_dims)</code>)</li> <li><code>init_bias</code>: initializer for the bias vector (ignored if <code>use_bias=false</code>)</li> <li><code>use_bias</code>: Trainable bias can be disabled entirely by setting this to <code>false</code></li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> </ul> <p>Input</p> <ul> <li><code>x</code> must be an Array of size <code>(dims..., B)</code> or <code>(dims...[0], ..., dims[k])</code> for <code>k \u2264 size(dims)</code></li> </ul> <p>Returns</p> <ul> <li>Array of size <code>(dims..., B)</code> or <code>(dims...[0], ..., dims[k])</code> for <code>k \u2264 size(dims)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>Parameters</p> <ul> <li><code>weight</code>: Weight Array of size <code>(dims...)</code></li> <li><code>bias</code>: Bias of size <code>(dims...)</code></li> </ul> <p>Lux 0.4.3</p> <p><code>Scale</code> with multiple dimensions requires at least Lux 0.4.3.</p> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#misc-helper-layers","title":"Misc. Helper Layers","text":"<p># <code>Lux.FlattenLayer</code> \u2014 Type.</p> <pre><code>FlattenLayer()\n</code></pre> <p>Flattens the passed array into a matrix.</p> <p>Inputs</p> <ul> <li><code>x</code>: AbstractArray</li> </ul> <p>Returns</p> <ul> <li>AbstractMatrix of size <code>(:, size(x, ndims(x)))</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>source</p> <p># <code>Lux.Maxout</code> \u2014 Type.</p> <pre><code>Maxout(layers...)\nMaxout(; layers...)\nMaxout(f::Function, n_alts::Int)\n</code></pre> <p>This contains a number of internal layers, each of which receives the same input. Its output is the elementwise maximum of the the internal layers' outputs.</p> <p>Maxout over linear dense layers satisfies the univeral approximation theorem. See [1].</p> <p>See also <code>Parallel</code> to reduce with other operators.</p> <p>Arguments</p> <ul> <li> <p>Layers can be specified in three formats:</p> <ul> <li>A list of <code>N</code> Lux layers</li> <li>Specified as <code>N</code> keyword arguments.</li> <li>A no argument function <code>f</code> and an integer <code>n_alts</code> which specifies the number of layers.</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Input that is passed to each of the layers</li> </ul> <p>Returns</p> <ul> <li>Output is computed by taking elementwise <code>max</code> of the outputs of the individual layers.</li> <li>Updated state of the <code>layers</code></li> </ul> <p>Parameters</p> <ul> <li>Parameters of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>States</p> <ul> <li>States of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code> (naming changes if using the kwargs API)</li> </ul> <p>References</p> <p>[1] Goodfellow, Warde-Farley, Mirza, Courville &amp; Bengio \"Maxout Networks\" https://arxiv.org/abs/1302.4389</p> <p>source</p> <p># <code>Lux.NoOpLayer</code> \u2014 Type.</p> <pre><code>NoOpLayer()\n</code></pre> <p>As the name suggests does nothing but allows pretty printing of layers. Whatever input is passed is returned.</p> <p>source</p> <p># <code>Lux.ReshapeLayer</code> \u2014 Type.</p> <pre><code>ReshapeLayer(dims)\n</code></pre> <p>Reshapes the passed array to have a size of <code>(dims..., :)</code></p> <p>Arguments</p> <ul> <li><code>dims</code>: The new dimensions of the array (excluding the last dimension).</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: AbstractArray of any shape which can be reshaped in <code>(dims..., size(x, ndims(x)))</code></li> </ul> <p>Returns</p> <ul> <li>AbstractArray of size <code>(dims..., size(x, ndims(x)))</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>source</p> <p># <code>Lux.SelectDim</code> \u2014 Type.</p> <pre><code>SelectDim(dim, i)\n</code></pre> <p>Return a view of all the data of the input <code>x</code> where the index for dimension <code>dim</code> equals <code>i</code>. Equivalent to <code>view(x,:,:,...,i,:,:,...)</code> where <code>i</code> is in position <code>d</code>.</p> <p>Arguments</p> <ul> <li><code>dim</code>: Dimension for indexing</li> <li><code>i</code>: Index for dimension <code>dim</code></li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: AbstractArray that can be indexed with <code>view(x,:,:,...,i,:,:,...)</code></li> </ul> <p>Returns</p> <ul> <li><code>view(x,:,:,...,i,:,:,...)</code> where <code>i</code> is in position <code>d</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>source</p> <p># <code>Lux.WrappedFunction</code> \u2014 Type.</p> <pre><code>WrappedFunction(f)\n</code></pre> <p>Wraps a stateless and parameter less function. Might be used when a function is added to <code>Chain</code>. For example, <code>Chain(x -&gt; relu.(x))</code> would not work and the right thing to do would be <code>Chain((x, ps, st) -&gt; (relu.(x), st))</code>. An easier thing to do would be <code>Chain(WrappedFunction(Base.Fix1(broadcast, relu)))</code></p> <p>Arguments</p> <ul> <li><code>f::Function</code>: A stateless and parameterless function</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: s.t <code>hasmethod(f, (typeof(x),))</code> is <code>true</code></li> </ul> <p>Returns</p> <ul> <li>Output of <code>f(x)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#normalization-layers","title":"Normalization Layers","text":"<p># <code>Lux.BatchNorm</code> \u2014 Type.</p> <pre><code>BatchNorm(chs::Integer, activation=identity; init_bias=zeros32, init_scale=ones32,\naffine=true, track_stats=true, epsilon=1f-5, momentum=0.1f0,\nallow_fast_activation::Bool=true)\n</code></pre> <p>Batch Normalization layer.</p> <p><code>BatchNorm</code> computes the mean and variance for each \\(D_1 \u00d7 ... \u00d7 D_{N-2} \u00d7 1 \u00d7 D_N\\) input slice and normalises the input accordingly.</p> <p>Arguments</p> <ul> <li><code>chs</code>: Size of the channel dimension in your data. Given an array with <code>N</code> dimensions, call the <code>N-1</code>th the channel dimension. For a batch of feature vectors this is just the data dimension, for <code>WHCN</code> images it's the usual channel dimension.</li> <li><code>activation</code>: After normalization, elementwise activation <code>activation</code> is applied.</li> </ul> <p>Keyword Arguments</p> <ul> <li>If <code>track_stats=true</code>, accumulates mean and variance statistics in training phase that will be used to renormalize the input in test phase.</li> <li><code>epsilon</code>: a value added to the denominator for numerical stability</li> <li><code>momentum</code>:  the value used for the <code>running_mean</code> and <code>running_var</code> computation</li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> <li> <p>If <code>affine=true</code>, it also applies  a shift and a rescale to the input through to learnable per-channel bias and scale parameters.</p> <ul> <li><code>init_bias</code>: Controls how the <code>bias</code> is initiliazed</li> <li><code>init_scale</code>: Controls how the <code>scale</code> is initiliazed</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Array where <code>size(x, N - 1) = chs</code> and <code>ndims(x) &gt; 2</code></li> </ul> <p>Returns</p> <ul> <li><code>y</code>: Normalized Array</li> <li>Update model state</li> </ul> <p>Parameters</p> <ul> <li> <p><code>affine=true</code></p> <ul> <li><code>bias</code>: Bias of shape <code>(chs,)</code></li> <li><code>scale</code>: Scale of shape <code>(chs,)</code></li> <li><code>affine=false</code> - Empty <code>NamedTuple()</code></li> </ul> </li> </ul> <p>States</p> <ul> <li> <p>Statistics if <code>track_stats=true</code></p> <ul> <li><code>running_mean</code>: Running mean of shape <code>(chs,)</code></li> <li><code>running_var</code>: Running variance of shape <code>(chs,)</code></li> <li> <p>Statistics if <code>track_stats=false</code></p> </li> <li> <p><code>running_mean</code>: nothing</p> </li> <li><code>running_var</code>: nothing</li> <li><code>training</code>: Used to check if training/inference mode</li> </ul> </li> </ul> <p>Use <code>Lux.testmode</code> during inference.</p> <p>Example</p> <pre><code>m = Chain(Dense(784 =&gt; 64), BatchNorm(64, relu), Dense(64 =&gt; 10), BatchNorm(10))\n</code></pre> <p>Warning</p> <p>Passing a batch size of 1, during training will result in NaNs.</p> <p>See also <code>BatchNorm</code>, <code>InstanceNorm</code>, <code>LayerNorm</code>, <code>WeightNorm</code></p> <p>source</p> <p># <code>Lux.GroupNorm</code> \u2014 Type.</p> <pre><code>GroupNorm(chs::Integer, groups::Integer, activation=identity; init_bias=zeros32,\ninit_scale=ones32, affine=true, epsilon=1f-5,\nallow_fast_activation::Bool=true)\n</code></pre> <p>Group Normalization layer.</p> <p>Arguments</p> <ul> <li><code>chs</code>: Size of the channel dimension in your data. Given an array with <code>N</code> dimensions, call the <code>N-1</code>th the channel dimension. For a batch of feature vectors this is just the data dimension, for <code>WHCN</code> images it's the usual channel dimension.</li> <li><code>groups</code> is the number of groups along which the statistics are computed. The number of channels must be an integer multiple of the number of groups.</li> <li><code>activation</code>: After normalization, elementwise activation <code>activation</code> is applied.</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>epsilon</code>: a value added to the denominator for numerical stability</li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> <li> <p>If <code>affine=true</code>, it also applies  a shift and a rescale to the input through to learnable per-channel bias and scale parameters.</p> <ul> <li><code>init_bias</code>: Controls how the <code>bias</code> is initiliazed</li> <li><code>init_scale</code>: Controls how the <code>scale</code> is initiliazed</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Array where <code>size(x, N - 1) = chs</code> and <code>ndims(x) &gt; 2</code></li> </ul> <p>Returns</p> <ul> <li><code>y</code>: Normalized Array</li> <li>Update model state</li> </ul> <p>Parameters</p> <ul> <li> <p><code>affine=true</code></p> <ul> <li><code>bias</code>: Bias of shape <code>(chs,)</code></li> <li><code>scale</code>: Scale of shape <code>(chs,)</code></li> <li><code>affine=false</code> - Empty <code>NamedTuple()</code></li> </ul> </li> </ul> <p>States</p> <ul> <li><code>training</code>: Used to check if training/inference mode</li> </ul> <p>Use <code>Lux.testmode</code> during inference.</p> <p>Example</p> <pre><code>m = Chain(Dense(784 =&gt; 64), GroupNorm(64, 4, relu), Dense(64 =&gt; 10), GroupNorm(10, 5))\n</code></pre> <p>Warning</p> <p>GroupNorm doesn't have CUDNN support. The GPU fallback is not very efficient.</p> <p>See also <code>GroupNorm</code>, <code>InstanceNorm</code>, <code>LayerNorm</code>, <code>WeightNorm</code></p> <p>source</p> <p># <code>Lux.InstanceNorm</code> \u2014 Type.</p> <pre><code>InstanceNorm(chs::Integer, activation=identity; init_bias=zeros32, init_scale=ones32,\naffine=true, epsilon=1f-5, allow_fast_activation::Bool=true)\n</code></pre> <p>Instance Normalization. For details see [1].</p> <p>Instance Normalization computes the mean and variance for each \\(D_1 \\times ... \\times D_{N - 2} \\times 1 \\times 1\\)` input slice and normalises the input accordingly.</p> <p>Arguments</p> <ul> <li><code>chs</code>: Size of the channel dimension in your data. Given an array with <code>N</code> dimensions, call the <code>N-1</code>th the channel dimension. For a batch of feature vectors this is just the data dimension, for <code>WHCN</code> images it's the usual channel dimension.</li> <li><code>activation</code>: After normalization, elementwise activation <code>activation</code> is applied.</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>epsilon</code>: a value added to the denominator for numerical stability</li> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> <li> <p>If <code>affine=true</code>, it also applies  a shift and a rescale to the input through to learnable per-channel bias and scale parameters.</p> <ul> <li><code>init_bias</code>: Controls how the <code>bias</code> is initiliazed</li> <li><code>init_scale</code>: Controls how the <code>scale</code> is initiliazed</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Array where <code>size(x, N - 1) = chs</code> and <code>ndims(x) &gt; 2</code></li> </ul> <p>Returns</p> <ul> <li><code>y</code>: Normalized Array</li> <li>Update model state</li> </ul> <p>Parameters</p> <ul> <li> <p><code>affine=true</code></p> <ul> <li><code>bias</code>: Bias of shape <code>(chs,)</code></li> <li><code>scale</code>: Scale of shape <code>(chs,)</code></li> <li><code>affine=false</code> - Empty <code>NamedTuple()</code></li> </ul> </li> </ul> <p>States</p> <ul> <li><code>training</code>: Used to check if training/inference mode</li> </ul> <p>Use <code>Lux.testmode</code> during inference.</p> <p>Example</p> <pre><code>m = Chain(Dense(784 =&gt; 64), InstanceNorm(64, relu), Dense(64 =&gt; 10), InstanceNorm(10, 5))\n</code></pre> <p>References</p> <p>[1] Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. \"Instance normalization: The     missing ingredient for fast stylization.\" arXiv preprint arXiv:1607.08022 (2016).</p> <p>Warning</p> <p>InstanceNorm doesn't have CUDNN support. The GPU fallback is not very efficient.</p> <p>See also <code>BatchNorm</code>, <code>GroupNorm</code>, <code>LayerNorm</code>, <code>WeightNorm</code></p> <p>source</p> <p># <code>Lux.LayerNorm</code> \u2014 Type.</p> <pre><code>LayerNorm(shape::NTuple{N, Int}, activation=identity; epsilon=1f-5, dims=Colon(),\naffine::Bool=true, init_bias=zeros32, init_scale=ones32,)\n</code></pre> <p>Computes mean and standard deviation over the whole input array, and uses these to normalize the whole array. Optionally applies an elementwise affine transformation afterwards.</p> <p>Given an input array \\(x\\), this layer computes</p> \\[ y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{Var[x] + \\epsilon}} * \\gamma + \\beta \\] <p>where \\(\\gamma\\) &amp; \\(\\beta\\) are trainable parameters if <code>affine=true</code>.</p> <p>Note</p> <p>As of v0.5.0, the doc used to say <code>affine::Bool=false</code>, but the code actually had <code>affine::Bool=true</code> as the default. Now the doc reflects the code, so please check whether your assumptions about the default (if made) were invalid.</p> <p>Arguments</p> <ul> <li><code>shape</code>: Broadcastable shape of input array excluding the batch dimension.</li> <li><code>activation</code>: After normalization, elementwise activation <code>activation</code> is applied.</li> </ul> <p>Keyword Arguments</p> <ul> <li><code>allow_fast_activation</code>: If <code>true</code>, then certain activations can be approximated with a faster version. The new activation function will be given by <code>NNlib.fast_act(activation)</code></li> <li><code>epsilon</code>: a value added to the denominator for numerical stability.</li> <li><code>dims</code>: Dimensions to normalize the array over.</li> <li> <p>If <code>affine=true</code>, it also applies  a shift and a rescale to the input through to learnable per-channel bias and scale parameters.</p> <ul> <li><code>init_bias</code>: Controls how the <code>bias</code> is initiliazed</li> <li><code>init_scale</code>: Controls how the <code>scale</code> is initiliazed</li> </ul> </li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: AbstractArray</li> </ul> <p>Returns</p> <ul> <li><code>y</code>: Normalized Array</li> <li>Empty NamedTuple()</li> </ul> <p>Parameters</p> <ul> <li><code>affine=false</code>: Empty <code>NamedTuple()</code></li> <li> <p><code>affine=true</code></p> <ul> <li><code>bias</code>: Bias of shape <code>(shape..., 1)</code></li> <li><code>scale</code>: Scale of shape <code>(shape..., 1)</code></li> </ul> </li> </ul> <p>source</p> <p># <code>Lux.WeightNorm</code> \u2014 Type.</p> <pre><code>WeightNorm(layer::AbstractExplicitLayer, which_params::NTuple{N,Symbol},\ndims::Union{Tuple,Nothing}=nothing)\n</code></pre> <p>Applies weight normalization to a parameter in the given layer.</p> <p>\\(w = g\\frac{v}{\\|v\\|}\\)</p> <p>Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This updates the parameters in <code>which_params</code> (e.g. <code>weight</code>) using two parameters: one specifying the magnitude (e.g. <code>weight_g</code>) and one specifying the direction (e.g. <code>weight_v</code>).</p> <p>Arguments</p> <ul> <li><code>layer</code> whose parameters are being reparameterized</li> <li><code>which_params</code>: parameter names for the parameters being reparameterized</li> <li>By default, a norm over the entire array is computed. Pass <code>dims</code> to modify the dimension.</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: Should be of valid type for input to <code>layer</code></li> </ul> <p>Returns</p> <ul> <li>Output from <code>layer</code></li> <li>Updated model state of <code>layer</code></li> </ul> <p>Parameters</p> <ul> <li><code>normalized</code>: Parameters of <code>layer</code> that are being normalized</li> <li><code>unnormalized</code>: Parameters of <code>layer</code> that are not being normalized</li> </ul> <p>States</p> <ul> <li>Same as that of <code>layer</code></li> </ul> <p>source</p> <p></p> <p></p>"},{"location":"api/layers/#upsampling","title":"Upsampling","text":"<p># <code>Lux.PixelShuffle</code> \u2014 Function.</p> <pre><code>PixelShuffle(r::Int)\n</code></pre> <p>Pixel shuffling layer with upscale factor <code>r</code>. Usually used for generating higher resolution images while upscaling them.</p> <p>See <code>NNlib.pixel_shuffle</code> for more details.</p> <p>PixelShuffle is not a Layer, rather it returns a <code>WrappedFunction</code> with the function set to <code>Base.Fix2(pixel_shuffle, r)</code></p> <p>Arguments</p> <ul> <li><code>r</code>: Upscale factor</li> </ul> <p>Inputs</p> <ul> <li><code>x</code>: For 4D-arrays representing N images, the operation converts input size(x) == (W, H, r^2 x C, N) to output of size (r x W, r x H, C, N). For D-dimensional data, it expects ndims(x) == D+2 with channel and batch dimensions, and divides the number of channels by r^D.</li> </ul> <p>Returns</p> <ul> <li>Output of size <code>(r x W, r x H, C, N)</code> for 4D-arrays, and <code>(r x W, r x H, ..., C, N)</code> for D-dimensional data, where <code>D = ndims(x) - 2</code></li> </ul> <p>source</p> <p># <code>Lux.Upsample</code> \u2014 Type.</p> <pre><code>Upsample(mode = :nearest; [scale, size]) Upsample(scale, mode = :nearest)\n</code></pre> <p>Upsampling Layer.</p> <p>Layer Construction</p> <p>Option 1</p> <ul> <li><code>mode</code>: Set to <code>:nearest</code>, <code>:linear</code>, <code>:bilinear</code> or <code>:trilinear</code></li> </ul> <p>Exactly one of two keywords must be specified:</p> <ul> <li>If <code>scale</code> is a number, this applies to all but the last two dimensions (channel and batch) of the input.  It may also be a tuple, to control dimensions individually.</li> <li>Alternatively, keyword <code>size</code> accepts a tuple, to directly specify the leading dimensions of the output.</li> </ul> <p>Option 2</p> <ul> <li>If <code>scale</code> is a number, this applies to all but the last two dimensions (channel and batch) of the input.  It may also be a tuple, to control dimensions individually.</li> <li><code>mode</code>: Set to <code>:nearest</code>, <code>:bilinear</code> or <code>:trilinear</code></li> </ul> <p>Currently supported upsampling <code>mode</code>s and corresponding NNlib's methods are:</p> <ul> <li><code>:nearest</code> -&gt; <code>NNlib.upsample_nearest</code></li> <li><code>:bilinear</code> -&gt; <code>NNlib.upsample_bilinear</code></li> <li><code>:trilinear</code> -&gt; <code>NNlib.upsample_trilinear</code></li> </ul> <p>Inputs</p> <ul> <li> <p><code>x</code>: For the input dimensions look into the documentation for the corresponding <code>NNlib</code> function</p> <ul> <li>As a rule of thumb, <code>:nearest</code> should work with arrays of arbitrary dimensions</li> <li><code>:bilinear</code> works with 4D Arrays</li> <li><code>:trilinear</code> works with 5D Arrays</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>Upsampled Input of size <code>size</code> or of size <code>(I_1 x scale[1], ..., I_N x scale[N], C, N)</code></li> <li>Empty <code>NamedTuple()</code></li> </ul> <p>source</p>"},{"location":"api/utilities/","title":"Utilities","text":""},{"location":"api/utilities/#utilities","title":"Utilities","text":""},{"location":"api/utilities/#index","title":"Index","text":"<ul> <li><code>Lux.cpu</code></li> <li><code>Lux.disable_stacktrace_truncation!</code></li> <li><code>Lux.foldl_init</code></li> <li><code>Lux.gpu</code></li> <li><code>Lux.istraining</code></li> <li><code>Lux.multigate</code></li> <li><code>Lux.replicate</code></li> </ul>"},{"location":"api/utilities/#device-management-data-transfer","title":"Device Management / Data Transfer","text":"<p># <code>Lux.cpu</code> \u2014 Function.</p> <pre><code>cpu(x)\n</code></pre> <p>Transfer <code>x</code> to CPU.</p> <p>Warning</p> <p>This function has been deprecated. Use <code>cpu_device</code> instead.</p> <p>source</p> <p># <code>Lux.gpu</code> \u2014 Function.</p> <pre><code>gpu(x)\n</code></pre> <p>Transfer <code>x</code> to GPU determined by the backend set using <code>Lux.gpu_backend!</code>.</p> <p>Warning</p> <p>This function has been deprecated. Use <code>gpu_device</code> instead. Using this function inside performance critical code will cause massive slowdowns due to type inference failure.</p> <p>source</p> <p>Note</p> <p>For detailed API documentation on Data Transfer check out the LuxDeviceUtils.jl</p> <p></p> <p></p>"},{"location":"api/utilities/#initialization","title":"Initialization","text":"<p>Note</p> <p>For API documentation on Initialization check out the WeightInitializers.jl</p> <p></p> <p></p>"},{"location":"api/utilities/#miscellaneous-utilities","title":"Miscellaneous Utilities","text":"<p># <code>Lux.foldl_init</code> \u2014 Function.</p> <pre><code>foldl_init(op, x)\nfoldl_init(op, x, init)\n</code></pre> <p>Exactly same as <code>foldl(op, x; init)</code> in the forward pass. But, gives gradients wrt <code>init</code> in the backward pass.</p> <p>source</p> <p># <code>Lux.istraining</code> \u2014 Function.</p> <pre><code>istraining(::Val{training})\nistraining(st::NamedTuple)\n</code></pre> <p>Returns <code>true</code> if <code>training</code> is <code>true</code> or if <code>st</code> contains a <code>training</code> field with value <code>true</code>. Else returns <code>false</code>.</p> <p>Method undefined if <code>st.training</code> is not of type <code>Val</code>.</p> <p>source</p> <p># <code>Lux.multigate</code> \u2014 Function.</p> <pre><code>multigate(x::AbstractArray, ::Val{N})\n</code></pre> <p>Split up <code>x</code> into <code>N</code> equally sized chunks (along dimension <code>1</code>).</p> <p>source</p> <p># <code>Lux.replicate</code> \u2014 Function.</p> <pre><code>replicate(rng::AbstractRNG)\nreplicate(rng::CUDA.RNG)\n</code></pre> <p>Creates a copy of the <code>rng</code> state depending on its type.</p> <p>source</p> <p></p> <p></p>"},{"location":"api/utilities/#truncated-stacktraces","title":"Truncated Stacktraces","text":"<p># <code>Lux.disable_stacktrace_truncation!</code> \u2014 Function.</p> <pre><code>disable_stacktrace_truncation!(; disable::Bool=true)\n</code></pre> <p>An easy way to update <code>TruncatedStacktraces.VERBOSE</code> without having to load it manually.</p> <p>Effectively does <code>TruncatedStacktraces.VERBOSE[] = disable</code></p> <p>source</p>"},{"location":"devdocs/layer_implementation/","title":"Layer Implementation","text":""},{"location":"devdocs/layer_implementation/#layer-implementation","title":"Layer Implementation","text":""},{"location":"devdocs/layer_implementation/#recurrent-neural-networks","title":"Recurrent Neural Networks","text":""},{"location":"devdocs/layer_implementation/#cell-implementations","title":"Cell Implementations","text":""},{"location":"devdocs/layer_implementation/#explicit-management-on-end-user-side","title":"Explicit Management on End-User Side","text":"<p>Note</p> <p>We currently use this implementation</p> <p>User is responsible for managing the memory and hidden states.</p> <p></p> <p></p>"},{"location":"devdocs/layer_implementation/#pros","title":"Pros","text":"<ol> <li>Simple Design and Implementation.</li> <li> <p>Hard for the User to mess up, i.e. there is no explicit requirement to call things like <code>Flux.reset!</code>.</p> <ul> <li>In the first call user passes the <code>input</code>.</li> <li>In the subsequent calls, the user passes a tuple containing the <code>input</code>, <code>hidden_state</code> and <code>memory</code> (if needed).</li> </ul> </li> </ol> <p></p> <p></p>"},{"location":"devdocs/layer_implementation/#cons","title":"Cons","text":"<ol> <li>Requires more explicit management from the user which might make it harder to use.</li> <li>Currently the call order convention is not enforced which could lead to sneaky errors. (Implementing a check is quite trivial if we store a call counter in the model <code>state</code>).</li> </ol>"},{"location":"devdocs/layer_implementation/#store-hidden-state-and-memory-in-model-state","title":"Store Hidden State and Memory in Model State","text":"<p>Storing the memory and hidden state in <code>st</code> would allow user to just pass <code>x</code> without varying how calls are made at different timesteps.</p> <p></p>"},{"location":"devdocs/layer_implementation/#pros_1","title":"Pros","text":"<ol> <li>Easier for the end-user.</li> </ol>"},{"location":"devdocs/layer_implementation/#cons_1","title":"Cons","text":"<ol> <li> <p><code>reset</code>ing the hidden-state and memory is slightly tricky.</p> <ol> <li>One way would be to store a <code>initial_hidden_state</code> and <code>initial_memory</code> in the state alongside the <code>hidden_state</code> and <code>memory</code>.</li> </ol> </li> </ol>"},{"location":"devdocs/style_guide/","title":"Style Guide","text":""},{"location":"devdocs/style_guide/#style-guide","title":"Style Guide","text":"<p>We strictly enforce a style guide across the repository. For the most part we rely on SciMLStyle. However, any additional guideline mentioned in this document takes precedence.</p> <p>How to auto-format your code?</p> <p>Firstly, install <code>JuliaFormatter</code> by running <code>julia -e 'using Pkg; Pkg.add(PackageSpec(name=\"JuliaFormatter\"))'</code>. Next, from the root directory of the project, simply run <code>julia -e 'using JuliaFormatter; format(\".\")'</code>.</p> <p>We do have automatic formatter, which opens PR after fixing common style issues, however, we strictly don't merge PRs without a green style check.</p> <p>Note</p> <p>If you find any existing code which doesn't adhere to these guidelines, open an issue so that we can fix that.</p> <p></p> <p></p>"},{"location":"devdocs/style_guide/#code-styling","title":"Code Styling","text":"<ul> <li>Keyword Arguments must be separated using a semicolon <code>;</code></li> <li>Functions must use <code>return</code>. Returning the last value is quite ambiguous \u2013 did the author actually want it returned?</li> <li>Format docstrings as you would format regular code. If the docstring contains LaTeX in multiple lines, use <code>math</code> block.</li> <li>No avoiding multiply symbol \u2013 so <code>2x</code> is invalid instead do it like other languages <code>2 * x</code>.</li> </ul>"},{"location":"devdocs/style_guide/#testing","title":"Testing","text":"<p>Note</p> <p>Unfortunately we haven't yet tested all the functionality in the base library using these guidelines.</p> <ul> <li>The file structure of the <code>test</code> folder should mirror that of the <code>src</code> folder. Every file in src should have a complementary file in the test folder, containing tests relevant to that file's contents.</li> <li>Add generic utilities for testing in <code>test/test_utils.jl</code> and include them in the relevant files.</li> <li>Use JET.jl to test for dynamic dispatch in the functionality you added, specifically use <code>run_JET_tests</code> from <code>test/test_utils.jl</code>.</li> <li>Always test for gradient correctness. Zygote can be notorious for incorrect gradients, so add tests using <code>test_gradient_correctness_fdm</code> for finite differencing or use any other AD framework and tally the results.</li> </ul> <p></p> <p></p>"},{"location":"devdocs/style_guide/#try-adding-to-backend-packages","title":"Try adding to backend packages","text":"<p>Lux is mostly a frontend for defining Neural Networks. As such, if an optimization needs to be applied to lets say <code>NNlib.jl</code>, it is better to open a PR there since all frameworks using <code>NNlib.jl</code> get to benefit from these fixes.</p> <p>Similarly, if a bug comes to the forefront from one of the backend packages, make sure to  open a corresponding issue there to ensure they are appropriately tracked.</p> <p></p> <p></p>"},{"location":"devdocs/style_guide/#mutability","title":"Mutability","text":"<p>This is strictly enforced, i.e. all layers/functions provided as part of the external API must be pure functions, even if they come with a performance penalty.</p> <p></p> <p></p>"},{"location":"devdocs/style_guide/#branching-generated-functions","title":"Branching \u2013 Generated Functions","text":"<p>Zygote doesn't like branches in code. Like it or not, we are stuck with it for the near future. Even if julia is able to optimize branches away, Zygote will most certainly throw away those optimizations (these can be tested via <code>Zygote.@code_ir</code>).</p> <p></p> <p></p>"},{"location":"devdocs/style_guide/#writing-efficient-non-branching-code-to-make-zygote-happy","title":"Writing efficient non-branching code to make Zygote happy","text":"<ul> <li> <p>Rely on <code>@generated</code> functions to remove most runtime branching. Certain examples:</p> <ul> <li>Layers behaving differently during training and inference \u2013 we know at compile-time whether a layer is being run in training/inference mode via <code>istraining(st)</code>.</li> <li>Composite Layers relying on a variable number of internal layers \u2013 Again we know the length of the number of internal layers at compile time. Hence we can manually unroll the loops. See <code>Parallel</code>, <code>Chain</code>, etc.</li> <li>Pass around <code>Val</code> in state. <code>Flux.jl</code> sets <code>training</code> to be <code>(:auto, true, false)</code>. Hence, which branch will be evaluated, will have to be determined at runtime time (bad). Instead if we pass <code>Val(true)</code>, we will be able to specialize functions directly based on <code>true</code>, <code>false</code>, etc. ensuring there is no runtime cost for these operations. See <code>BatchNorm</code>, <code>Dropout</code>, etc.</li> </ul> </li> </ul> <p></p> <p></p>"},{"location":"devdocs/style_guide/#deprecation","title":"Deprecation","text":"<p>Deprecations should be handled according to SemVer recommendations, i.e. there should be at least one version where we throw a deprecation warning. This ensures users know how to modify their code for upcoming releases.</p> <p>This blog details the process of deprecating functionalities in Julia packages. We follow the same process. Some additional guidelines are:</p> <ul> <li>Add tests using <code>Test.@test_deprecated</code> to ensure that deprecations are indeed working as expected.</li> <li>Add a warning to the documentation about deprecations (and how to use the new recommended functionality).</li> <li>Add <code># Deprecated Functionality (Remove in &lt;VERSION NUMBER&gt;)</code> before the tests and deprecated functionality not placed in <code>src/deprecated.jl</code> (like kwarg deprecations). This makes it easier to search and delete the functionalities before making a breaking release.</li> </ul> <p></p> <p></p>"},{"location":"devdocs/style_guide/#documentation","title":"Documentation","text":"<p>We use <code>Documenter.jl</code> + <code>mkdocs</code> for our documentation.</p> <p></p> <p></p>"},{"location":"devdocs/style_guide/#adding-tutorials","title":"Adding Tutorials","text":"<p>Add tutorials must be added to the <code>examples</code> directory. Then add an entry for the path and tutorial name in <code>docs/make.jl</code>. Finally, update the navigation <code>nav</code> in <code>docs/mkdocs.yml</code></p> <p></p> <p></p>"},{"location":"devdocs/style_guide/#documentation-for-layers","title":"Documentation for Layers","text":"<p>The first line must be indented by 4 spaces and should contain the possible ways to construct the layer. This should be followed up with a description about what the layer does. If mathematical equations are needed to explain what the layer does, go for it. Often times we fuse parameters to make computation faster, this should be reflected in the equations being used, i.e. equations and the internal code must be consistent. (See <code>LSTMCell</code>, <code>GRUCell</code> for some examples.)</p> <p>Note</p> <p>There is no need to document how the layers are being called since they must adhere to <code>layer(x, ps, st)</code>. Any deviation from that and the PR will not be accepted.</p> <p>Next, we will have certain subsections (though all of them might not be necessary for all layers).</p> <ul> <li> <p>Arguments: This section should be present unless the layer is constructed without any arguments (See <code>NoOpLayer</code>). All the arguments and their explicit constraints must be explained.</p> <ul> <li>It is recommended to separate out the Keyword Arguments in their own section.</li> <li>Inputs: This section should always be present. List out the requirements <code>x</code> needs to satisfy. (Don't write about <code>ps</code> and <code>st</code> since that is expected by default.)</li> <li>Returns: What will the layer return? We know the second element will be a state but is that updated in any form or not?</li> <li>Parameters: What are the properties of the NamedTuple returned from <code>initialparameters</code>? Omit if the layer is parameterless.</li> <li>States: What are the properties of the NamedTuple returned from <code>initialstates</code>? Omit if the layer is stateless.</li> </ul> </li> </ul>"},{"location":"examples/examples/","title":"Home","text":""},{"location":"examples/examples/#tutorials-examples-using-lux","title":"Tutorials &amp; Examples using Lux","text":""},{"location":"examples/examples/#tutorials","title":"Tutorials","text":"<ul> <li>Julia &amp; Lux for the Uninitiated</li> <li>Fitting a Simple Polynomial</li> <li>Training a Simple LSTM</li> <li>MNIST Classification using NeuralODE</li> <li>Bayesian Neural Network</li> <li>Hyper Network on MNIST and FashionMNIST</li> </ul>"},{"location":"examples/examples/#scipts","title":"Scipts","text":"<ul> <li>ImageNet Classification</li> </ul>"},{"location":"examples/examples/#packages","title":"Packages","text":"<p>See Ecosystem for more details.</p>"},{"location":"examples/generated/beginner/Basics/main/","title":"Julia & Lux for the Uninitiated","text":""},{"location":"examples/generated/beginner/Basics/main/#julia-lux-for-the-uninitiated","title":"Julia &amp; Lux for the Uninitiated","text":"<p>This is a quick intro to Lux loosely based on:</p> <ol> <li>PyTorch's tutorial.</li> <li>Flux's tutorial.</li> <li>Flax's tutorial.</li> </ol> <p>It introduces basic Julia programming, as well <code>Zygote</code>, a source-to-source automatic differentiation (AD) framework in Julia. We'll use these tools to build a very simple neural network. Let's start with importing <code>Lux.jl</code></p> <pre><code>using Lux, Random\n</code></pre> <pre><code>  Activating project at `/var/lib/buildkite-agent/builds/gpuci-17/julialang/lux-dot-jl/examples`\n</code></pre> <p>Now let us control the randomness in our code using proper Pseudo Random Number Generator (PRNG)</p> <pre><code>rng = Random.default_rng()\nRandom.seed!(rng, 0)\n</code></pre> <pre><code>Random.TaskLocalRNG()\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#arrays","title":"Arrays","text":"<p>The starting point for all of our models is the <code>Array</code> (sometimes referred to as a <code>Tensor</code> in other frameworks). This is really just a list of numbers, which might be arranged into a shape like a square. Let's write down an array with three elements.</p> <pre><code>x = [1, 2, 3]\n</code></pre> <pre><code>3-element Vector{Int64}:\n 1\n 2\n 3\n</code></pre> <p>Here's a matrix \u2013 a square array with four elements.</p> <pre><code>x = [1 2; 3 4]\n</code></pre> <pre><code>2\u00d72 Matrix{Int64}:\n 1  2\n 3  4\n</code></pre> <p>We often work with arrays of thousands of elements, and don't usually write them down by hand. Here's how we can create an array of 5\u00d73 = 15 elements, each a random number from zero to one.</p> <pre><code>x = rand(rng, 5, 3)\n</code></pre> <pre><code>5\u00d73 Matrix{Float64}:\n 0.455238   0.746943   0.193291\n 0.547642   0.746801   0.116989\n 0.773354   0.97667    0.899766\n 0.940585   0.0869468  0.422918\n 0.0296477  0.351491   0.707534\n</code></pre> <p>There's a few functions like this; try replacing <code>rand</code> with <code>ones</code>, <code>zeros</code>, or <code>randn</code>.</p> <p>By default, Julia works stores numbers is a high-precision format called <code>Float64</code>. In ML we often don't need all those digits, and can ask Julia to work with <code>Float32</code> instead. We can even ask for more digits using <code>BigFloat</code>.</p> <pre><code>x = rand(BigFloat, 5, 3)\n</code></pre> <pre><code>5\u00d73 Matrix{BigFloat}:\n 0.981339    0.793159  0.459019\n 0.043883    0.624384  0.56055\n 0.164786    0.524008  0.0355555\n 0.414769    0.577181  0.621958\n 0.00823197  0.30215   0.655881\n</code></pre> <pre><code>x = rand(Float32, 5, 3)\n</code></pre> <pre><code>5\u00d73 Matrix{Float32}:\n 0.567794   0.369178   0.342539\n 0.0985227  0.201145   0.587206\n 0.776598   0.148248   0.0851708\n 0.723731   0.0770206  0.839303\n 0.404728   0.230954   0.679087\n</code></pre> <p>We can ask the array how many elements it has.</p> <pre><code>length(x)\n</code></pre> <pre><code>15\n</code></pre> <p>Or, more specifically, what size it has.</p> <pre><code>size(x)\n</code></pre> <pre><code>(5, 3)\n</code></pre> <p>We sometimes want to see some elements of the array on their own.</p> <pre><code>x\n</code></pre> <pre><code>5\u00d73 Matrix{Float32}:\n 0.567794   0.369178   0.342539\n 0.0985227  0.201145   0.587206\n 0.776598   0.148248   0.0851708\n 0.723731   0.0770206  0.839303\n 0.404728   0.230954   0.679087\n</code></pre> <pre><code>x[2, 3]\n</code></pre> <pre><code>0.58720636f0\n</code></pre> <p>This means get the second row and the third column. We can also get every row of the third column.</p> <pre><code>x[:, 3]\n</code></pre> <pre><code>5-element Vector{Float32}:\n 0.34253937\n 0.58720636\n 0.085170805\n 0.8393034\n 0.67908657\n</code></pre> <p>We can add arrays, and subtract them, which adds or subtracts each element of the array.</p> <pre><code>x + x\n</code></pre> <pre><code>5\u00d73 Matrix{Float32}:\n 1.13559   0.738356  0.685079\n 0.197045  0.40229   1.17441\n 1.5532    0.296496  0.170342\n 1.44746   0.154041  1.67861\n 0.809456  0.461908  1.35817\n</code></pre> <pre><code>x - x\n</code></pre> <pre><code>5\u00d73 Matrix{Float32}:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n</code></pre> <p>Julia supports a feature called broadcasting, using the <code>.</code> syntax. This tiles small arrays (or single numbers) to fill bigger ones.</p> <pre><code>x .+ 1\n</code></pre> <pre><code>5\u00d73 Matrix{Float32}:\n 1.56779  1.36918  1.34254\n 1.09852  1.20114  1.58721\n 1.7766   1.14825  1.08517\n 1.72373  1.07702  1.8393\n 1.40473  1.23095  1.67909\n</code></pre> <p>We can see Julia tile the column vector <code>1:5</code> across all rows of the larger array.</p> <pre><code>zeros(5, 5) .+ (1:5)\n</code></pre> <pre><code>5\u00d75 Matrix{Float64}:\n 1.0  1.0  1.0  1.0  1.0\n 2.0  2.0  2.0  2.0  2.0\n 3.0  3.0  3.0  3.0  3.0\n 4.0  4.0  4.0  4.0  4.0\n 5.0  5.0  5.0  5.0  5.0\n</code></pre> <p>The x' syntax is used to transpose a column <code>1:5</code> into an equivalent row, and Julia will tile that across columns.</p> <pre><code>zeros(5, 5) .+ (1:5)'\n</code></pre> <pre><code>5\u00d75 Matrix{Float64}:\n 1.0  2.0  3.0  4.0  5.0\n 1.0  2.0  3.0  4.0  5.0\n 1.0  2.0  3.0  4.0  5.0\n 1.0  2.0  3.0  4.0  5.0\n 1.0  2.0  3.0  4.0  5.0\n</code></pre> <p>We can use this to make a times table.</p> <pre><code>(1:5) .* (1:5)'\n</code></pre> <pre><code>5\u00d75 Matrix{Int64}:\n 1   2   3   4   5\n 2   4   6   8  10\n 3   6   9  12  15\n 4   8  12  16  20\n 5  10  15  20  25\n</code></pre> <p>Finally, and importantly for machine learning, we can conveniently do things like matrix multiply.</p> <pre><code>W = randn(5, 10)\nx = rand(10)\nW * x\n</code></pre> <pre><code>5-element Vector{Float64}:\n  1.2197981041108443\n -2.62625877100596\n -2.8573820474674845\n -2.4319346874291314\n  1.0108668577150213\n</code></pre> <p>Julia's arrays are very powerful, and you can learn more about what they can do here.</p> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#cuda-arrays","title":"CUDA Arrays","text":"<p>CUDA functionality is provided separately by the CUDA.jl package. If you have a GPU and LuxCUDA is installed, Lux will provide CUDA capabilities. For additional details on backends see the manual section.</p> <p>You can manually add <code>CUDA</code>. Once CUDA is loaded you can move any array to the GPU with the <code>cu</code> function (or the <code>gpu</code> function exported by `Lux``), and it supports all of the above operations with the same syntax.</p> <pre><code># using CUDA\n# x = cu(rand(5, 3))\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#immutability","title":"(Im)mutability","text":"<p>Lux as you might have read is Immutable by convention which means that the core library is built without any form of mutation and all functions are pure. However, we don't enforce it in any form. We do strongly recommend that users extending this framework for their respective applications don't mutate their arrays.</p> <pre><code>x = reshape(1:8, 2, 4)\n</code></pre> <pre><code>2\u00d74 reshape(::UnitRange{Int64}, 2, 4) with eltype Int64:\n 1  3  5  7\n 2  4  6  8\n</code></pre> <p>To update this array, we should first copy the array.</p> <pre><code>x_copy = copy(x)\nview(x_copy, :, 1) .= 0\n\nprintln(\"Original Array \", x)\nprintln(\"Mutated Array \", x_copy)\n</code></pre> <pre><code>Original Array [1 3 5 7; 2 4 6 8]\nMutated Array [0 3 5 7; 0 4 6 8]\n</code></pre> <p>Note that our current default AD engine (Zygote) is unable to differentiate through this mutation, however, for these specialized cases it is quite trivial to write custom backward passes. (This problem will be fixed once we move towards Enzyme.jl)</p> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#managing-randomness","title":"Managing Randomness","text":"<p>We rely on the Julia StdLib <code>Random</code> for managing the randomness in our execution. First, we create an PRNG (pseudorandom number generator) and seed it.</p> <pre><code>rng = Random.default_rng() # Creates a Xoshiro PRNG\nRandom.seed!(rng, 0)\n</code></pre> <pre><code>Random.TaskLocalRNG()\n</code></pre> <p>If we call any function that relies on <code>rng</code> and uses it via <code>randn</code>, <code>rand</code>, etc. <code>rng</code> will be mutated. As we have already established we care a lot about immutability, hence we should use <code>Lux.replicate</code> on PRNGs before using them.</p> <p>First, let us run a random number generator 3 times with the <code>replicate</code>d rng.</p> <pre><code>for i in 1:3\nprintln(\"Iteration $i \", rand(Lux.replicate(rng), 10))\nend\n</code></pre> <pre><code>Iteration 1 [0.4552384158732863, 0.5476424498276177, 0.7733535276924052, 0.9405848223512736, 0.02964765308691042, 0.74694291453392, 0.7468008914093891, 0.9766699015845924, 0.08694684883050086, 0.35149138733595564]\nIteration 2 [0.4552384158732863, 0.5476424498276177, 0.7733535276924052, 0.9405848223512736, 0.02964765308691042, 0.74694291453392, 0.7468008914093891, 0.9766699015845924, 0.08694684883050086, 0.35149138733595564]\nIteration 3 [0.4552384158732863, 0.5476424498276177, 0.7733535276924052, 0.9405848223512736, 0.02964765308691042, 0.74694291453392, 0.7468008914093891, 0.9766699015845924, 0.08694684883050086, 0.35149138733595564]\n</code></pre> <p>As expected we get the same output. We can remove the <code>replicate</code> call and we will get different outputs.</p> <pre><code>for i in 1:3\nprintln(\"Iteration $i \", rand(rng, 10))\nend\n</code></pre> <pre><code>Iteration 1 [0.4552384158732863, 0.5476424498276177, 0.7733535276924052, 0.9405848223512736, 0.02964765308691042, 0.74694291453392, 0.7468008914093891, 0.9766699015845924, 0.08694684883050086, 0.35149138733595564]\nIteration 2 [0.018743665453639813, 0.8601828553599953, 0.6556360448565952, 0.7746656838366666, 0.7817315740767116, 0.5553797706980106, 0.1261990389976131, 0.4488101521328277, 0.624383955429775, 0.05657739601024536]\nIteration 3 [0.19597391412112541, 0.6830945313415872, 0.6776220912718907, 0.6456416023530093, 0.6340362477836592, 0.5595843665394066, 0.5675557670686644, 0.34351700231383653, 0.7237308297251812, 0.3691778381831775]\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#automatic-differentiation","title":"Automatic Differentiation","text":"<p>Julia has quite a few (maybe too many) AD tools. For the purpose of this tutorial, we will use AbstractDifferentiation.jl which provides a uniform API across multiple AD backends. For the backends we will use:</p> <ol> <li>ForwardDiff.jl \u2013 For Jacobian-Vector Product (JVP)</li> <li>Zygote.jl \u2013 For Vector-Jacobian Product (VJP)</li> </ol> <p>Slight Detour: We have had several questions regarding if we will be considering any other AD system for the reverse-diff backend. For now we will stick to Zygote.jl, however once we have tested Lux extensively with Enzyme.jl, we will make the switch.</p> <p>Even though, theoretically, a VJP (Vector-Jacobian product - reverse autodiff) and a JVP (Jacobian-Vector product - forward-mode autodiff) are similar\u2014they compute a product of a Jacobian and a vector\u2014they differ by the computational complexity of the operation. In short, when you have a large number of parameters (hence a wide matrix), a JVP is less efficient computationally than a VJP, and, conversely, a JVP is more efficient when the Jacobian matrix is a tall matrix.</p> <pre><code>using ComponentArrays, ForwardDiff, Zygote\nimport AbstractDifferentiation as AD\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#gradients","title":"Gradients","text":"<p>For our first example, consider a simple function computing \\(f(x) = \\frac{1}{2}x^T x\\), where \\(\\nabla f(x) = x\\)</p> <pre><code>f(x) = x' * x / 2\n\u2207f(x) = x  # `\u2207` can be typed as `\\nabla&lt;TAB&gt;`\nv = randn(rng, Float32, 4)\n</code></pre> <pre><code>4-element Vector{Float32}:\n -0.4051151\n -0.4593922\n  0.92155594\n  1.1871622\n</code></pre> <p>Let's use AbstractDifferentiation and Zygote to compute the gradients.</p> <pre><code>println(\"Actual Gradient: \", \u2207f(v))\nprintln(\"Computed Gradient via Reverse Mode AD (Zygote): \",\nAD.gradient(AD.ZygoteBackend(), f, v)[1])\nprintln(\"Computed Gradient via Forward Mode AD (ForwardDiff): \",\nAD.gradient(AD.ForwardDiffBackend(), f, v)[1])\n</code></pre> <pre><code>Actual Gradient: Float32[-0.4051151, -0.4593922, 0.92155594, 1.1871622]\nComputed Gradient via Reverse Mode AD (Zygote): Float32[-0.4051151, -0.4593922, 0.92155594, 1.1871622]\nComputed Gradient via Forward Mode AD (ForwardDiff): Float32[-0.4051151, -0.4593922, 0.92155594, 1.1871622]\n</code></pre> <p>Note that <code>AD.gradient</code> will only work for scalar valued outputs.</p> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#jacobian-vector-product","title":"Jacobian-Vector Product","text":"<p>I will defer the discussion on forward-mode AD to https://book.sciml.ai/notes/08/. Here let us just look at a mini example on how to use it.</p> <pre><code>f(x) = x .* x ./ 2\nx = randn(rng, Float32, 5)\nv = ones(Float32, 5)\n</code></pre> <pre><code>5-element Vector{Float32}:\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n</code></pre> <p>Construct the pushforward function.</p> <pre><code>pf_f = AD.value_and_pushforward_function(AD.ForwardDiffBackend(), f, x)\n</code></pre> <pre><code>#17 (generic function with 1 method)\n</code></pre> <p>Compute the jvp.</p> <pre><code>val, jvp = pf_f(v)\nprintln(\"Computed Value: f(\", x, \") = \", val)\nprintln(\"JVP: \", jvp[1])\n</code></pre> <pre><code>Computed Value: f(Float32[-0.877497, 1.1953009, -0.057005208, 0.25055695, 0.09351656]) = Float32[0.3850005, 0.71437216, 0.0016247969, 0.031389393, 0.0043726736]\nJVP: Float32[-0.877497, 1.1953009, -0.057005208, 0.25055695, 0.09351656]\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#vector-jacobian-product","title":"Vector-Jacobian Product","text":"<p>Using the same function and inputs, let us compute the VJP.</p> <pre><code>pb_f = AD.value_and_pullback_function(AD.ZygoteBackend(), f, x)\n</code></pre> <pre><code>#25 (generic function with 1 method)\n</code></pre> <p>Compute the vjp.</p> <pre><code>val, vjp = pb_f(v)\nprintln(\"Computed Value: f(\", x, \") = \", val)\nprintln(\"VJP: \", vjp[1])\n</code></pre> <pre><code>Computed Value: f(Float32[-0.877497, 1.1953009, -0.057005208, 0.25055695, 0.09351656]) = Float32[0.3850005, 0.71437216, 0.0016247969, 0.031389393, 0.0043726736]\nVJP: Float32[-0.877497, 1.1953009, -0.057005208, 0.25055695, 0.09351656]\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/Basics/main/#linear-regression","title":"Linear Regression","text":"<p>Finally, now let us consider a linear regression problem. From a set of data-points \\(\\left\\{ (x_i, y_i), i \\in \\left\\{ 1, \\dots, k \\right\\}, x_i \\in \\mathbb{R}^n, y_i \\in \\mathbb{R}^m \\right\\}\\), we try to find a set of parameters \\(W\\) and \\(b\\), s.t. \\(f_{W,b}(x) = Wx + b\\), which minimizes the mean squared error:</p> \\[ L(W, b) \\longrightarrow \\sum_{i = 1}^{k} \\frac{1}{2} \\| y_i - f_{W,b}(x_i) \\|_2^2 \\] <p>We can write <code>f</code> from scratch, but to demonstrate <code>Lux</code>, let us use the <code>Dense</code> layer.</p> <pre><code>model = Dense(10 =&gt; 5)\n\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n</code></pre> <pre><code>Random.TaskLocalRNG()\n</code></pre> <p>Let us initialize the parameters and states (in this case it is empty) for the model.</p> <pre><code>ps, st = Lux.setup(rng, model)\nps = ps |&gt; ComponentArray\n</code></pre> <pre><code>ComponentVector{Float32}(weight = Float32[-0.5583162 0.3457679 \u2026 -0.35419345 0.039559156; -0.05661944 -0.4899126 \u2026 0.22614014 0.27704597; \u2026 ; 0.06026341 -0.11202827 \u2026 0.42526972 -0.3576447; 0.23414856 -0.5949539 \u2026 0.08254115 -0.5224755], bias = Float32[0.0; 0.0; \u2026 ; 0.0; 0.0;;])\n</code></pre> <p>Set problem dimensions.</p> <pre><code>n_samples = 20\nx_dim = 10\ny_dim = 5\n</code></pre> <pre><code>5\n</code></pre> <p>Generate random ground truth W and b.</p> <pre><code>W = randn(rng, Float32, y_dim, x_dim)\nb = randn(rng, Float32, y_dim)\n</code></pre> <pre><code>5-element Vector{Float32}:\n  0.68468636\n -0.57578707\n  0.0594993\n -0.9436797\n  1.5164032\n</code></pre> <p>Generate samples with additional noise.</p> <pre><code>x_samples = randn(rng, Float32, x_dim, n_samples)\ny_samples = W * x_samples .+ b .+ 0.01f0 .* randn(rng, Float32, y_dim, n_samples)\nprintln(\"x shape: \", size(x_samples), \"; y shape: \", size(y_samples))\n</code></pre> <pre><code>x shape: (10, 20); y shape: (5, 20)\n</code></pre> <p>For updating our parameters let's use Optimisers.jl. We will use Stochastic Gradient Descent (SGD) with a learning rate of <code>0.01</code>.</p> <pre><code>using Optimisers\n\nopt = Optimisers.Descent(0.01f0)\n</code></pre> <pre><code>Optimisers.Descent{Float32}(0.01f0)\n</code></pre> <p>Initialize the initial state of the optimiser</p> <pre><code>opt_state = Optimisers.setup(opt, ps)\n</code></pre> <pre><code>Leaf(Descent{Float32}(0.01), nothing)\n</code></pre> <p>Define the loss function</p> <pre><code>mse(model, ps, st, X, y) = sum(abs2, model(X, ps, st)[1] .- y)\nmse(weight, bias, X, y) = sum(abs2, weight * X .+ bias .- y)\nloss_function(ps, X, y) = mse(model, ps, st, X, y)\n\nprintln(\"Loss Value with ground true parameters: \", mse(W, b, x_samples, y_samples))\n\nfor i in 1:100\n# In actual code, don't use globals. But here I will simply for the sake of\n# demonstration\nglobal ps, st, opt_state\n# Compute the gradient\ngs = gradient(loss_function, ps, x_samples, y_samples)[1]\n# Update model parameters\nopt_state, ps = Optimisers.update(opt_state, ps, gs)\nif i % 10 == 1 || i == 100\nprintln(\"Loss Value after $i iterations: \",\nmse(model, ps, st, x_samples, y_samples))\nend\nend\n</code></pre> <pre><code>Loss Value with ground true parameters: 0.009175307\nLoss Value after 1 iterations: 165.57005\nLoss Value after 11 iterations: 4.351237\nLoss Value after 21 iterations: 0.6856849\nLoss Value after 31 iterations: 0.15421417\nLoss Value after 41 iterations: 0.041469414\nLoss Value after 51 iterations: 0.014032223\nLoss Value after 61 iterations: 0.006883738\nLoss Value after 71 iterations: 0.004938521\nLoss Value after 81 iterations: 0.004391277\nLoss Value after 91 iterations: 0.0042331247\nLoss Value after 100 iterations: 0.0041888584\n</code></pre> <p>This page was generated using Literate.jl.</p>"},{"location":"examples/generated/beginner/PolynomialFitting/main/","title":"Fitting a Polynomial","text":""},{"location":"examples/generated/beginner/PolynomialFitting/main/#fitting-a-polynomial-using-mlp","title":"Fitting a Polynomial using MLP","text":"<p>In this tutorial we will fit a MultiLayer Perceptron (MLP) on data generated from a polynomial.</p> <p></p> <p></p>"},{"location":"examples/generated/beginner/PolynomialFitting/main/#package-imports","title":"Package Imports","text":"<pre><code>using Lux\nusing LuxAMDGPU,\nLuxCUDA, Optimisers, Random, Statistics, Zygote, CairoMakie, MakiePublication\n</code></pre> <pre><code>  Activating project at `/var/lib/buildkite-agent/builds/gpuci-17/julialang/lux-dot-jl/examples`\n\u250c Error: Error during loading of extension NNlibAMDGPUExt of NNlib, use `Base.retry_load_extensions()` to retry.\n\u2502   exception =\n\u2502    1-element ExceptionStack:\n\u2502    ArgumentError: Package NNlibAMDGPUExt [244f68ed-b92b-5712-87ae-6c617c41e16a] is required but does not seem to be installed:\n\u2502     - Run `Pkg.instantiate()` to install all recorded dependencies.\n\u2502\n\u2502    Stacktrace:\n\u2502      [1] _require(pkg::Base.PkgId, env::Nothing)\n\u2502        @ Base ./loading.jl:1774\n\u2502      [2] _require_prelocked(uuidkey::Base.PkgId, env::Nothing)\n\u2502        @ Base ./loading.jl:1660\n\u2502      [3] _require_prelocked(uuidkey::Base.PkgId)\n\u2502        @ Base ./loading.jl:1658\n\u2502      [4] run_extension_callbacks(extid::Base.ExtensionId)\n\u2502        @ Base ./loading.jl:1255\n\u2502      [5] run_extension_callbacks(pkgid::Base.PkgId)\n\u2502        @ Base ./loading.jl:1290\n\u2502      [6] run_package_callbacks(modkey::Base.PkgId)\n\u2502        @ Base ./loading.jl:1124\n\u2502      [7] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::Nothing, sourcepath::String, depmods::Vector{Any})\n\u2502        @ Base ./loading.jl:1398\n\u2502      [8] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128)\n\u2502        @ Base ./loading.jl:1494\n\u2502      [9] _require(pkg::Base.PkgId, env::String)\n\u2502        @ Base ./loading.jl:1783\n\u2502     [10] _require_prelocked(uuidkey::Base.PkgId, env::String)\n\u2502        @ Base ./loading.jl:1660\n\u2502     [11] macro expansion\n\u2502        @ ./loading.jl:1648 [inlined]\n\u2502     [12] macro expansion\n\u2502        @ ./lock.jl:267 [inlined]\n\u2502     [13] require(into::Module, mod::Symbol)\n\u2502        @ Base ./loading.jl:1611\n\u2502     [14] eval\n\u2502        @ ./boot.jl:370 [inlined]\n\u2502     [15] #17\n\u2502        @ ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Expanders.jl:629 [inlined]\n\u2502     [16] cd(f::Documenter.Expanders.var\"#17#19\"{Module, Expr}, dir::String)\n\u2502        @ Base.Filesystem ./file.jl:112\n\u2502     [17] (::Documenter.Expanders.var\"#16#18\"{Documenter.Documents.Page, Module, Expr})()\n\u2502        @ Documenter.Expanders ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Expanders.jl:628\n\u2502     [18] (::IOCapture.var\"#3#5\"{DataType, Documenter.Expanders.var\"#16#18\"{Documenter.Documents.Page, Module, Expr}, Task, IOContext{Base.PipeEndpoint}, IOContext{Base.PipeEndpoint}, Base.TTY, Base.TTY})()\n\u2502        @ IOCapture ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/IOCapture/8Uj7o/src/IOCapture.jl:119\n\u2502     [19] with_logstate(f::Function, logstate::Any)\n\u2502        @ Base.CoreLogging ./logging.jl:514\n\u2502     [20] with_logger\n\u2502        @ ./logging.jl:626 [inlined]\n\u2502     [21] capture(f::Documenter.Expanders.var\"#16#18\"{Documenter.Documents.Page, Module, Expr}; rethrow::Type, color::Bool)\n\u2502        @ IOCapture ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/IOCapture/8Uj7o/src/IOCapture.jl:116\n\u2502     [22] runner(#unused#::Type{Documenter.Expanders.ExampleBlocks}, x::Markdown.Code, page::Documenter.Documents.Page, doc::Documenter.Documents.Document)\n\u2502        @ Documenter.Expanders ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Expanders.jl:627\n\u2502     [23] dispatch(::Type{Documenter.Expanders.ExpanderPipeline}, ::Markdown.Code, ::Vararg{Any})\n\u2502        @ Documenter.Utilities.Selectors ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Utilities/Selectors.jl:170\n\u2502     [24] expand(doc::Documenter.Documents.Document)\n\u2502        @ Documenter.Expanders ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Expanders.jl:42\n\u2502     [25] runner(#unused#::Type{Documenter.Builder.ExpandTemplates}, doc::Documenter.Documents.Document)\n\u2502        @ Documenter.Builder ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Builder.jl:226\n\u2502     [26] dispatch(#unused#::Type{Documenter.Builder.DocumentPipeline}, x::Documenter.Documents.Document)\n\u2502        @ Documenter.Utilities.Selectors ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Utilities/Selectors.jl:170\n\u2502     [27] #2\n\u2502        @ ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Documenter.jl:273 [inlined]\n\u2502     [28] cd(f::Documenter.var\"#2#3\"{Documenter.Documents.Document}, dir::String)\n\u2502        @ Base.Filesystem ./file.jl:112\n\u2502     [29] #makedocs#1\n\u2502        @ ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Documenter.jl:272 [inlined]\n\u2502     [30] top-level scope\n\u2502        @ /var/lib/buildkite-agent/builds/gpuci-17/julialang/lux-dot-jl/docs/make.jl:8\n\u2502     [31] include(fname::String)\n\u2502        @ Base.MainInclude ./client.jl:478\n\u2502     [32] top-level scope\n\u2502        @ none:15\n\u2502     [33] eval\n\u2502        @ ./boot.jl:370 [inlined]\n\u2502     [34] exec_options(opts::Base.JLOptions)\n\u2502        @ Base ./client.jl:280\n\u2502     [35] _start()\n\u2502        @ Base ./client.jl:522\n\u2514 @ Base loading.jl:1261\n\u250c Error: Error during loading of extension NNlibCUDAExt of NNlib, use `Base.retry_load_extensions()` to retry.\n\u2502   exception =\n\u2502    1-element ExceptionStack:\n\u2502    ArgumentError: Package NNlibCUDAExt [8a688d86-d2bc-5ad3-8ed1-384f9f2c8cc5] is required but does not seem to be installed:\n\u2502     - Run `Pkg.instantiate()` to install all recorded dependencies.\n\u2502\n\u2502    Stacktrace:\n\u2502      [1] _require(pkg::Base.PkgId, env::Nothing)\n\u2502        @ Base ./loading.jl:1774\n\u2502      [2] _require_prelocked(uuidkey::Base.PkgId, env::Nothing)\n\u2502        @ Base ./loading.jl:1660\n\u2502      [3] _require_prelocked(uuidkey::Base.PkgId)\n\u2502        @ Base ./loading.jl:1658\n\u2502      [4] run_extension_callbacks(extid::Base.ExtensionId)\n\u2502        @ Base ./loading.jl:1255\n\u2502      [5] run_extension_callbacks(pkgid::Base.PkgId)\n\u2502        @ Base ./loading.jl:1290\n\u2502      [6] run_package_callbacks(modkey::Base.PkgId)\n\u2502        @ Base ./loading.jl:1124\n\u2502      [7] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::Nothing, sourcepath::String, depmods::Vector{Any})\n\u2502        @ Base ./loading.jl:1398\n\u2502      [8] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128)\n\u2502        @ Base ./loading.jl:1494\n\u2502      [9] _require(pkg::Base.PkgId, env::String)\n\u2502        @ Base ./loading.jl:1783\n\u2502     [10] _require_prelocked(uuidkey::Base.PkgId, env::String)\n\u2502        @ Base ./loading.jl:1660\n\u2502     [11] macro expansion\n\u2502        @ ./loading.jl:1648 [inlined]\n\u2502     [12] macro expansion\n\u2502        @ ./lock.jl:267 [inlined]\n\u2502     [13] require(into::Module, mod::Symbol)\n\u2502        @ Base ./loading.jl:1611\n\u2502     [14] eval\n\u2502        @ ./boot.jl:370 [inlined]\n\u2502     [15] #17\n\u2502        @ ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Expanders.jl:629 [inlined]\n\u2502     [16] cd(f::Documenter.Expanders.var\"#17#19\"{Module, Expr}, dir::String)\n\u2502        @ Base.Filesystem ./file.jl:112\n\u2502     [17] (::Documenter.Expanders.var\"#16#18\"{Documenter.Documents.Page, Module, Expr})()\n\u2502        @ Documenter.Expanders ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Expanders.jl:628\n\u2502     [18] (::IOCapture.var\"#3#5\"{DataType, Documenter.Expanders.var\"#16#18\"{Documenter.Documents.Page, Module, Expr}, Task, IOContext{Base.PipeEndpoint}, IOContext{Base.PipeEndpoint}, Base.TTY, Base.TTY})()\n\u2502        @ IOCapture ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/IOCapture/8Uj7o/src/IOCapture.jl:119\n\u2502     [19] with_logstate(f::Function, logstate::Any)\n\u2502        @ Base.CoreLogging ./logging.jl:514\n\u2502     [20] with_logger\n\u2502        @ ./logging.jl:626 [inlined]\n\u2502     [21] capture(f::Documenter.Expanders.var\"#16#18\"{Documenter.Documents.Page, Module, Expr}; rethrow::Type, color::Bool)\n\u2502        @ IOCapture ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/IOCapture/8Uj7o/src/IOCapture.jl:116\n\u2502     [22] runner(#unused#::Type{Documenter.Expanders.ExampleBlocks}, x::Markdown.Code, page::Documenter.Documents.Page, doc::Documenter.Documents.Document)\n\u2502        @ Documenter.Expanders ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Expanders.jl:627\n\u2502     [23] dispatch(::Type{Documenter.Expanders.ExpanderPipeline}, ::Markdown.Code, ::Vararg{Any})\n\u2502        @ Documenter.Utilities.Selectors ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Utilities/Selectors.jl:170\n\u2502     [24] expand(doc::Documenter.Documents.Document)\n\u2502        @ Documenter.Expanders ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Expanders.jl:42\n\u2502     [25] runner(#unused#::Type{Documenter.Builder.ExpandTemplates}, doc::Documenter.Documents.Document)\n\u2502        @ Documenter.Builder ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Builder.jl:226\n\u2502     [26] dispatch(#unused#::Type{Documenter.Builder.DocumentPipeline}, x::Documenter.Documents.Document)\n\u2502        @ Documenter.Utilities.Selectors ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Utilities/Selectors.jl:170\n\u2502     [27] #2\n\u2502        @ ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Documenter.jl:273 [inlined]\n\u2502     [28] cd(f::Documenter.var\"#2#3\"{Documenter.Documents.Document}, dir::String)\n\u2502        @ Base.Filesystem ./file.jl:112\n\u2502     [29] #makedocs#1\n\u2502        @ ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Documenter.jl:272 [inlined]\n\u2502     [30] top-level scope\n\u2502        @ /var/lib/buildkite-agent/builds/gpuci-17/julialang/lux-dot-jl/docs/make.jl:8\n\u2502     [31] include(fname::String)\n\u2502        @ Base.MainInclude ./client.jl:478\n\u2502     [32] top-level scope\n\u2502        @ none:15\n\u2502     [33] eval\n\u2502        @ ./boot.jl:370 [inlined]\n\u2502     [34] exec_options(opts::Base.JLOptions)\n\u2502        @ Base ./client.jl:280\n\u2502     [35] _start()\n\u2502        @ Base ./client.jl:522\n\u2514 @ Base loading.jl:1261\n\u250c Error: Error during loading of extension NNlibCUDACUDNNExt of NNlib, use `Base.retry_load_extensions()` to retry.\n\u2502   exception =\n\u2502    1-element ExceptionStack:\n\u2502    ArgumentError: Package NNlibCUDACUDNNExt [ab3ce674-22af-5de9-b6c7-795b17302dcb] is required but does not seem to be installed:\n\u2502     - Run `Pkg.instantiate()` to install all recorded dependencies.\n\u2502\n\u2502    Stacktrace:\n\u2502      [1] _require(pkg::Base.PkgId, env::Nothing)\n\u2502        @ Base ./loading.jl:1774\n\u2502      [2] _require_prelocked(uuidkey::Base.PkgId, env::Nothing)\n\u2502        @ Base ./loading.jl:1660\n\u2502      [3] _require_prelocked(uuidkey::Base.PkgId)\n\u2502        @ Base ./loading.jl:1658\n\u2502      [4] run_extension_callbacks(extid::Base.ExtensionId)\n\u2502        @ Base ./loading.jl:1255\n\u2502      [5] run_extension_callbacks(pkgid::Base.PkgId)\n\u2502        @ Base ./loading.jl:1290\n\u2502      [6] run_package_callbacks(modkey::Base.PkgId)\n\u2502        @ Base ./loading.jl:1124\n\u2502      [7] _tryrequire_from_serialized(modkey::Base.PkgId, path::String, ocachepath::Nothing, sourcepath::String, depmods::Vector{Any})\n\u2502        @ Base ./loading.jl:1398\n\u2502      [8] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128)\n\u2502        @ Base ./loading.jl:1494\n\u2502      [9] _require(pkg::Base.PkgId, env::String)\n\u2502        @ Base ./loading.jl:1783\n\u2502     [10] _require_prelocked(uuidkey::Base.PkgId, env::String)\n\u2502        @ Base ./loading.jl:1660\n\u2502     [11] macro expansion\n\u2502        @ ./loading.jl:1648 [inlined]\n\u2502     [12] macro expansion\n\u2502        @ ./lock.jl:267 [inlined]\n\u2502     [13] require(into::Module, mod::Symbol)\n\u2502        @ Base ./loading.jl:1611\n\u2502     [14] eval\n\u2502        @ ./boot.jl:370 [inlined]\n\u2502     [15] #17\n\u2502        @ ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Expanders.jl:629 [inlined]\n\u2502     [16] cd(f::Documenter.Expanders.var\"#17#19\"{Module, Expr}, dir::String)\n\u2502        @ Base.Filesystem ./file.jl:112\n\u2502     [17] (::Documenter.Expanders.var\"#16#18\"{Documenter.Documents.Page, Module, Expr})()\n\u2502        @ Documenter.Expanders ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Expanders.jl:628\n\u2502     [18] (::IOCapture.var\"#3#5\"{DataType, Documenter.Expanders.var\"#16#18\"{Documenter.Documents.Page, Module, Expr}, Task, IOContext{Base.PipeEndpoint}, IOContext{Base.PipeEndpoint}, Base.TTY, Base.TTY})()\n\u2502        @ IOCapture ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/IOCapture/8Uj7o/src/IOCapture.jl:119\n\u2502     [19] with_logstate(f::Function, logstate::Any)\n\u2502        @ Base.CoreLogging ./logging.jl:514\n\u2502     [20] with_logger\n\u2502        @ ./logging.jl:626 [inlined]\n\u2502     [21] capture(f::Documenter.Expanders.var\"#16#18\"{Documenter.Documents.Page, Module, Expr}; rethrow::Type, color::Bool)\n\u2502        @ IOCapture ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/IOCapture/8Uj7o/src/IOCapture.jl:116\n\u2502     [22] runner(#unused#::Type{Documenter.Expanders.ExampleBlocks}, x::Markdown.Code, page::Documenter.Documents.Page, doc::Documenter.Documents.Document)\n\u2502        @ Documenter.Expanders ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Expanders.jl:627\n\u2502     [23] dispatch(::Type{Documenter.Expanders.ExpanderPipeline}, ::Markdown.Code, ::Vararg{Any})\n\u2502        @ Documenter.Utilities.Selectors ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Utilities/Selectors.jl:170\n\u2502     [24] expand(doc::Documenter.Documents.Document)\n\u2502        @ Documenter.Expanders ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Expanders.jl:42\n\u2502     [25] runner(#unused#::Type{Documenter.Builder.ExpandTemplates}, doc::Documenter.Documents.Document)\n\u2502        @ Documenter.Builder ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Builder.jl:226\n\u2502     [26] dispatch(#unused#::Type{Documenter.Builder.DocumentPipeline}, x::Documenter.Documents.Document)\n\u2502        @ Documenter.Utilities.Selectors ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Utilities/Selectors.jl:170\n\u2502     [27] #2\n\u2502        @ ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Documenter.jl:273 [inlined]\n\u2502     [28] cd(f::Documenter.var\"#2#3\"{Documenter.Documents.Document}, dir::String)\n\u2502        @ Base.Filesystem ./file.jl:112\n\u2502     [29] #makedocs#1\n\u2502        @ ~/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6/packages/Documenter/bYYzK/src/Documenter.jl:272 [inlined]\n\u2502     [30] top-level scope\n\u2502        @ /var/lib/buildkite-agent/builds/gpuci-17/julialang/lux-dot-jl/docs/make.jl:8\n\u2502     [31] include(fname::String)\n\u2502        @ Base.MainInclude ./client.jl:478\n\u2502     [32] top-level scope\n\u2502        @ none:15\n\u2502     [33] eval\n\u2502        @ ./boot.jl:370 [inlined]\n\u2502     [34] exec_options(opts::Base.JLOptions)\n\u2502        @ Base ./client.jl:280\n\u2502     [35] _start()\n\u2502        @ Base ./client.jl:522\n\u2514 @ Base loading.jl:1261\n</code></pre>"},{"location":"examples/generated/beginner/PolynomialFitting/main/#dataset","title":"Dataset","text":"<p>Generate 128 datapoints from the polynomial \\(y = x^2 - 2x\\).</p> <pre><code>function generate_data(rng::AbstractRNG)\nx = reshape(collect(range(-2.0f0, 2.0f0, 128)), (1, 128))\ny = evalpoly.(x, ((0, -2, 1),)) .+ randn(rng, (1, 128)) .* 0.1f0\nreturn (x, y)\nend\n</code></pre> <pre><code>generate_data (generic function with 1 method)\n</code></pre> <p>Initialize the random number generator and fetch the dataset.</p> <pre><code>rng = MersenneTwister()\nRandom.seed!(rng, 12345)\n\n(x, y) = generate_data(rng)\n</code></pre> <pre><code>(Float32[-2.0 -1.968504 \u2026 1.968504 2.0], [8.11723579535073 7.8972862806322315 \u2026 -0.21213293699653427 0.049985105882301])\n</code></pre> <p>Let's visualize the dataset</p> <pre><code>with_theme(theme_web()) do\nfig = Figure()\nax = CairoMakie.Axis(fig[1, 1]; xlabel=\"x\", ylabel=\"y\")\n\nl = lines!(ax, x[1, :], x -&gt; evalpoly(x, (0, -2, 1)); linewidth=3)\ns = scatter!(ax,\nx[1, :],\ny[1, :];\nmarkersize=8,\ncolor=:orange,\nstrokecolor=:black,\nstrokewidth=1)\n\naxislegend(ax, [l, s], [\"True Quadratic Function\", \"Data Points\"])\n\nreturn fig\nend\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"examples/generated/beginner/PolynomialFitting/main/#neural-network","title":"Neural Network","text":"<p>For this problem, you should not be using a neural network. But let's still do that!</p> <pre><code>model = Chain(Dense(1 =&gt; 16, relu), Dense(16 =&gt; 1))\n</code></pre> <pre><code>Chain(\n    layer_1 = Dense(1 =&gt; 16, relu),     # 32 parameters\n    layer_2 = Dense(16 =&gt; 1),           # 17 parameters\n)         # Total: 49 parameters,\n          #        plus 0 states.\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/PolynomialFitting/main/#optimizer","title":"Optimizer","text":"<p>We will use Adam from Optimisers.jl</p> <pre><code>opt = Adam(0.03f0)\n</code></pre> <pre><code>Optimisers.Adam{Float32}(0.03f0, (0.9f0, 0.999f0), 1.1920929f-7)\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/PolynomialFitting/main/#loss-function","title":"Loss Function","text":"<p>We will use the <code>Lux.Training</code> API so we need to ensure that our loss function takes 4 inputs \u2013 model, parameters, states and data. The function must return 3 values \u2013 loss, updated_state, and any computed statistics.</p> <pre><code>function loss_function(model, ps, st, data)\ny_pred, st = Lux.apply(model, data[1], ps, st)\nmse_loss = mean(abs2, y_pred .- data[2])\nreturn mse_loss, st, ()\nend\n</code></pre> <pre><code>loss_function (generic function with 1 method)\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/PolynomialFitting/main/#training","title":"Training","text":"<p>First we will create a <code>Lux.Training.TrainState</code> which is essentially a convenience wrapper over parameters, states and optimizer states.</p> <pre><code>tstate = Lux.Training.TrainState(rng, model, opt)\n</code></pre> <pre><code>Lux.Training.TrainState{Chain{NamedTuple{(:layer_1, :layer_2), Tuple{Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing}, NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:weight, :bias), Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}}}, NamedTuple{(:weight, :bias), Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}}}}}, NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(), Tuple{}}, NamedTuple{(), Tuple{}}}}, NamedTuple{(:layer_1, :layer_2), Tuple{NamedTuple{(:weight, :bias), Tuple{Optimisers.Leaf{Optimisers.Adam{Float32}, Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, Tuple{Float32, Float32}}}, Optimisers.Leaf{Optimisers.Adam{Float32}, Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, Tuple{Float32, Float32}}}}}, NamedTuple{(:weight, :bias), Tuple{Optimisers.Leaf{Optimisers.Adam{Float32}, Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, Tuple{Float32, Float32}}}, Optimisers.Leaf{Optimisers.Adam{Float32}, Tuple{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, Tuple{Float32, Float32}}}}}}}}(Chain(), (layer_1 = (weight = Float32[0.36222202; 0.23371002; \u2026 ; 0.5260752; -0.07562564;;], bias = Float32[0.0; 0.0; \u2026 ; 0.0; 0.0;;]), layer_2 = (weight = Float32[-0.14330137 -0.39328107 \u2026 -0.34761065 -0.05758927], bias = Float32[0.0;;])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()), (layer_1 = (weight = Leaf(Adam{Float32}(0.03, (0.9, 0.999), 1.19209f-7), (Float32[0.0; 0.0; \u2026 ; 0.0; 0.0;;], Float32[0.0; 0.0; \u2026 ; 0.0; 0.0;;], (0.9, 0.999))), bias = Leaf(Adam{Float32}(0.03, (0.9, 0.999), 1.19209f-7), (Float32[0.0; 0.0; \u2026 ; 0.0; 0.0;;], Float32[0.0; 0.0; \u2026 ; 0.0; 0.0;;], (0.9, 0.999)))), layer_2 = (weight = Leaf(Adam{Float32}(0.03, (0.9, 0.999), 1.19209f-7), (Float32[0.0 0.0 \u2026 0.0 0.0], Float32[0.0 0.0 \u2026 0.0 0.0], (0.9, 0.999))), bias = Leaf(Adam{Float32}(0.03, (0.9, 0.999), 1.19209f-7), (Float32[0.0;;], Float32[0.0;;], (0.9, 0.999))))), 0)\n</code></pre> <p>Now we will use Zygote for our AD requirements.</p> <pre><code>vjp_rule = Lux.Training.AutoZygote()\n</code></pre> <pre><code>ADTypes.AutoZygote()\n</code></pre> <p>Finally the training loop.</p> <pre><code>function main(tstate::Lux.Training.TrainState, vjp, data, epochs)\ndata = data .|&gt; gpu_device()\nfor epoch in 1:epochs\ngrads, loss, stats, tstate = Lux.Training.compute_gradients(vjp,\nloss_function,\ndata,\ntstate)\n@info epoch=epoch loss=loss\ntstate = Lux.Training.apply_gradients(tstate, grads)\nend\nreturn tstate\nend\n\ndev_cpu = cpu_device()\ndev_gpu = gpu_device()\n\ntstate = main(tstate, vjp_rule, (x, y), 250)\ny_pred = dev_cpu(Lux.apply(tstate.model, dev_gpu(x), tstate.parameters, tstate.states)[1])\n</code></pre> <pre><code>1\u00d7128 Matrix{Float32}:\n 7.93183  7.76661  7.60138  7.43616  \u2026  -0.305275  -0.280902  -0.25653\n</code></pre> <p>Let's plot the results</p> <pre><code>with_theme(theme_web()) do\nfig = Figure()\nax = CairoMakie.Axis(fig[1, 1]; xlabel=\"x\", ylabel=\"y\")\n\nl = lines!(ax, x[1, :], x -&gt; evalpoly(x, (0, -2, 1)); linewidth=3)\ns1 = scatter!(ax,\nx[1, :],\ny[1, :];\nmarkersize=8,\ncolor=:orange,\nstrokecolor=:black,\nstrokewidth=1)\ns2 = scatter!(ax,\nx[1, :],\ny_pred[1, :];\nmarkersize=8,\ncolor=:green,\nstrokecolor=:black,\nstrokewidth=1)\n\naxislegend(ax, [l, s1, s2], [\"True Quadratic Function\", \"Actual Data\", \"Predictions\"])\n\nreturn fig\nend\n</code></pre> <p></p> <p>This page was generated using Literate.jl.</p>"},{"location":"examples/generated/beginner/SimpleRNN/main/","title":"Training a Simple LSTM","text":""},{"location":"examples/generated/beginner/SimpleRNN/main/#training-a-simple-lstm","title":"Training a Simple LSTM","text":"<p>In this tutorial we will go over using a recurrent neural network to classify clockwise and anticlockwise spirals. By the end of this tutorial you will be able to:</p> <ol> <li>Create custom Lux models.</li> <li>Become familiar with the Lux recurrent neural network API.</li> <li>Training using Optimisers.jl and Zygote.jl.</li> </ol> <p></p> <p></p>"},{"location":"examples/generated/beginner/SimpleRNN/main/#package-imports","title":"Package Imports","text":"<pre><code>using Lux\nusing LuxAMDGPU, LuxCUDA, JLD2, MLUtils, Optimisers, Zygote, Random, Statistics\n</code></pre> <pre><code>  Activating project at `/var/lib/buildkite-agent/builds/gpuci-17/julialang/lux-dot-jl/examples`\n</code></pre>"},{"location":"examples/generated/beginner/SimpleRNN/main/#dataset","title":"Dataset","text":"<p>We will use MLUtils to generate 500 (noisy) clockwise and 500 (noisy) anticlockwise spirals. Using this data we will create a <code>MLUtils.DataLoader</code>. Our dataloader will give us sequences of size 2 \u00d7 seqlen \u00d7 batchsize and we need to predict a binary value whether the sequence is clockwise or anticlockwise.</p> <pre><code>function get_dataloaders(; dataset_size=1000, sequence_length=50)\n# Create the spirals\ndata = [MLUtils.Datasets.make_spiral(sequence_length) for _ in 1:dataset_size]\n# Get the labels\nlabels = vcat(repeat([0.0f0], dataset_size \u00f7 2), repeat([1.0f0], dataset_size \u00f7 2))\nclockwise_spirals = [reshape(d[1][:, 1:sequence_length], :, sequence_length, 1)\nfor d in data[1:(dataset_size \u00f7 2)]]\nanticlockwise_spirals = [reshape(d[1][:, (sequence_length + 1):end],\n:,\nsequence_length,\n1) for d in data[((dataset_size \u00f7 2) + 1):end]]\nx_data = Float32.(cat(clockwise_spirals..., anticlockwise_spirals...; dims=3))\n# Split the dataset\n(x_train, y_train), (x_val, y_val) = splitobs((x_data, labels); at=0.8, shuffle=true)\n# Create DataLoaders\nreturn (\n# Use DataLoader to automatically minibatch and shuffle the data\nDataLoader(collect.((x_train, y_train)); batchsize=128, shuffle=true),\n# Don't shuffle the validation data\nDataLoader(collect.((x_val, y_val)); batchsize=128, shuffle=false))\nend\n</code></pre> <pre><code>get_dataloaders (generic function with 1 method)\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/SimpleRNN/main/#creating-a-classifier","title":"Creating a Classifier","text":"<p>We will be extending the <code>Lux.AbstractExplicitContainerLayer</code> type for our custom model since it will contain a lstm block and a classifier head.</p> <p>We pass the fieldnames <code>lstm_cell</code> and <code>classifier</code> to the type to ensure that the parameters and states are automatically populated and we don't have to define <code>Lux.initialparameters</code> and <code>Lux.initialstates</code>.</p> <p>To understand more about container layers, please look at Container Layer.</p> <pre><code>struct SpiralClassifier{L, C} &lt;:\nLux.AbstractExplicitContainerLayer{(:lstm_cell, :classifier)}\nlstm_cell::L\nclassifier::C\nend\n</code></pre> <p>We won't define the model from scratch but rather use the <code>Lux.LSTMCell</code> and <code>Lux.Dense</code>.</p> <pre><code>function SpiralClassifier(in_dims, hidden_dims, out_dims)\nreturn SpiralClassifier(LSTMCell(in_dims =&gt; hidden_dims),\nDense(hidden_dims =&gt; out_dims, sigmoid))\nend\n</code></pre> <pre><code>Main.SpiralClassifier\n</code></pre> <p>We can use default Lux blocks \u2013 <code>Recurrence(LSTMCell(in_dims =&gt; hidden_dims)</code> \u2013 instead of defining the following. But let's still do it for the sake of it.</p> <p>Now we need to define the behavior of the Classifier when it is invoked.</p> <pre><code>function (s::SpiralClassifier)(x::AbstractArray{T, 3},\nps::NamedTuple,\nst::NamedTuple) where {T}\n# First we will have to run the sequence through the LSTM Cell\n# The first call to LSTM Cell will create the initial hidden state\n# See that the parameters and states are automatically populated into a field called\n# `lstm_cell` We use `eachslice` to get the elements in the sequence without copying,\n# and `Iterators.peel` to split out the first element for LSTM initialization.\nx_init, x_rest = Iterators.peel(eachslice(x; dims=2))\n(y, carry), st_lstm = s.lstm_cell(x_init, ps.lstm_cell, st.lstm_cell)\n# Now that we have the hidden state and memory in `carry` we will pass the input and\n# `carry` jointly\nfor x in x_rest\n(y, carry), st_lstm = s.lstm_cell((x, carry), ps.lstm_cell, st_lstm)\nend\n# After running through the sequence we will pass the output through the classifier\ny, st_classifier = s.classifier(y, ps.classifier, st.classifier)\n# Finally remember to create the updated state\nst = merge(st, (classifier=st_classifier, lstm_cell=st_lstm))\nreturn vec(y), st\nend\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/SimpleRNN/main/#defining-accuracy-loss-and-optimiser","title":"Defining Accuracy, Loss and Optimiser","text":"<p>Now let's define the binarycrossentropy loss. Typically it is recommended to use <code>logitbinarycrossentropy</code> since it is more numerically stable, but for the sake of simplicity we will use <code>binarycrossentropy</code>.</p> <pre><code>function xlogy(x, y)\nresult = x * log(y)\nreturn ifelse(iszero(x), zero(result), result)\nend\n\nfunction binarycrossentropy(y_pred, y_true)\ny_pred = y_pred .+ eps(eltype(y_pred))\nreturn mean(@. -xlogy(y_true, y_pred) - xlogy(1 - y_true, 1 - y_pred))\nend\n\nfunction compute_loss(x, y, model, ps, st)\ny_pred, st = model(x, ps, st)\nreturn binarycrossentropy(y_pred, y), y_pred, st\nend\n\nmatches(y_pred, y_true) = sum((y_pred .&gt; 0.5) .== y_true)\naccuracy(y_pred, y_true) = matches(y_pred, y_true) / length(y_pred)\n</code></pre> <pre><code>accuracy (generic function with 1 method)\n</code></pre> <p>Finally lets create an optimiser given the model parameters.</p> <pre><code>function create_optimiser(ps)\nopt = Optimisers.ADAM(0.01f0)\nreturn Optimisers.setup(opt, ps)\nend\n</code></pre> <pre><code>create_optimiser (generic function with 1 method)\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/beginner/SimpleRNN/main/#training-the-model","title":"Training the Model","text":"<pre><code>function main()\n# Get the dataloaders\n(train_loader, val_loader) = get_dataloaders()\n\n# Create the model\nmodel = SpiralClassifier(2, 8, 1)\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\nps, st = Lux.setup(rng, model)\n\ndev = gpu_device()\nps = ps |&gt; dev\nst = st |&gt; dev\n\n# Create the optimiser\nopt_state = create_optimiser(ps)\n\nfor epoch in 1:25\n# Train the model\nfor (x, y) in train_loader\nx = x |&gt; dev\ny = y |&gt; dev\n(loss, y_pred, st), back = pullback(p -&gt; compute_loss(x, y, model, p, st), ps)\ngs = back((one(loss), nothing, nothing))[1]\nopt_state, ps = Optimisers.update(opt_state, ps, gs)\n\nprintln(\"Epoch [$epoch]: Loss $loss\")\nend\n\n# Validate the model\nst_ = Lux.testmode(st)\nfor (x, y) in val_loader\nx = x |&gt; dev\ny = y |&gt; dev\n(loss, y_pred, st_) = compute_loss(x, y, model, ps, st_)\nacc = accuracy(y_pred, y)\nprintln(\"Validation: Loss $loss Accuracy $acc\")\nend\nend\n\nreturn (ps, st) |&gt; cpu_device()\nend\n\nps_trained, st_trained = main()\n</code></pre> <pre><code>((lstm_cell = (weight_i = Float32[-0.86215186 -0.41712183; -0.25195318 -0.75371134; \u2026 ; -0.21583231 0.6070311; 0.6209413 -0.30775198], weight_h = Float32[-0.5092645 -0.05500876 \u2026 -0.6747616 0.54833186; -0.67166865 0.21679494 \u2026 0.12911268 -0.055484284; \u2026 ; -0.4269104 0.51515096 \u2026 0.085147865 -0.08741968; -0.54997367 0.75317866 \u2026 -0.49941224 0.64469534], bias = Float32[0.29049262; 0.27159366; \u2026 ; -0.1182655; 0.8806431;;]), classifier = (weight = Float32[-1.4358501 0.76847446 \u2026 -0.26181647 1.2245896], bias = Float32[-0.6143867;;])), (lstm_cell = (rng = Random.Xoshiro(0x2026f555c226bf09, 0x8a6bb764b93cadda, 0x5ba3c10439600514, 0x446f763658f71987),), classifier = NamedTuple()))\n</code></pre>"},{"location":"examples/generated/beginner/SimpleRNN/main/#saving-the-model","title":"Saving the Model","text":"<p>We can save the model using JLD2 (and any other serialization library of your choice) Note that we transfer the model to CPU before saving. Additionally, we recommend that you don't save the model</p> <pre><code>@save \"trained_model.jld2\" {compress = true} ps_trained st_trained\n</code></pre> <p>Let's try loading the model</p> <pre><code>@load \"trained_model.jld2\" ps_trained st_trained\n</code></pre> <pre><code>2-element Vector{Symbol}:\n :ps_trained\n :st_trained\n</code></pre> <p>This page was generated using Literate.jl.</p>"},{"location":"examples/generated/intermediate/BayesianNN/main/","title":"Bayesian Neural Network","text":""},{"location":"examples/generated/intermediate/BayesianNN/main/#bayesian-neural-network","title":"Bayesian Neural Network","text":"<p>We borrow this tutorial from the official Turing Docs. We will show how the explicit parameterization of Lux enables first-class composability with packages which expect flattened out parameter vectors.</p> <p>We will use Turing.jl with Lux.jl to implement implementing a classification algorithm. Lets start by importing the relevant libraries.</p> <pre><code># Import libraries\nusing Lux\nusing Turing, CairoMakie, Random, ReverseDiff, Functors, MakiePublication\n\n# Hide sampling progress\nTuring.setprogress!(false);\n\n# Use reverse_diff due to the number of parameters in neural networks\nTuring.setadbackend(:reversediff)\n</code></pre> <pre><code>:reversediff\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/intermediate/BayesianNN/main/#generating-data","title":"Generating data","text":"<p>Our goal here is to use a Bayesian neural network to classify points in an artificial dataset. The code below generates data points arranged in a box-like pattern and displays a graph of the dataset we'll be working with.</p> <pre><code># Number of points to generate\nN = 80\nM = round(Int, N / 4)\nrng = Random.default_rng()\nRandom.seed!(rng, 1234)\n\n# Generate artificial data\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nxt1s = Array([[x1s[i] + 0.5f0; x2s[i] + 0.5f0] for i in 1:M])\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nappend!(xt1s, Array([[x1s[i] - 5.0f0; x2s[i] - 5.0f0] for i in 1:M]))\n\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nxt0s = Array([[x1s[i] + 0.5f0; x2s[i] - 5.0f0] for i in 1:M])\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nappend!(xt0s, Array([[x1s[i] - 5.0f0; x2s[i] + 0.5f0] for i in 1:M]))\n\n# Store all the data for later\nxs = [xt1s; xt0s]\nts = [ones(2 * M); zeros(2 * M)]\n\n# Plot data points\n\nfunction plot_data()\nx1 = first.(xt1s)\ny1 = last.(xt1s)\nx2 = first.(xt0s)\ny2 = last.(xt0s)\n\nfig = with_theme(theme_web()) do\nfig = Figure()\nax = CairoMakie.Axis(fig[1, 1]; xlabel=\"x\", ylabel=\"y\")\n\nscatter!(ax, x1, y1; markersize=8, color=:red, strokecolor=:black, strokewidth=1)\nscatter!(ax, x2, y2; markersize=8, color=:blue, strokecolor=:black, strokewidth=1)\n\nreturn fig\nend\n\nreturn fig\nend\n\nplot_data()\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"examples/generated/intermediate/BayesianNN/main/#building-the-neural-network","title":"Building the Neural Network","text":"<p>The next step is to define a feedforward neural network where we express our parameters as distributions, and not single points as with traditional neural networks. For this we will use <code>Dense</code> to define liner layers and compose them via <code>Chain</code>, both are neural network primitives from <code>Lux</code>. The network <code>nn</code> we will create will have two hidden layers with <code>tanh</code> activations and one output layer with <code>sigmoid</code> activation, as shown below.</p> <p>The <code>nn</code> is an instance that acts as a function and can take data, parameters and current state as inputs and output predictions. We will define distributions on the neural network parameters.</p> <pre><code># Construct a neural network using Lux\nnn = Chain(Dense(2 =&gt; 3, tanh), Dense(3 =&gt; 2, tanh), Dense(2 =&gt; 1, sigmoid))\n\n# Initialize the model weights and state\nps, st = Lux.setup(rng, nn)\n\nLux.parameterlength(nn) # number of paraemters in NN\n</code></pre> <pre><code>20\n</code></pre> <p>The probabilistic model specification below creates a parameters variable, which has IID normal variables. The parameters represents all parameters of our neural net (weights and biases).</p> <pre><code># Create a regularization term and a Gaussian prior variance term.\nalpha = 0.09\nsig = sqrt(1.0 / alpha)\n</code></pre> <pre><code>3.3333333333333335\n</code></pre> <p>Construct named tuple from a sampled parameter vector. We could also use ComponentArrays here and simply broadcast to avoid doing this. But let's do it this way to avoid dependencies.</p> <pre><code>function vector_to_parameters(ps_new::AbstractVector, ps::NamedTuple)\n@assert length(ps_new) == Lux.parameterlength(ps)\ni = 1\nfunction get_ps(x)\nz = reshape(view(ps_new, i:(i + length(x) - 1)), size(x))\ni += length(x)\nreturn z\nend\nreturn fmap(get_ps, ps)\nend\n\n# Specify the probabilistic model.\n@model function bayes_nn(xs, ts)\nglobal st\n\n# Sample the parameters\nnparameters = Lux.parameterlength(nn)\nparameters ~ MvNormal(zeros(nparameters), sig .* ones(nparameters))\n\n# Forward NN to make predictions\npreds, st = nn(xs, vector_to_parameters(parameters, ps), st)\n\n# Observe each prediction.\nfor i in 1:length(ts)\nts[i] ~ Bernoulli(preds[i])\nend\nend\n</code></pre> <pre><code>bayes_nn (generic function with 2 methods)\n</code></pre> <p>Inference can now be performed by calling sample. We use the HMC sampler here.</p> <pre><code># Perform inference.\nN = 5000\nch = sample(bayes_nn(reduce(hcat, xs), ts), HMC(0.05, 4), N)\n</code></pre> <pre><code>Chains MCMC chain (5000\u00d730\u00d71 Array{Float64, 3}):\n\nIterations        = 1:1:5000\nNumber of chains  = 1\nSamples per chain = 5000\nWall duration     = 70.67 seconds\nCompute duration  = 70.67 seconds\nparameters        = parameters[1], parameters[2], parameters[3], parameters[4], parameters[5], parameters[6], parameters[7], parameters[8], parameters[9], parameters[10], parameters[11], parameters[12], parameters[13], parameters[14], parameters[15], parameters[16], parameters[17], parameters[18], parameters[19], parameters[20]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n      parameters      mean       std      mcse   ess_bulk   ess_tail      rhat \u22ef\n          Symbol   Float64   Float64   Float64    Float64    Float64   Float64 \u22ef\n\n   parameters[1]   -2.0521    0.7625    0.1721    20.6080    90.3161    1.0078 \u22ef\n   parameters[2]   -3.0083    3.1557    0.9621    12.7726    21.0999    1.5426 \u22ef\n   parameters[3]    1.3245    0.8417    0.2430    13.3461    24.3246    1.4587 \u22ef\n   parameters[4]   -1.4877    1.0772    0.2493    15.1287    26.2870    1.1494 \u22ef\n   parameters[5]    0.7760    0.5463    0.1115    27.1685   102.1179    1.2247 \u22ef\n   parameters[6]    3.4906    1.7171    0.4559    14.8031    45.0051    1.0628 \u22ef\n   parameters[7]   -3.9710    1.4956    0.3945    15.0640    45.7012    1.1078 \u22ef\n   parameters[8]   -3.4420    1.3931    0.3730    14.6316    21.0172    1.4495 \u22ef\n   parameters[9]   -4.5405    3.0887    0.9352    12.0159    20.0739    1.6221 \u22ef\n  parameters[10]   -3.2319    2.7507    0.8110    12.1252    22.4200    1.3478 \u22ef\n  parameters[11]   -3.5758    1.4822    0.4076    13.6637    33.8975    1.1729 \u22ef\n  parameters[12]   -1.0570    2.4954    0.7492    11.7223    21.0856    1.9121 \u22ef\n  parameters[13]    2.9443    1.5863    0.4630    12.8882    20.7551    1.4709 \u22ef\n  parameters[14]    1.7753    3.9004    1.1683    11.7170    20.8646    1.4595 \u22ef\n  parameters[15]   -2.5492    0.9557    0.2088    22.5673    38.5675    1.1274 \u22ef\n  parameters[16]    1.3716    2.0800    0.5691    14.0441    34.5833    1.1682 \u22ef\n  parameters[17]   -0.4643    1.9644    0.5871    11.5634    21.2536    1.8976 \u22ef\n        \u22ee             \u22ee         \u22ee         \u22ee         \u22ee          \u22ee          \u22ee    \u22f1\n                                                     1 column and 3 rows omitted\n\nQuantiles\n      parameters       2.5%     25.0%     50.0%     75.0%     97.5%\n          Symbol    Float64   Float64   Float64   Float64   Float64\n\n   parameters[1]    -3.3949   -2.6340   -2.0720   -1.4720   -0.6806\n   parameters[2]   -10.8256   -4.7942   -1.1807   -0.8554   -0.4700\n   parameters[3]     0.2152    0.6476    1.1033    1.9170    3.1882\n   parameters[4]    -5.7669   -1.7303   -1.3366   -0.9107   -0.3575\n   parameters[5]    -0.4868    0.5344    0.8637    1.1188    1.7370\n   parameters[6]     0.6259    2.2367    3.5342    4.5333    6.7872\n   parameters[7]    -6.6437   -5.1932   -3.9708   -2.7398   -1.3800\n   parameters[8]    -5.8277   -4.4686   -3.5359   -2.5644   -0.2680\n   parameters[9]   -11.4662   -7.0356   -3.2984   -2.0051   -0.8413\n  parameters[10]    -8.1245   -4.8659   -3.5562   -1.3859    2.0080\n  parameters[11]    -6.4989   -4.7170   -3.5606   -2.3902   -1.0114\n  parameters[12]    -4.6375   -3.1774   -1.3213    0.7895    4.0594\n  parameters[13]     0.8759    1.7717    2.6272    3.5623    6.9655\n  parameters[14]    -5.8894   -2.1307    3.2297    4.9656    7.0390\n  parameters[15]    -4.6926   -3.0672   -2.5009   -1.9073   -0.9006\n  parameters[16]    -2.5810   -0.0622    0.9090    2.9472    5.4754\n  parameters[17]    -3.6208   -2.0939   -0.6436    1.0709    3.5339\n        \u22ee             \u22ee          \u22ee         \u22ee         \u22ee         \u22ee\n                                                       3 rows omitted\n</code></pre> <p>Now we extract the parameter samples from the sampled chain as \u03b8 (this is of size <code>5000 x 20</code> where <code>5000</code> is the number of iterations and <code>20</code> is the number of parameters). We'll use these primarily to determine how good our model's classifier is.</p> <pre><code># Extract all weight and bias parameters.\n\u03b8 = MCMCChains.group(ch, :parameters).value;\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/intermediate/BayesianNN/main/#prediction-visualization","title":"Prediction Visualization","text":"<pre><code># A helper to run the nn through data `x` using parameters `\u03b8`\nnn_forward(x, \u03b8) = first(nn(x, vector_to_parameters(\u03b8, ps), st))\n\n# Plot the data we have.\nfig = plot_data()\n\n# Find the index that provided the highest log posterior in the chain.\n_, i = findmax(ch[:lp])\n\n# Extract the max row value from i.\ni = i.I[1]\n\n# Plot the posterior distribution with a contour plot\nx1_range = collect(range(-6; stop=6, length=25))\nx2_range = collect(range(-6; stop=6, length=25))\nZ = [nn_forward([x1, x2], \u03b8[i, :])[1] for x1 in x1_range, x2 in x2_range]\ncontour!(x1_range, x2_range, Z)\nfig\n</code></pre> <p>The contour plot above shows that the MAP method is not too bad at classifying our data. Now we can visualize our predictions.</p> \\[ p(\\tilde{x} | X, \\alpha) = \\int_{\\\u03b8} p(\\tilde{x} | \\\u03b8) p(\\\u03b8 | X, \\alpha) \\approx \\sum_{\\\u03b8 \\sim p(\\\u03b8 | X, \\alpha)}f_{\\\u03b8}(\\tilde{x}) \\] <p>The <code>nn_predict</code> function takes the average predicted value from a network parameterized by weights drawn from the MCMC chain.</p> <pre><code># Return the average predicted value across multiple weights.\nnn_predict(x, \u03b8, num) = mean([first(nn_forward(x, view(\u03b8, i, :))) for i in 1:10:num])\n</code></pre> <pre><code>nn_predict (generic function with 1 method)\n</code></pre> <p>Next, we use the <code>nn_predict</code> function to predict the value at a sample of points where the x1 and x2 coordinates range between -6 and 6. As we can see below, we still have a satisfactory fit to our data, and more importantly, we can also see where the neural network is uncertain about its predictions much easier\u2013-those regions between cluster boundaries.</p> <p>Plot the average prediction.</p> <pre><code>fig = plot_data()\n\nn_end = 1500\nx1_range = collect(range(-6; stop=6, length=25))\nx2_range = collect(range(-6; stop=6, length=25))\nZ = [nn_predict([x1, x2], \u03b8, n_end)[1] for x1 in x1_range, x2 in x2_range]\ncontour!(x1_range, x2_range, Z)\nfig\n</code></pre> <p></p> <p>Suppose we are interested in how the predictive power of our Bayesian neural network evolved between samples. In that case, the following graph displays an animation of the contour plot generated from the network weights in samples 1 to 1,000.</p> <pre><code># Number of iterations to plot.\nn_end = 1000\n\nfig = plot_data()\nZ = [first(nn_forward([x1, x2], \u03b8[1, :])) for x1 in x1_range, x2 in x2_range]\nc = contour!(x1_range, x2_range, Z)\nfig\n</code></pre> <p></p> <p>Plotting the Final contour</p> <pre><code>fig = plot_data()\nZ = [first(nn_forward([x1, x2], \u03b8[n_end, :])) for x1 in x1_range, x2 in x2_range]\nc = contour!(x1_range, x2_range, Z)\nfig\n</code></pre> <p></p> <p>This page was generated using Literate.jl.</p>"},{"location":"examples/generated/intermediate/HyperNet/main/","title":"Training a Hyper Network","text":""},{"location":"examples/generated/intermediate/HyperNet/main/#package-imports","title":"Package Imports","text":"<pre><code>using Lux\nusing ComponentArrays,\nLuxAMDGPU,\nLuxCUDA,\nMLDatasets,\nMLUtils,\nOneHotArrays,\nOptimisers,\nRandom,\nSetfield,\nStatistics,\nZygote\nCUDA.allowscalar(false)\n</code></pre> <pre><code>  Activating project at `/var/lib/buildkite-agent/builds/gpuci-17/julialang/lux-dot-jl/examples`\n</code></pre>"},{"location":"examples/generated/intermediate/HyperNet/main/#loading-datasets","title":"Loading Datasets","text":"<pre><code>function _load_dataset(dset, n_train::Int, n_eval::Int, batchsize::Int)\nimgs, labels = dset(:train)[1:n_train]\nx_train, y_train = reshape(imgs, 28, 28, 1, n_train), onehotbatch(labels, 0:9)\n\nimgs, labels = dset(:test)[1:n_eval]\nx_test, y_test = reshape(imgs, 28, 28, 1, n_eval), onehotbatch(labels, 0:9)\n\nreturn (DataLoader((x_train, y_train); batchsize=min(batchsize, n_train), shuffle=true),\nDataLoader((x_test, y_test); batchsize=min(batchsize, n_eval), shuffle=false))\nend\n\nfunction load_datasets(n_train=1024, n_eval=32, batchsize=256)\nreturn _load_dataset.((MNIST, FashionMNIST), n_train, n_eval, batchsize)\nend\n</code></pre> <pre><code>load_datasets (generic function with 4 methods)\n</code></pre>"},{"location":"examples/generated/intermediate/HyperNet/main/#implement-a-hypernet-layer","title":"Implement a HyperNet Layer","text":"<pre><code>struct HyperNet{W &lt;: Lux.AbstractExplicitLayer, C &lt;: Lux.AbstractExplicitLayer, A} &lt;:\nLux.AbstractExplicitContainerLayer{(:weight_generator, :core_network)}\nweight_generator::W\ncore_network::C\nca_axes::A\nend\n\nfunction HyperNet(w::Lux.AbstractExplicitLayer, c::Lux.AbstractExplicitLayer)\nca_axes = Lux.initialparameters(Random.default_rng(), c) |&gt; ComponentArray |&gt; getaxes\nreturn HyperNet(w, c, ca_axes)\nend\n\nfunction Lux.initialparameters(rng::AbstractRNG, h::HyperNet)\nreturn (weight_generator=Lux.initialparameters(rng, h.weight_generator),)\nend\n\nfunction (hn::HyperNet)(x, ps, st::NamedTuple)\nps_new, st_ = hn.weight_generator(x, ps.weight_generator, st.weight_generator)\n@set! st.weight_generator = st_\nreturn ComponentArray(vec(ps_new), hn.ca_axes), st\nend\n\nfunction (hn::HyperNet)((x, y)::T, ps, st::NamedTuple) where {T &lt;: Tuple}\nps_ca, st = hn(x, ps, st)\npred, st_ = hn.core_network(y, ps_ca, st.core_network)\n@set! st.core_network = st_\nreturn pred, st\nend\n</code></pre>"},{"location":"examples/generated/intermediate/HyperNet/main/#create-and-initialize-the-hypernet","title":"Create and Initialize the HyperNet","text":"<pre><code>function create_model()\n# Doesn't need to be a MLP can have any Lux Layer\ncore_network = Chain(FlattenLayer(), Dense(784, 256, relu), Dense(256, 10))\nweight_generator = Chain(Embedding(2 =&gt; 32),\nDense(32, 64, relu),\nDense(64, Lux.parameterlength(core_network)))\n\nmodel = HyperNet(weight_generator, core_network)\n\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n\nps, st = Lux.setup(rng, model) .|&gt; gpu_device()\n\nreturn model, ps, st\nend\n</code></pre> <pre><code>create_model (generic function with 1 method)\n</code></pre>"},{"location":"examples/generated/intermediate/HyperNet/main/#define-utility-functions","title":"Define Utility Functions","text":"<pre><code>logitcrossentropy(y_pred, y) = mean(-sum(y .* logsoftmax(y_pred); dims=1))\n\nfunction loss(data_idx, x, y, model, ps, st)\ny_pred, st = model((data_idx, x), ps, st)\nreturn logitcrossentropy(y_pred, y), st\nend\n\nfunction accuracy(model, ps, st, dataloader, data_idx)\ntotal_correct, total = 0, 0\nst = Lux.testmode(st)\ndev = gpu_device()\ncpu_dev = cpu_device()\nfor (x, y) in dataloader\nx = x |&gt; dev\ny = y |&gt; dev\ntarget_class = onecold(cpu_dev(y))\npredicted_class = onecold(cpu_dev(model((data_idx, x), ps, st)[1]))\ntotal_correct += sum(target_class .== predicted_class)\ntotal += length(target_class)\nend\nreturn total_correct / total\nend\n</code></pre> <pre><code>accuracy (generic function with 1 method)\n</code></pre>"},{"location":"examples/generated/intermediate/HyperNet/main/#training","title":"Training","text":"<pre><code>function train()\nmodel, ps, st = create_model()\n\n# Training\ndataloaders = load_datasets()\n\nopt = Adam(0.001f0)\nst_opt = Optimisers.setup(opt, ps)\n\ndev = gpu_device()\n\n### Warmup the Model\nimg, lab = dev(dataloaders[1][1].data[1][:, :, :, 1:1]),\ndev(dataloaders[1][1].data[2][:, 1:1])\nloss(1, img, lab, model, ps, st)\n(l, _), back = pullback(p -&gt; loss(1, img, lab, model, p, st), ps)\nback((one(l), nothing))\n\n### Lets train the model\nnepochs = 9\nfor epoch in 1:nepochs\nfor data_idx in 1:2\ntrain_dataloader, test_dataloader = dataloaders[data_idx]\n\nstime = time()\nfor (x, y) in train_dataloader\nx = x |&gt; dev\ny = y |&gt; dev\n(l, st), back = pullback(p -&gt; loss(data_idx, x, y, model, p, st), ps)\ngs = back((one(l), nothing))[1]\nst_opt, ps = Optimisers.update(st_opt, ps, gs)\nend\nttime = time() - stime\n\ntrain_acc = round(accuracy(model, ps, st, train_dataloader, data_idx) * 100;\ndigits=2)\ntest_acc = round(accuracy(model, ps, st, test_dataloader, data_idx) * 100;\ndigits=2)\n\ndata_name = data_idx == 1 ? \"MNIST\" : \"FashionMNIST\"\n\nprintln(\"[$epoch/$nepochs] \\t $data_name Time $(round(ttime; digits=2))s \\t \" *\n\"Training Accuracy: $(train_acc)% \\t Test Accuracy: $(test_acc)%\")\nend\nend\n\nfor data_idx in 1:2\ntrain_dataloader, test_dataloader = dataloaders[data_idx]\ntrain_acc = round(accuracy(model, ps, st, train_dataloader, data_idx) * 100;\ndigits=2)\ntest_acc = round(accuracy(model, ps, st, test_dataloader, data_idx) * 100; digits=2)\n\ndata_name = data_idx == 1 ? \"MNIST\" : \"FashionMNIST\"\n\nprintln(\"[FINAL] \\t $data_name Training Accuracy: $(train_acc)% \\t \" *\n\"Test Accuracy: $(test_acc)%\")\nend\nend\n\ntrain()\n</code></pre> <pre><code>[1/9]    MNIST Time 5.1s     Training Accuracy: 72.66%   Test Accuracy: 78.12%\n[1/9]    FashionMNIST Time 0.02s     Training Accuracy: 60.06%   Test Accuracy: 50.0%\n[2/9]    MNIST Time 0.02s    Training Accuracy: 66.11%   Test Accuracy: 62.5%\n[2/9]    FashionMNIST Time 0.07s     Training Accuracy: 55.37%   Test Accuracy: 50.0%\n[3/9]    MNIST Time 0.02s    Training Accuracy: 72.07%   Test Accuracy: 68.75%\n[3/9]    FashionMNIST Time 0.02s     Training Accuracy: 71.78%   Test Accuracy: 65.62%\n[4/9]    MNIST Time 0.02s    Training Accuracy: 79.49%   Test Accuracy: 62.5%\n[4/9]    FashionMNIST Time 0.02s     Training Accuracy: 73.44%   Test Accuracy: 59.38%\n[5/9]    MNIST Time 0.02s    Training Accuracy: 85.45%   Test Accuracy: 81.25%\n[5/9]    FashionMNIST Time 0.02s     Training Accuracy: 76.46%   Test Accuracy: 68.75%\n[6/9]    MNIST Time 0.02s    Training Accuracy: 89.26%   Test Accuracy: 81.25%\n[6/9]    FashionMNIST Time 0.02s     Training Accuracy: 75.98%   Test Accuracy: 68.75%\n[7/9]    MNIST Time 0.02s    Training Accuracy: 94.14%   Test Accuracy: 93.75%\n[7/9]    FashionMNIST Time 0.02s     Training Accuracy: 76.46%   Test Accuracy: 62.5%\n[8/9]    MNIST Time 0.02s    Training Accuracy: 94.63%   Test Accuracy: 90.62%\n[8/9]    FashionMNIST Time 0.02s     Training Accuracy: 79.49%   Test Accuracy: 62.5%\n[9/9]    MNIST Time 0.02s    Training Accuracy: 94.73%   Test Accuracy: 96.88%\n[9/9]    FashionMNIST Time 0.02s     Training Accuracy: 78.71%   Test Accuracy: 62.5%\n[FINAL]      MNIST Training Accuracy: 95.02%     Test Accuracy: 93.75%\n[FINAL]      FashionMNIST Training Accuracy: 78.71%      Test Accuracy: 62.5%\n</code></pre> <p>This page was generated using Literate.jl.</p>"},{"location":"examples/generated/intermediate/NeuralODE/main/","title":"MNIST Classification using NeuralODE","text":""},{"location":"examples/generated/intermediate/NeuralODE/main/#mnist-classification-using-neural-odes","title":"MNIST Classification using Neural ODEs","text":"<p>To understand Neural ODEs, users should look up these lecture notes. We recommend users to directly use DiffEqFlux.jl, instead of implementing Neural ODEs from scratch.</p> <p></p> <p></p>"},{"location":"examples/generated/intermediate/NeuralODE/main/#package-imports","title":"Package Imports","text":"<pre><code>using Lux\nusing ComponentArrays,\nSciMLSensitivity,\nLuxAMDGPU,\nLuxCUDA,\nOptimisers,\nOrdinaryDiffEq,\nRandom,\nStatistics,\nZygote,\nOneHotArrays\nimport MLDatasets: MNIST\nimport MLUtils: DataLoader, splitobs\nCUDA.allowscalar(false)\n</code></pre> <pre><code>  Activating project at `/var/lib/buildkite-agent/builds/gpuci-17/julialang/lux-dot-jl/examples`\n</code></pre>"},{"location":"examples/generated/intermediate/NeuralODE/main/#loading-mnist","title":"Loading MNIST","text":"<pre><code>function loadmnist(batchsize, train_split)\n# Load MNIST: Only 1500 for demonstration purposes\nN = 1500\ndataset = MNIST(; split=:train)\nimgs = dataset.features[:, :, 1:N]\nlabels_raw = dataset.targets[1:N]\n\n# Process images into (H,W,C,BS) batches\nx_data = Float32.(reshape(imgs, size(imgs, 1), size(imgs, 2), 1, size(imgs, 3)))\ny_data = onehotbatch(labels_raw, 0:9)\n(x_train, y_train), (x_test, y_test) = splitobs((x_data, y_data); at=train_split)\n\nreturn (\n# Use DataLoader to automatically minibatch and shuffle the data\nDataLoader(collect.((x_train, y_train)); batchsize=batchsize, shuffle=true),\n# Don't shuffle the test data\nDataLoader(collect.((x_test, y_test)); batchsize=batchsize, shuffle=false))\nend\n</code></pre> <pre><code>loadmnist (generic function with 1 method)\n</code></pre>"},{"location":"examples/generated/intermediate/NeuralODE/main/#define-the-neural-ode-layer","title":"Define the Neural ODE Layer","text":"<p>The NeuralODE is a ContainerLayer, which stores a <code>model</code>. The parameters and states of the NeuralODE are same as those of the underlying model.</p> <pre><code>struct NeuralODE{M &lt;: Lux.AbstractExplicitLayer, So, Se, T, K} &lt;:\nLux.AbstractExplicitContainerLayer{(:model,)}\nmodel::M\nsolver::So\nsensealg::Se\ntspan::T\nkwargs::K\nend\n\nfunction NeuralODE(model::Lux.AbstractExplicitLayer;\nsolver=Tsit5(),\nsensealg=InterpolatingAdjoint(; autojacvec=ZygoteVJP()),\ntspan=(0.0f0, 1.0f0),\nkwargs...)\nreturn NeuralODE(model, solver, sensealg, tspan, kwargs)\nend\n\nfunction (n::NeuralODE)(x, ps, st)\nfunction dudt(u, p, t)\nu_, st = n.model(u, p, st)\nreturn u_\nend\nprob = ODEProblem{false}(ODEFunction{false}(dudt), x, n.tspan, ps)\nreturn solve(prob, n.solver; sensealg=n.sensealg, n.kwargs...), st\nend\n\nfunction diffeqsol_to_array(x::ODESolution{T, N, &lt;:AbstractVector{&lt;:CuArray}}) where {T, N}\ndev = gpu_device()\nreturn dropdims(dev(x); dims=3)\nend\nfunction diffeqsol_to_array(x::ODESolution{T, N, &lt;:AbstractVector{&lt;:ROCArray}}) where {T, N}\ndev = gpu_device()\nreturn dropdims(dev(x); dims=3)\nend\ndiffeqsol_to_array(x::ODESolution) = dropdims(Array(x); dims=3)\n</code></pre> <pre><code>diffeqsol_to_array (generic function with 3 methods)\n</code></pre> <p></p> <p></p>"},{"location":"examples/generated/intermediate/NeuralODE/main/#create-and-initialize-the-neural-ode-layer","title":"Create and Initialize the Neural ODE Layer","text":"<pre><code>function create_model()\n# Construct the Neural ODE Model\nmodel = Chain(FlattenLayer(),\nDense(784, 20, tanh),\nNeuralODE(Chain(Dense(20, 10, tanh), Dense(10, 10, tanh), Dense(10, 20, tanh));\nsave_everystep=false,\nreltol=1.0f-3,\nabstol=1.0f-3,\nsave_start=false),\ndiffeqsol_to_array,\nDense(20, 10))\n\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n\nps, st = Lux.setup(rng, model)\ndev = gpu_device()\nps = ComponentArray(ps) |&gt; dev\nst = st |&gt; dev\n\nreturn model, ps, st\nend\n</code></pre> <pre><code>create_model (generic function with 1 method)\n</code></pre>"},{"location":"examples/generated/intermediate/NeuralODE/main/#define-utility-functions","title":"Define Utility Functions","text":"<pre><code>logitcrossentropy(y_pred, y) = mean(-sum(y .* logsoftmax(y_pred); dims=1))\n\nfunction loss(x, y, model, ps, st)\ny_pred, st = model(x, ps, st)\nreturn logitcrossentropy(y_pred, y), st\nend\n\nfunction accuracy(model, ps, st, dataloader)\ntotal_correct, total = 0, 0\nst = Lux.testmode(st)\niterator = CUDA.functional() ? CuIterator(dataloader) : dataloader\ncpu_dev = cpu_device()\nfor (x, y) in iterator\ntarget_class = onecold(cpu_dev(y))\npredicted_class = onecold(cpu_dev(first(model(x, ps, st))))\ntotal_correct += sum(target_class .== predicted_class)\ntotal += length(target_class)\nend\nreturn total_correct / total\nend\n</code></pre> <pre><code>accuracy (generic function with 1 method)\n</code></pre>"},{"location":"examples/generated/intermediate/NeuralODE/main/#training","title":"Training","text":"<pre><code>function train()\nmodel, ps, st = create_model()\n\n# Training\ntrain_dataloader, test_dataloader = loadmnist(128, 0.9)\n\nopt = Optimisers.ADAM(0.001f0)\nst_opt = Optimisers.setup(opt, ps)\n\ndev = gpu_device()\n\n### Warmup the Model\nimg, lab = dev(train_dataloader.data[1][:, :, :, 1:1]),\ndev(train_dataloader.data[2][:, 1:1])\nloss(img, lab, model, ps, st)\n(l, _), back = pullback(p -&gt; loss(img, lab, model, p, st), ps)\nback((one(l), nothing))\n\n### Lets train the model\nnepochs = 9\nfor epoch in 1:nepochs\nstime = time()\nfor (x, y) in train_dataloader\nx = dev(x)\ny = dev(y)\n(l, st), back = pullback(p -&gt; loss(x, y, model, p, st), ps)\n### We need to add `nothing`s equal to the number of returned values - 1\ngs = back((one(l), nothing))[1]\nst_opt, ps = Optimisers.update(st_opt, ps, gs)\nend\nttime = time() - stime\n\nprintln(\"[$epoch/$nepochs] \\t Time $(round(ttime; digits=2))s \\t Training Accuracy: \" *\n\"$(round(accuracy(model, ps, st, train_dataloader) * 100; digits=2))% \\t \" *\n\"Test Accuracy: $(round(accuracy(model, ps, st, test_dataloader) * 100; digits=2))%\")\nend\nend\n\ntrain()\n</code></pre> <pre><code>[1/9]    Time 4.07s      Training Accuracy: 50.52%   Test Accuracy: 42.67%\n[2/9]    Time 0.33s      Training Accuracy: 70.74%   Test Accuracy: 65.33%\n[3/9]    Time 0.27s      Training Accuracy: 77.85%   Test Accuracy: 73.33%\n[4/9]    Time 0.3s   Training Accuracy: 80.74%   Test Accuracy: 74.0%\n[5/9]    Time 0.31s      Training Accuracy: 82.44%   Test Accuracy: 77.33%\n[6/9]    Time 0.54s      Training Accuracy: 84.37%   Test Accuracy: 80.0%\n[7/9]    Time 0.29s      Training Accuracy: 85.93%   Test Accuracy: 80.67%\n[8/9]    Time 0.29s      Training Accuracy: 86.74%   Test Accuracy: 81.33%\n[9/9]    Time 0.3s   Training Accuracy: 87.56%   Test Accuracy: 82.67%\n</code></pre> <p>This page was generated using Literate.jl.</p>"},{"location":"introduction/ecosystem/","title":"Ecosystem","text":""},{"location":"introduction/ecosystem/#ecosystem","title":"Ecosystem","text":""},{"location":"introduction/ecosystem/#frameworks-extending-lux","title":"Frameworks extending Lux","text":"<ul> <li>Boltz.jl \u2013 Prebuilt deep learning models for image classification tasks</li> <li>DeepEquilibriumNetworks.jl \u2013 Continuous and Discrete Deep Equilibrium Networks</li> <li>DiffEqFlux.jl \u2013 Neural Differential Equations, Continuous Normalizing Flows, etc.</li> </ul>"},{"location":"introduction/ecosystem/#extended-julia-ecosystem","title":"Extended Julia Ecosystem","text":"<p>As you might have noticed we don't do much apart from Neural Networks. All other parts of the DL training/evaluation pipeline should be offloaded to:</p> <p></p> <p></p>"},{"location":"introduction/ecosystem/#automatic-differentiation","title":"Automatic Differentiation","text":"<ul> <li>Zygote.jl \u2013 Currently the default and recommended AD library</li> <li>Tracker.jl \u2013 Well tested and robust AD library (might fail on edge cases)</li> <li>Enzyme.jl \u2013 (Very) Experimental Support</li> <li>ForwardDiff.jl \u2013 For forward mode AD support</li> <li>ReverseDiff.jl \u2013 Tape based reverse mode AD (might fail on edge cases and doesn't work on GPU)</li> </ul>"},{"location":"introduction/ecosystem/#data-manipulation-and-loading","title":"Data Manipulation and Loading","text":"<ul> <li>Augmentor.jl</li> <li>DataLoaders.jl</li> <li>Images.jl</li> <li>DataAugmentation.jl</li> </ul>"},{"location":"introduction/ecosystem/#distributed-dataparallel-training","title":"Distributed DataParallel Training","text":"<ul> <li>FluxMPI.jl</li> </ul>"},{"location":"introduction/ecosystem/#neural-network-primitives","title":"Neural Network Primitives","text":"<ul> <li>NNlib.jl</li> <li>LuxLib.jl</li> </ul>"},{"location":"introduction/ecosystem/#optimization","title":"Optimization","text":"<ul> <li>Optimisers.jl</li> <li>ParameterSchedulers.jl</li> <li>Optimization.jl</li> </ul>"},{"location":"introduction/ecosystem/#parameter-manipulation","title":"Parameter Manipulation","text":"<ul> <li>Functors.jl</li> </ul>"},{"location":"introduction/ecosystem/#serialization","title":"Serialization","text":"<ul> <li>Serialization.jl</li> <li>JLD2.jl</li> </ul>"},{"location":"introduction/ecosystem/#testing-utilities","title":"Testing Utilities","text":"<ul> <li>FiniteDifferences.jl \u2013 Finite Differencing. Useful for testing gradient correctness</li> <li>JET.jl</li> <li>LuxTestUtils.jl</li> </ul>"},{"location":"introduction/ecosystem/#training-visualization-logging","title":"Training Visualization &amp; Logging","text":"<ul> <li>Wandb.jl</li> <li>TensorBoardLogger.jl</li> </ul>"},{"location":"introduction/overview/","title":"All about Lux","text":""},{"location":"introduction/overview/#why-we-wrote-lux","title":"Why we wrote Lux?","text":"<p>Julia already has quite a few well established Neural Network Frameworks \u2013 Flux &amp; KNet. However, certain design elements \u2013 Coupled Model and Parameters &amp; Internal Mutations \u2013 associated with these frameworks make them less compiler and user friendly. Making changes to address these problems in the respective frameworks would be too disruptive for users. Here comes in <code>Lux</code>: a neural network framework built completely using pure functions to make it both compiler and autodiff friendly.</p> <p></p> <p></p>"},{"location":"introduction/overview/#design-principles","title":"Design Principles","text":"<ul> <li>Layers must be immutable \u2013 cannot store any parameter/state but rather store the information to construct them</li> <li>Layers are pure functions</li> <li>Layers return a Tuple containing the result and the updated state</li> <li>Given same inputs the outputs must be same \u2013 yes this must hold true even for stochastic functions. Randomness must be controlled using <code>rng</code>s passed in the state.</li> <li>Easily extensible</li> </ul>"},{"location":"introduction/overview/#why-use-lux-over-flux","title":"Why use Lux over Flux?","text":"<ul> <li>Neural Networks for SciML: For SciML Applications (Neural ODEs, Deep Equilibrium Models) solvers typically expect a monolithic parameter vector. Flux enables this via its <code>destructure</code> mechanism, but <code>destructure</code> comes with various edge cases and limitations. Lux forces users to make an explicit distinction between state variables and parameter variables to avoid these issues. Also, it comes battery-included for distributed training using FluxMPI.jl (I know :P the naming)</li> <li>Sensible display of Custom Layers \u2013 Ever wanted to see Pytorch like Network printouts or wondered how to extend the pretty printing of Flux's layers? Lux handles all of that by default.</li> <li>Truly immutable models - No unexpected internal mutations since all layers are implemented as pure functions. All layers are also deterministic given the parameters and state: if a layer is supposed to be stochastic (say <code>Dropout</code>), the state must contain a seed which is then updated after the function call.</li> <li>Easy Parameter Manipulation \u2013 By separating parameter data and layer structures, Lux makes implementing <code>WeightNorm</code>, <code>SpectralNorm</code>, etc. downright trivial. Without this separation, it is much harder to pass such parameters around without mutations which AD systems don't like.</li> </ul>"},{"location":"introduction/overview/#why-not-use-lux","title":"Why not use Lux?","text":"<ul> <li>Small Neural Networks on CPU \u2013 Lux is developed for training large neural networks. For smaller architectures, we recommend using SimpleChains.jl.</li> <li>Lux won't magically speed up your code (yet) \u2013 Lux shares the same backend with Flux and so if your primary desire to shift is driven by performance, you will be disappointed.</li> <li>Special Architecture Support \u2013 Unfortunately, we currently don't support Cloud TPUs and even AMD GPUs are not well tested. (We do plan to support these in the nearish future)</li> </ul>"},{"location":"manual/dispatch_custom_inputs/","title":"Dispatch on Custom Inputs","text":""},{"location":"manual/dispatch_custom_inputs/#dispatching-on-custom-input-types","title":"Dispatching on Custom Input Types","text":""},{"location":"manual/dispatch_custom_inputs/#which-function-should-participate-in-dispatch","title":"Which function should participate in dispatch?","text":"<ul> <li>Defining a dispatch on <code>(::Layer)(x::MyInputType, ps, st::NamedTuple)</code> is inconvenient, since it requires the user to define a new method for every layer type.</li> <li><code>(::AbstractExplicitLayer)(x::MyInputType, ps, st::NamedTuple)</code> doesn't work.</li> <li>Instead, we need to define the dispatch on <code>Lux.apply(::AbstractExplicitLayer, x::MyInputType, ps, st::NamedTuple)</code>.</li> </ul>"},{"location":"manual/dispatch_custom_inputs/#concrete-example","title":"Concrete Example","text":"<p>Consider Neural ODEs. In these models, often time we want to every iteration of the neural network to take the current time as input. Here, we won't go through implementing an entire Neural ODE model. Instead we will define a time dependent version of <code>Chain</code>.</p> <p></p> <p></p>"},{"location":"manual/dispatch_custom_inputs/#time-dependent-chain-implementation","title":"Time-Dependent Chain Implementation","text":"<pre><code>using Lux, Random\n\nstruct TDChain{L &lt;: NamedTuple} &lt;: Lux.AbstractExplicitContainerLayer{(:layers,)}\nlayers::L\nend\n\nfunction (l::TDChain)((x, t)::Tuple, ps, st::NamedTuple)\n# Concatenate along the 2nd last dimension\nsz = ntuple(i -&gt; i == ndims(x) - 1 ? 1 : size(x, i), ndims(x))\nt_ = ones(eltype(x), sz) .* t  # Needs to be modified for GPU\nfor name in keys(l.layers)\nx, st_ = Lux.apply(getfield(l.layers, name), cat(x, t_; dims=ndims(x) - 1),\ngetfield(ps, name), getfield(st, name))\nst = merge(st, NamedTuple{(name,)}((st_,)))\nend\nreturn x, st\nend\n\nmodel = Chain(Dense(3, 4), TDChain((; d1=Dense(5, 4), d2=Dense(5, 4))), Dense(4, 1))\n</code></pre> <pre><code>Chain(\n    layer_1 = Dense(3 =&gt; 4),            # 16 parameters\n    layer_2 = TDChain(\n        layers = NamedTuple(\n            d1 = Dense(5 =&gt; 4),         # 24 parameters\n            d2 = Dense(5 =&gt; 4),         # 24 parameters\n        ),\n    ),\n    layer_3 = Dense(4 =&gt; 1),            # 5 parameters\n)         # Total: 69 parameters,\n          #        plus 0 states.\n</code></pre>"},{"location":"manual/dispatch_custom_inputs/#running-the-tdchain","title":"Running the TDChain","text":"<pre><code>rng = MersenneTwister(0)\nps, st = Lux.setup(rng, model)\nx = randn(rng, Float32, 3, 2)\n\n# model(x, ps, st)\n</code></pre> <pre><code>3\u00d72 Matrix{Float32}:\n  0.473714  1.42305\n  0.300234  0.408387\n -0.762677  0.588621\n</code></pre> <p>The last line is commented out, since it will not work. Try uncommenting it and see what happens.</p> <p></p>"},{"location":"manual/dispatch_custom_inputs/#dispatching-on-custom-input-types_1","title":"Dispatching on Custom Input Types","text":"<ul> <li>Create a Custom Layer storing the time.</li> </ul> <pre><code>struct ArrayAndTime{A &lt;: AbstractArray, T &lt;: Real}\narray::A\ntime::T\nend\n</code></pre> <ul> <li>Define the dispatch on <code>Lux.apply(::AbstractExplicitLayer, x::ArrayAndTime, ps, st::NamedTuple)</code>.</li> </ul> <pre><code>function Lux.apply(layer::Lux.AbstractExplicitLayer, x::ArrayAndTime, ps, st::NamedTuple)\ny, st = layer(x.array, ps, st)\nreturn ArrayAndTime(y, x.time), st\nend\n\nfunction Lux.apply(layer::TDChain, x::ArrayAndTime, ps, st::NamedTuple)\ny, st = layer((x.array, x.time), ps, st)\nreturn ArrayAndTime(y, x.time), st\nend\n</code></pre> <ul> <li>Run the model.</li> </ul> <pre><code>xt = ArrayAndTime(x, 10.0f0)\n\nmodel(xt, ps, st)[1]\n</code></pre> <pre><code>Main.ArrayAndTime{Matrix{Float32}, Float32}(Float32[4.8016562 5.174927], 10.0f0)\n</code></pre>"},{"location":"manual/dispatch_custom_inputs/#using-the-same-input-for-non-td-models","title":"Using the same input for non-TD models","text":"<p>Writing proper dispatch means we can simply replace the <code>TDChain</code> with a <code>Chain</code> (of course with dimension corrections) and the pipeline still works.</p> <pre><code>model = Chain(Dense(3, 4), Chain((; d1=Dense(4, 4), d2=Dense(4, 4))), Dense(4, 1))\n\nps, st = Lux.setup(rng, model)\n\nmodel(xt, ps, st)[1]\n</code></pre> <pre><code>Main.ArrayAndTime{Matrix{Float32}, Float32}(Float32[-0.08124366 -1.1121564], 10.0f0)\n</code></pre>"},{"location":"manual/freezing_parameters/","title":"Freezing Model Parameters","text":""},{"location":"manual/freezing_parameters/#freezing-model-parameters","title":"Freezing Model Parameters","text":"<p>Warning</p> <p>API for freezing parameters should be considered experimental at this point.</p> <p>In this manual we will go over how to freeze certain parameters in a model.</p> <p></p> <p></p>"},{"location":"manual/freezing_parameters/#freezing-layers-of-a-particular-kind","title":"Freezing layers of a particular kind","text":"<p>To freeze a particular kind of layer, let's say <code>Dense</code> in the following example. We can use <code>Lux.@layer_map</code> and freeze layers if they are of type <code>Dense</code>.</p> <pre><code>using Lux, Random\n\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n\nmodel = Chain(Dense(3, 4), Chain(Dense(4, 4), Dropout(0.5f0), BatchNorm(4)),\nDense(4, 1); disable_optimizations=true)\n\nps, st = Lux.setup(rng, model)\n\nx = randn(rng, Float32, 3, 2)\n\nmodel(x, ps, st)\n\nfunction freeze_dense(d::Lux.Dense, ps, st, name::String)\nreturn Lux.freeze(d, ps, st, (:weight, :bias))\nend\nfreeze_dense(l, ps, st, name) = (l, ps, st)\n\nmodel_frozen, ps_frozen, st_frozen = Lux.@layer_map freeze_dense model ps st\n\nmodel_frozen(x, ps_frozen, st_frozen)\n</code></pre> <pre><code>(Float32[1.7641534 -1.7641534], (layer_1 = (frozen_params = (weight = Float32[-0.026350189 -0.5554656 -0.35653266; -0.17461072 0.6705545 0.29924855; -0.8935247 -0.42453378 -0.3020351; -0.7988979 -0.7666331 -0.7104237], bias = Float32[0.0; 0.0; 0.0; 0.0;;]), states = NamedTuple()), layer_2 = (layer_1 = (frozen_params = (weight = Float32[-0.47289538 -0.680748 0.1764085 0.34383082; 0.42747158 -0.13819042 -0.109261915 -0.6143286; -0.35790488 -0.20881107 0.70390546 0.48137343; 0.82561636 0.38187847 0.05779423 -0.35181466], bias = Float32[0.0; 0.0; 0.0; 0.0;;]), states = NamedTuple()), layer_2 = (rng = Random.Xoshiro(0x87711e5ce1a49ffe, 0xa210b60ecab6b8c5, 0x436c749552fc8172, 0x03e9c7d813a9f096), training = Val{true}()), layer_3 = (running_mean = Float32[-0.04517859, 0.03484953, -0.004917746, 0.0074841487], running_var = Float32[0.94082206, 0.92428976, 0.90048367, 0.90112025], training = Val{true}())), layer_3 = (frozen_params = (weight = Float32[0.3981135 0.45468387 -0.07694905 0.8353388], bias = Float32[0.0;;]), states = NamedTuple())))\n</code></pre> <p></p> <p></p>"},{"location":"manual/freezing_parameters/#freezing-by-layer-name","title":"Freezing by layer name","text":"<p>When the function in <code>layer_map</code> is called, the 4th argument is the name of the layer. For example, if you want to freeze the 1st layer inside the inner Chain. The name for this would be <code>&lt;model&gt;.layer_2.layer_1</code>.</p> Freezing by layer nameFreezing by layer type <pre><code>function freeze_by_name(d, ps, st, name::String)\nif name == \"model.layer_2.layer_1\"\nreturn Lux.freeze(d, ps, st, (:weight, :bias))\nelse\nreturn d, ps, st\nend\nend\n</code></pre> <pre><code>function freeze_dense(d::Dense, ps, st, name::String)\nreturn Lux.freeze(d, ps, st, (:weight, :bias))\nend\nfreeze_dense(l, ps, st, name) = (l, ps, st)\n</code></pre> <p></p> <p></p>"},{"location":"manual/freezing_parameters/#freezing-part-of-the-parameters","title":"Freezing part of the parameters","text":"<p>Instead of freezing all the parameters, we can simply specify <code>(:weight,)</code> to freeze only the <code>weight</code> parameter while training the <code>bias</code> parameter.</p> Freezing some parameters of a layerFreezing all parameters of a layer <pre><code>function freeze_by_name(d, ps, st, name::String)\nif name == \"model.layer_2.layer_1\"\nreturn Lux.freeze(d, ps, st, (:weight,))\nelse\nreturn d, ps, st\nend\nend\n</code></pre> <pre><code>function freeze_by_name(d, ps, st, name::String)\nif name == \"model.layer_2.layer_1\"\nreturn Lux.freeze(d, ps, st, (:weight, :bias))\nelse\nreturn d, ps, st\nend\nend\n</code></pre> <p></p> <p></p>"},{"location":"manual/freezing_parameters/#freezing-part-of-a-chain","title":"Freezing part of a Chain","text":"<p>Starting <code>v0.4.22</code>, we can directly index into a <code>Chain</code>. So freezing a part of a <code>Chain</code>, is extremely easy.</p> <pre><code>using Lux, Random\n\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n\nmodel = Chain(Dense(3, 4), Dense(4, 4), Dropout(0.5f0), BatchNorm(4), Dense(4, 1))\n\nmodel_frozen = Chain(model[1:2], Lux.freeze(model[3:4]), model[5])\nps, st = Lux.setup(rng, model_frozen)\n\nx = randn(rng, Float32, 3, 2)\n\nmodel_frozen(x, ps, st)\n</code></pre> <pre><code>(Float32[1.7641534 -1.7641534], (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = (frozen_params = (layer_3 = NamedTuple(), layer_4 = (scale = Float32[1.0, 1.0, 1.0, 1.0], bias = Float32[0.0, 0.0, 0.0, 0.0])), states = (layer_3 = (rng = Random.Xoshiro(0x87711e5ce1a49ffe, 0xa210b60ecab6b8c5, 0x436c749552fc8172, 0x03e9c7d813a9f096), training = Val{true}()), layer_4 = (running_mean = Float32[-0.04517859, 0.03484953, -0.004917746, 0.0074841487], running_var = Float32[0.94082206, 0.92428976, 0.90048367, 0.90112025], training = Val{true}()))), layer_4 = NamedTuple()))\n</code></pre>"},{"location":"manual/gpu_management/","title":"GPU Management","text":""},{"location":"manual/gpu_management/#gpu-management","title":"GPU Management","text":"<p>Note</p> <p>Starting from <code>v0.5</code>, Lux has transitioned to a new GPU management system. The old system using <code>cpu</code> and <code>gpu</code> functions is still in place but will be removed in <code>v0.6</code>. Using the old functions might lead to performance regressions if used inside performance critical code.</p> <p><code>Lux.jl</code> can handle multiple GPU backends. Currently, the following backends are supported:</p> <pre><code>using Lux, LuxCUDA, LuxAMDGPU  # Important to load trigger packages\n\nsupported_gpu_backends()\n</code></pre> <pre><code>(\"CUDA\", \"AMDGPU\", \"Metal\")\n</code></pre> <p></p> <p></p>"},{"location":"manual/gpu_management/#automatic-backend-management-recommended-approach","title":"Automatic Backend Management (Recommended Approach)","text":"<p>Automatic Backend Management is done by two simple functions: <code>cpu_device</code> and <code>gpu_device</code>.</p> <ol> <li><code>cpu_device</code>: This is a simple function and just returns a <code>LuxCPUDevice</code> object.</li> </ol> <pre><code>cdev = cpu_device()\n</code></pre> <pre><code>(::LuxCPUDevice) (generic function with 2 methods)\n</code></pre> <pre><code>x_cpu = randn(Float32, 3, 2)\n</code></pre> <pre><code>3\u00d72 Matrix{Float32}:\n  0.433884   0.229779\n -0.459193  -1.95972\n -0.541064  -1.40102\n</code></pre> <ol> <li> <p><code>gpu_device</code>: This function performs automatic GPU device selection and returns an object.</p> <ol> <li>If no GPU is available, it returns a <code>LuxCPUDevice</code> object.</li> <li> <p>If a LocalPreferences file is present, then the backend specified in the file is used. To set a backend, use <code>Lux.gpu_backend!(&lt;backend_name&gt;)</code>.</p> <ol> <li>If the trigger package corresponding to the device is not loaded, then a warning is displayed.</li> <li>If no LocalPreferences file is present, then the first working GPU with loaded trigger package is used.</li> </ol> </li> </ol> </li> </ol> <pre><code>gdev = gpu_device()\n\nx_gpu = x_cpu |&gt; gdev\n</code></pre> <pre><code>3\u00d72 CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n  0.433884   0.229779\n -0.459193  -1.95972\n -0.541064  -1.40102\n</code></pre> <pre><code>(x_gpu |&gt; cdev) \u2248 x_cpu\n</code></pre> <pre><code>true\n</code></pre> <p></p> <p></p>"},{"location":"manual/gpu_management/#manual-backend-management","title":"Manual Backend Management","text":"<p>Automatic Device Selection can be circumvented by directly using <code>LuxCPUDevice</code> and <code>AbstractLuxGPUDevice</code> objects.</p> <pre><code>cdev = LuxCPUDevice()\n\nx_cpu = randn(Float32, 3, 2)\n\nif LuxCUDA.functional()\ngdev = LuxCUDADevice()\nx_gpu = x_cpu |&gt; gdev\nelseif LuxAMDGPU.functional()\ngdev = LuxAMDGPUDevice()\nx_gpu = x_cpu |&gt; gdev\nelse\n@info \"No GPU is available. Using CPU.\"\nx_gpu = x_cpu\nend\n\n(x_gpu |&gt; cdev) \u2248 x_cpu\n</code></pre> <pre><code>true\n</code></pre>"},{"location":"manual/interface/","title":"Lux Interface","text":""},{"location":"manual/interface/#lux-interface","title":"Lux Interface","text":"<p>Tip</p> <p>If you just want to define compatibility with Lux without actually using any of the other functionality provided by Lux (like layers), it is recommended to depend on <code>LuxCore.jl</code> instead of <code>Lux.jl</code>. <code>LuxCore.jl</code> is a significantly lighter dependency.</p> <p>First let's set the expectations straight.</p> <ul> <li>Do you have to follow the interface? No.</li> <li>Should you follow it? Probably yes.</li> <li>Why? It provides the ability for frameworks built on top of Lux to be cross compatible. Additionally, any new functionality built into Lux, will just work for your framework.</li> </ul> <p>Warning</p> <p>The interface is optional for frameworks being developed independent of Lux. All functionality in the core library (and officially supported ones) must adhere to the interface</p> <p></p> <p></p>"},{"location":"manual/interface/#layer-interface","title":"Layer Interface","text":""},{"location":"manual/interface/#singular-layer","title":"Singular Layer","text":"<p>If the layer doesn't contain any other Lux layer, then it is a <code>Singular Layer</code>. This means it should optionally subtype <code>Lux.AbstractExplicitLayer</code> but mandatorily define all the necessary functions mentioned in the docstrings. Consider a simplified version of <code>Dense</code> called <code>Linear</code>.</p> <p>First, setup the architectural details for this layer. Note, that the architecture doesn't contain any mutable structure like arrays. When in doubt, remember, once constructed a model architecture cannot change.</p> <p>Tip</p> <p>For people coming from Flux.jl background this might be weird. We recommend checking out the Flux to Lux migration guide first before proceeding.</p> <pre><code>using Lux, Random\n\nstruct Linear{F1, F2} &lt;: Lux.AbstractExplicitLayer\nin_dims::Int\nout_dims::Int\ninit_weight::F1\ninit_bias::F2\nend\n\nfunction Linear(in_dims::Int, out_dims::Int; init_weight=Lux.glorot_uniform,\ninit_bias=Lux.zeros32)\nreturn Linear{typeof(init_weight), typeof(init_bias)}(in_dims, out_dims, init_weight,\ninit_bias)\nend\n\nl = Linear(2, 4)\n</code></pre> <pre><code>Linear()\n</code></pre> <p>Next, we need to implement functions which return the parameters and states for the layer. In case of <code>Linear</code>, the parameters are <code>weight</code> and <code>bias</code> while the states are empty. States become important when defining layers like <code>BatchNorm</code>, <code>WeightNorm</code>, etc. The recommended data structure for returning parameters is a NamedTuple, though anything satisfying the Parameter Interface is valid.</p> <pre><code>function Lux.initialparameters(rng::AbstractRNG, l::Linear)\nreturn (weight=l.init_weight(rng, l.out_dims, l.in_dims),\nbias=l.init_bias(rng, l.out_dims, 1))\nend\n\nLux.initialstates(::AbstractRNG, ::Linear) = NamedTuple()\n</code></pre> <p>You could also implement <code>Lux.parameterlength</code> and <code>Lux.statelength</code> to prevent wasteful reconstruction of the parameters and states.</p> <pre><code># This works\nprintln(\"Parameter Length: \", Lux.parameterlength(l), \"; State Length: \",\nLux.statelength(l))\n\n# But still recommened to define these\nLux.parameterlength(l::Linear) = l.out_dims * l.in_dims + l.out_dims\n\nLux.statelength(::Linear) = 0\n</code></pre> <pre><code>Parameter Length: 12; State Length: 0\n</code></pre> <p>Tip</p> <p>You might notice that we don't pass in a <code>PRNG</code> for these functions. If your parameter length and/or state length depend on a random number generator, you should think really hard about what you are trying to do and why.</p> <p>Now, we need to define how the layer works. For this you make your layer a function with exactly 3 arguments \u2013 <code>x</code> the input, <code>ps</code> the parameters, and <code>st</code> the states. This function must return two things \u2013 <code>y</code> the output, and <code>st_new</code> the updated state.</p> <pre><code>function (l::Linear)(x::AbstractMatrix, ps, st::NamedTuple)\ny = ps.weight * x .+ ps.bias\nreturn y, st\nend\n</code></pre> <p>Finally, let's run this layer. If you have made this far into the documentation, we don't feel you need a refresher on that.</p> <pre><code>rng = Random.default_rng()\nRandom.seed!(rng, 0)\n\nps, st = Lux.setup(rng, l)\n\nprintln(\"Parameter Length: \", Lux.parameterlength(l), \"; State Length: \",\nLux.statelength(l))\n\nx = randn(rng, Float32, 2, 1)\n\nLux.apply(l, x, ps, st) # or `l(x, ps, st)`\n</code></pre> <pre><code>(Float32[-0.15276335; 0.45325348; 1.0207279; 0.78226817;;], NamedTuple())\n</code></pre> <p></p> <p></p>"},{"location":"manual/interface/#container-layer","title":"Container Layer","text":"<p>If your layer comprises of other Lux layers, then it is a <code>Container Layer</code>. Note that you could treat it as a <code>Singular Layer</code>, and it is still fine. FWIW, if you cannot subtype your layer with <code>Lux.AbstractExplicitContainerLayer</code> then you should go down the <code>Singular Layer</code> route. But subtyping allows us to bypass some of these common definitions. Let us now define a layer, which is basically a composition of two linear layers.</p> <pre><code>struct ComposedLinear{L1, L2} &lt;: Lux.AbstractExplicitContainerLayer{(:linear_1, :linear_2)}\nlinear_1::L1\nlinear_2::L2\nend\n\nfunction (cl::ComposedLinear)(x::AbstractMatrix, ps, st::NamedTuple)\n# To access the parameters and states for `linear_1` we do `ps.linear_1` and\n# `st.linear_1`. Similarly for `linear_2`\ny, st_l1 = cl.linear_1(x, ps.linear_1, st.linear_1)\ny, st_l2 = cl.linear_2(y, ps.linear_2, st.linear_2)\n# Finally, we need to return the new state which has the exact structure as `st`\nreturn y, (linear_1 = st_l1, linear_2 = st_l2)\nend\n</code></pre> <p>Here, you will notice we have passed <code>(:linear_1, :linear_2)</code> to the supertype. It essentially informs the type that, <code>&lt;obj&gt;.linear_1</code> and <code>&lt;obj&gt;.linear_2</code> are Lux layers and we need to construct parameters and states for those. Let's construct these and see:</p> <pre><code>model = ComposedLinear(Linear(2, 4), Linear(4, 2))\ndisplay(model)\n\nps, st = Lux.setup(rng, model)\n\nprintln(\"Parameters: \", ps)\nprintln(\"States: \", st)\n\nprintln(\"Parameter Length: \", Lux.parameterlength(model), \"; State Length: \",\nLux.statelength(model))\n\nx = randn(rng, Float32, 2, 1)\n\nLux.apply(model, x, ps, st) # or `model(x, ps, st)`\n</code></pre> <pre><code>(Float32[1.3410565; 0.78000563;;], (linear_1 = NamedTuple(), linear_2 = NamedTuple()))\n</code></pre> <p></p> <p></p>"},{"location":"manual/interface/#parameter-interface","title":"Parameter Interface","text":"<p>We accept any parameter type as long as we can fetch the parameters using <code>getproperty(obj, :parameter_name)</code>. This allows us to simultaneously support <code>NamedTuple</code>s and <code>ComponentArray</code>s. Let us go through a concrete example of what it means. Consider <code>Dense</code> which expects two parameters named <code>weight</code> and <code>bias</code>.</p> <p>Note</p> <p>If you are defining your own parameter type, it is your responsibility to make sure that it works with the AutoDiff System you are using.</p> <pre><code>using Lux, Random\n\nd = Dense(2, 3)\nrng = Random.default_rng()\nRandom.seed!(rng, 0)\n\nps_default, st = Lux.setup(rng, d)\n\nx = randn(rng, Float32, 2, 1)\n\nprintln(\"Result with `NamedTuple` parameters: \", first(d(x, ps_default, st)))\n</code></pre> <pre><code>Result with `NamedTuple` parameters: Float32[1.135916; 0.7668784; -1.0876652;;]\n</code></pre> <p>Let, us define a custom parameter type with fields <code>myweight</code> and <code>mybias</code> but if we try to access <code>weight</code> we get back <code>myweight</code>, similar for <code>bias</code>.</p> <p>Warning</p> <p>This is for demonstrative purposes, don't try this at home!</p> <pre><code>struct DenseLayerParameters{W, B}\nmyweight::W\nmybias::B\nend\n\nfunction Base.getproperty(ps::DenseLayerParameters, x::Symbol)\nif x == :weight\nreturn getfield(ps, :myweight)\nelseif x == :bias\nreturn getfield(ps, :mybias)\nend\nreturn getfield(ps, x)\nend\n\nps = DenseLayerParameters(ps_default.weight, ps_default.bias)\n\nprintln(\"Result with `DenseLayerParameters` parameters: \", first(d(x, ps, st)))\n</code></pre> <pre><code>Result with `DenseLayerParameters` parameters: Float32[1.135916; 0.7668784; -1.0876652;;]\n</code></pre> <p>The takeaway from this shouldn't be \u2013 lets define weird parameter types. Simply because you can do weird things like this doesn't mean you should, since it only leads to bugs.</p> <p>Instead this shows the flexibility you have for how your parameters can be structured.</p> <p></p> <p></p>"},{"location":"manual/interface/#state-interface","title":"State Interface","text":"<p>States are always type constrained to be <code>NamedTuple</code>. The structure of the input state must match that of the output state, i.e. <code>keys(st_in) == keys(st_out)</code>. This doesn't imply that types of the input and output state match. To generate efficient code, we often do dispatch on the state, for example, <code>Dropout</code>, <code>BatchNorm</code>, etc.</p>"},{"location":"manual/migrate_from_flux/","title":"Migrating from Flux to Lux","text":""},{"location":"manual/migrate_from_flux/#migrating-from-flux-to-lux","title":"Migrating from Flux to Lux","text":"<p>For the core library layers like <code>Dense</code>, <code>Conv</code>, etc. we have intentionally kept the API very similar to Flux. In most cases, replacing <code>using Flux</code> with <code>using Lux</code> should be enough to get you started. We cover the additional changes that you will have to make in the following example.</p> LuxFlux <pre><code>using Lux, Random, NNlib, Zygote\nmodel = Chain(Dense(2 =&gt; 4), BatchNorm(4, relu), Dense(4 =&gt; 2))\nrng = Random.default_rng()\nx = randn(rng, Float32, 2, 4)\n\nps, st = Lux.setup(rng, model)\nmodel(x, ps, st)\ngradient(ps -&gt; sum(first(model(x, ps, st))), ps)\n</code></pre> <pre><code>using Flux, Random, NNlib, Zygote\n\nmodel = Chain(Dense(2 =&gt; 4), BatchNorm(4, relu), Dense(4 =&gt; 2))\nrng = Random.default_rng()\nx = randn(rng, Float32, 2, 4)\n\n\n\nmodel(x)\n\ngradient(model -&gt; sum(model(x)), model)\n</code></pre> <p></p> <p></p>"},{"location":"manual/migrate_from_flux/#implementing-custom-layers","title":"Implementing Custom Layers","text":"<p>Flux and Lux operate under extremely different design philosophies regarding how layers should be implemented. A summary of the differences would be:</p> <ul> <li>Flux stores everything in a single struct and relies on <code>Functors.@functor</code> and <code>Flux.trainable</code> to distinguish between trainable and non-trainable parameters.</li> <li>Lux relies on the user to define <code>Lux.initialparameters</code> and <code>Lux.initialstates</code> to distinguish between trainable parameters (called \"parameters\") and non-trainable parameters (called \"states\"). Additionally, Lux layers define the model architecture, hence device transfer utilities like <code>gpu</code>, <code>cpu</code>, etc. cannot be applied on Lux layers, instead they need to be applied on the parameters and states.</li> </ul> <p>Let's work through a concrete example to demonstrate this. We will implement a very simple layer that computes \\(A \\times B \\times x\\) where \\(A\\) is not trainable and \\(B\\) is trainable.</p> LuxFlux <pre><code>using Lux, Random, NNlib, Zygote\n\nstruct LuxLinear &lt;: Lux.AbstractExplicitLayer\ninit_A\ninit_B\nend\n\nfunction LuxLinear(A::AbstractArray, B::AbstractArray)\n# Storing Arrays or any mutable structure inside a Lux Layer is not recommended\n# instead we will convert this to a function to perform lazy initialization\nreturn LuxLinear(() -&gt; copy(A), () -&gt; copy(B))\nend\n\n# `B` is a parameter\nLux.initialparameters(rng::AbstractRNG, layer::LuxLinear) = (B=layer.init_B(),)\n\n# `A` is a state\nLux.initialstates(rng::AbstractRNG, layer::LuxLinear) = (A=layer.init_A(),)\n\n(l::LuxLinear)(x, ps, st) = st.A * ps.B * x, st\n</code></pre> <pre><code>using Flux, Random, NNlib, Zygote, Optimisers\n\nstruct FluxLinear\nA\nB\nend\n\n\n\n\n\n\n\n# `A` is not trainable\nOptimisers.trainable(f::FluxLinear) = (B=f.B,)\n\n# Needed so that both `A` and `B` can be transfered between devices\nFlux.@functor FluxLinear\n\n(l::FluxLinear)(x) = l.A * l.B * x\n</code></pre> <p>Now let us run the model.</p> LuxFlux <pre><code>rng = Random.default_rng()\nmodel = LuxLinear(randn(rng, 2, 4), randn(rng, 4, 2))\nx = randn(rng, 2, 1)\n\nps, st = Lux.setup(rng, model)\nmodel(x, ps, st)\ngradient(ps -&gt; sum(first(model(x, ps, st))), ps)\n</code></pre> <pre><code>rng = Random.default_rng()\nmodel = FluxLinear(randn(rng, 2, 4), randn(rng, 4, 2))\nx = randn(rng, 2, 1)\n\n\n\nmodel(x)\n\ngradient(model -&gt; sum(model(x)), model)\n</code></pre> <p>To reiterate some important points:</p> <ul> <li>Don't store mutables like Arrays inside a Lux Layer.</li> <li>Parameters and States should be constructured inside the respective <code>initial*</code> functions.</li> </ul> <p></p> <p></p>"},{"location":"manual/migrate_from_flux/#certain-important-implementation-details","title":"Certain Important Implementation Details","text":""},{"location":"manual/migrate_from_flux/#traininginference-mode","title":"Training/Inference Mode","text":"<p>Flux supports a mode called <code>:auto</code> which automatically decides if the user is training the model or running inference. This is the default mode for <code>Flux.BatchNorm</code>, <code>Flux.GroupNorm</code>, <code>Flux.Dropout</code>, etc. Lux doesn't support this mode (specifically to keep code simple and do exactly what the user wants), hence our default mode is <code>training</code>. This can be changed using <code>Lux.testmode</code>.</p> <p></p> <p></p>"},{"location":"manual/migrate_from_flux/#cant-access-functions-like-relu-sigmoid-etc","title":"Can't access functions like <code>relu</code>, <code>sigmoid</code>, etc?","text":"<p>Unlike Flux we don't reexport functionality from <code>NNlib</code>, all you need to do to fix this is add <code>using NNlib</code>.</p> <p></p> <p></p>"},{"location":"manual/migrate_from_flux/#missing-some-common-layers-from-flux","title":"Missing some common layers from Flux","text":"<p>Lux is a very new framework, as such we haven't implemented all Layers that are a part of Flux. We are tracking the missing features in this issue, and hope to have them implemented soon. If you really need those functionality check out the next section.</p> <p></p> <p></p>"},{"location":"manual/migrate_from_flux/#can-we-still-use-flux-layers","title":"Can we still use Flux Layers?","text":"<p>We don't recommend this method, but here is a way to compose Flux with Lux.</p> <p>Tip</p> <p>Starting <code>v0.4.37</code>, if you have <code>using Flux</code> in your code, Lux will automatically provide a function <code>transform</code> that can convert Flux layers to Lux layers</p> <pre><code>using Lux, NNlib, Random, Optimisers\nimport Flux\n\n# Layer Implementation\nstruct FluxCompatLayer{L,I} &lt;: Lux.AbstractExplicitLayer\nlayer::L\ninit_parameters::I\nend\n\nfunction FluxCompatLayer(flayer)\np, re = Optimisers.destructure(flayer)\np_ = copy(p)\nreturn FluxCompatLayer(re, () -&gt; p_)\nend\n\nLux.initialparameters(rng::AbstractRNG, l::FluxCompatLayer) = (p=l.init_parameters(),)\n\n(f::FluxCompatLayer)(x, ps, st) = f.layer(ps.p)(x), st\n\n# Running the model\nfmodel = Flux.Chain(Flux.Dense(3 =&gt; 4, relu), Flux.Dense(4 =&gt; 1))\n\nlmodel = FluxCompatLayer(fmodel)\n\nrng = Random.default_rng()\nx = randn(rng, 3, 1)\n\nps, st = Lux.setup(rng, lmodel)\n\nlmodel(x, ps, st)[1] == fmodel(x)\n</code></pre>"}]}