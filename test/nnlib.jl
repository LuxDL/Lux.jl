using CUDA, Lux, Random, Test

include("test_utils.jl")

@testset "Elementwise Operation Dispatches" begin
    rng = Random.default_rng()
    Random.seed!(rng, 0)
    custom_activation(x) = abs(x)

    for T in [Float64, Float32, ComplexF64, ComplexF32]
        x = randn(rng, T, 10, 5, 2)
        y = randn(rng, T, 10, 1, 2)

        # On CPU the fallback should always work
        @test Lux.elementwise_add(x, y) == x .+ y
        @test Lux.elementwise_mul(x, y) == x .* y
        @test Lux.applyactivation(tanh, x) == tanh.(x)
        @test Lux.applyactivation(custom_activation, x) == custom_activation.(x)

        if T <: Real
            # Gradient for complex outputs are not defined
            test_gradient_correctness_fdm(sum âˆ˜ Lux.elementwise_add, x, y)
        end

        # On GPU try to use CUDNN
        if CUDA.functional()
            x_g = x |> gpu
            y_g = y |> gpu

            @test Lux.elementwise_add(x_g, y_g) == x_g .+ y_g
            @test Lux.elementwise_mul(x_g, y_g) == x_g .* y_g
            @test Lux.applyactivation(tanh, x_g) == tanh.(x_g)
            # Custom Activation test
            @test Lux.applyactivation(custom_activation, x_g) == custom_activation.(x_g)
        end

        # Deprecated Functionality (Remove in v0.5)
        @test_deprecated Lux.elementwise_add(x, y)
        @test_deprecated Lux.elementwise_mul(x, y)
        @test_deprecated Lux.applyactivation(tanh, x)
    end
end
