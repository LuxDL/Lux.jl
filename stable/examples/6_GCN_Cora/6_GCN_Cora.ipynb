{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Graph Convolutional Networks on Cora"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This example is based on [GCN MLX tutorial](https://github.com/ml-explore/mlx-examples/blob/main/gcn/).\n",
    "While we are doing this manually, we recommend directly using\n",
    "[GNNLux.jl](https://juliagraphs.org/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Lux,\n",
    "    Reactant,\n",
    "    MLDatasets,\n",
    "    Random,\n",
    "    Statistics,\n",
    "    GNNGraphs,\n",
    "    ConcreteStructs,\n",
    "    Printf,\n",
    "    OneHotArrays,\n",
    "    Optimisers\n",
    "\n",
    "const xdev = reactant_device(; force=true)\n",
    "const cdev = cpu_device()\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Cora Dataset"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function loadcora()\n",
    "    data = Cora()\n",
    "    gph = data.graphs[1]\n",
    "    gnngraph = GNNGraph(\n",
    "        gph.edge_index; ndata=gph.node_data, edata=gph.edge_data, gph.num_nodes\n",
    "    )\n",
    "    return (\n",
    "        gph.node_data.features,\n",
    "        onehotbatch(gph.node_data.targets, data.metadata[\"classes\"]),\n",
    "        # We use a dense matrix here to avoid incompatibility with Reactant\n",
    "        Matrix{Int32}(adjacency_matrix(gnngraph)),\n",
    "        # We use this since Reactant doesn't yet support gather adjoint\n",
    "        (1:140, 141:640, 1709:2708),\n",
    "    )\n",
    "end\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Definition"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function GCNLayer(args...; kwargs...)\n",
    "    return @compact(; dense=Dense(args...; kwargs...)) do (x, adj)\n",
    "        @return dense(x) * adj\n",
    "    end\n",
    "end\n",
    "\n",
    "function GCN(x_dim, h_dim, out_dim; nb_layers=2, dropout=0.5, kwargs...)\n",
    "    layer_sizes = vcat(x_dim, [h_dim for _ in 1:nb_layers])\n",
    "    gcn_layers = [\n",
    "        GCNLayer(in_dim => out_dim; kwargs...) for\n",
    "        (in_dim, out_dim) in zip(layer_sizes[1:(end - 1)], layer_sizes[2:end])\n",
    "    ]\n",
    "    last_layer = GCNLayer(layer_sizes[end] => out_dim; kwargs...)\n",
    "    dropout = Dropout(dropout)\n",
    "\n",
    "    return @compact(; gcn_layers, dropout, last_layer) do (x, adj, mask)\n",
    "        for layer in gcn_layers\n",
    "            x = relu.(layer((x, adj)))\n",
    "            x = dropout(x)\n",
    "        end\n",
    "        @return last_layer((x, adj))[:, mask]\n",
    "    end\n",
    "end\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function loss_function(model, ps, st, (x, y, adj, mask))\n",
    "    y_pred, st = model((x, adj, mask), ps, st)\n",
    "    loss = CrossEntropyLoss(; agg=mean, logits=Val(true))(y_pred, y[:, mask])\n",
    "    return loss, st, (; y_pred)\n",
    "end\n",
    "\n",
    "accuracy(y_pred, y) = mean(onecold(y_pred) .== onecold(y)) * 100\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the Model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function main(;\n",
    "    hidden_dim::Int=64,\n",
    "    dropout::Float64=0.1,\n",
    "    nb_layers::Int=2,\n",
    "    use_bias::Bool=true,\n",
    "    lr::Float64=0.001,\n",
    "    weight_decay::Float64=0.0,\n",
    "    patience::Int=20,\n",
    "    epochs::Int=200,\n",
    ")\n",
    "    rng = Random.default_rng()\n",
    "    Random.seed!(rng, 0)\n",
    "\n",
    "    features, targets, adj, (train_idx, val_idx, test_idx) = xdev(loadcora())\n",
    "\n",
    "    gcn = GCN(size(features, 1), hidden_dim, size(targets, 1); nb_layers, dropout, use_bias)\n",
    "    ps, st = xdev(Lux.setup(rng, gcn))\n",
    "    opt = iszero(weight_decay) ? Adam(lr) : AdamW(; eta=lr, lambda=weight_decay)\n",
    "\n",
    "    train_state = Training.TrainState(gcn, ps, st, opt)\n",
    "\n",
    "    @printf \"Total Trainable Parameters: %0.4f M\\n\" (Lux.parameterlength(ps) / 1.0e6)\n",
    "\n",
    "    val_loss_compiled = Reactant.with_config(;\n",
    "        dot_general_precision=PrecisionConfig.HIGH,\n",
    "        convolution_precision=PrecisionConfig.HIGH,\n",
    "    ) do\n",
    "        @compile loss_function(gcn, ps, Lux.testmode(st), (features, targets, adj, val_idx))\n",
    "    end\n",
    "\n",
    "    train_model_compiled = Reactant.with_config(;\n",
    "        dot_general_precision=PrecisionConfig.HIGH,\n",
    "        convolution_precision=PrecisionConfig.HIGH,\n",
    "    ) do\n",
    "        @compile gcn((features, adj, train_idx), ps, Lux.testmode(st))\n",
    "    end\n",
    "    val_model_compiled = Reactant.with_config(;\n",
    "        dot_general_precision=PrecisionConfig.HIGH,\n",
    "        convolution_precision=PrecisionConfig.HIGH,\n",
    "    ) do\n",
    "        @compile gcn((features, adj, val_idx), ps, Lux.testmode(st))\n",
    "    end\n",
    "\n",
    "    best_loss_val = Inf\n",
    "    cnt = 0\n",
    "\n",
    "    for epoch in 1:epochs\n",
    "        (_, loss, _, train_state) = Lux.Training.single_train_step!(\n",
    "            AutoEnzyme(),\n",
    "            loss_function,\n",
    "            (features, targets, adj, train_idx),\n",
    "            train_state;\n",
    "            return_gradients=Val(false),\n",
    "        )\n",
    "        train_acc = accuracy(\n",
    "            Array(\n",
    "                train_model_compiled(\n",
    "                    (features, adj, train_idx),\n",
    "                    train_state.parameters,\n",
    "                    Lux.testmode(train_state.states),\n",
    "                )[1],\n",
    "            ),\n",
    "            Array(targets)[:, train_idx],\n",
    "        )\n",
    "\n",
    "        val_loss = first(\n",
    "            val_loss_compiled(\n",
    "                gcn,\n",
    "                train_state.parameters,\n",
    "                Lux.testmode(train_state.states),\n",
    "                (features, targets, adj, val_idx),\n",
    "            ),\n",
    "        )\n",
    "        val_acc = accuracy(\n",
    "            Array(\n",
    "                val_model_compiled(\n",
    "                    (features, adj, val_idx),\n",
    "                    train_state.parameters,\n",
    "                    Lux.testmode(train_state.states),\n",
    "                )[1],\n",
    "            ),\n",
    "            Array(targets)[:, val_idx],\n",
    "        )\n",
    "\n",
    "        @printf \"Epoch %3d\\tTrain Loss: %.6f\\tTrain Acc: %.4f%%\\tVal Loss: %.6f\\t\\\n",
    "                 Val Acc: %.4f%%\\n\" epoch loss train_acc val_loss val_acc\n",
    "\n",
    "        if val_loss < best_loss_val\n",
    "            best_loss_val = val_loss\n",
    "            cnt = 0\n",
    "        else\n",
    "            cnt += 1\n",
    "            if cnt == patience\n",
    "                @printf \"Early Stopping at Epoch %d\\n\" epoch\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    Reactant.with_config(;\n",
    "        dot_general_precision=PrecisionConfig.HIGH,\n",
    "        convolution_precision=PrecisionConfig.HIGH,\n",
    "    ) do\n",
    "        test_loss = @jit(\n",
    "            loss_function(\n",
    "                gcn,\n",
    "                train_state.parameters,\n",
    "                Lux.testmode(train_state.states),\n",
    "                (features, targets, adj, test_idx),\n",
    "            )\n",
    "        )[1]\n",
    "        test_acc = accuracy(\n",
    "            Array(\n",
    "                @jit(\n",
    "                    gcn(\n",
    "                        (features, adj, test_idx),\n",
    "                        train_state.parameters,\n",
    "                        Lux.testmode(train_state.states),\n",
    "                    )\n",
    "                )[1],\n",
    "            ),\n",
    "            Array(targets)[:, test_idx],\n",
    "        )\n",
    "\n",
    "        @printf \"Test Loss: %.6f\\tTest Acc: %.4f%%\\n\" test_loss test_acc\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "main()\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.7",
   "language": "julia"
  }
 },
 "nbformat": 4
}